* Accomplishments <2017-08-31 Thu>
** The ULTRA Project
A goal set last year was  to investigate Automatic Speech Recognition (ASR) models that have well defined pathways to implementation  in speech to speech (S2S) devices. 
I pledged to  develop models that result in software that can be demoed with realtime interaction. 

I believe I achieved this goal and together with my Applications Team (ATeam) colleagues we far exceeded our expectations of what could be done in this area. 
I was part of an effort by the ATeam to develop a platform where the products we produce could be implemented and tested under conditions that would closely resemble the actual conditions that Soldiers would face in the field. 
The ATeam made the necessary contacts with a contractor from industry  named GXM that was already working for a  branch in the SED division  to develop smart phone apps for use by Soldiers in the field. 
GXM performed the device-level software engineering required to run the ASR and Machine Translation decoding code under the Android OS on a smart phone. 
They also implemented a prototype interface for an S2S system that would be capable of running the software products we develop. 
The prototype system is currently called ULTRA.
A demo run on that prototype system used acoustic models and other components that I developed this year.
Although the future of this project is not certain, in the past year the ATeam went from  baseline systems that ran only on laboratory computers to having a functioning prototype running our own acoustic models and other software components on a smart phone. 

** African French:
I was part of an ATeam effort to coach the VoxTek team on adapting their existing English French S2S application to African Accented Speech. 
I developed African Accented French ASR components as a part of the VoxTek project and for the ULTRA project as well. 

To make sure I was getting state-of-the-art performance from the components I was building, I took on the task of replicating the results that were published in the Kaldi repository for the baseline GP French ASR system. 
Despite the fact that I did not have the exact same lexicon as the one that was used in the published results, my results were within  half a percentage point of the published Word Error Rates (WER). 
I also produced better WER scores using chain models that were not available at the time the results were published. 
Specifically, the best published WER results are 22.51% with boosted maximum mutual information trained Subspace Gaussian Mixture Models (SGMMs) while I obtained a 21.21% WER with chain models and 21.24 WER with online chain models.
It should be noted that the online chain models are designed to run in online or real time mode, whereas it is not clear that this can be accomplished with the sgmm models. 
The tri2b models that we have verified do work on the ULTRA smart phone platform achieved a 25.65% WER. 

Dr. Steve LaRocca and I curated several small speech corpora for use as training data including African Accented French speech from Cameroon, Chad, Congo, Gabon and Niger. 
These corpora can easily be used later in experiments that measure the influence of Accent on ASR systems. 
We have been using one particular corpus of 515 African Accented French utterances  that we call Central Accord 2016 (CA16) as a test set on the VoxTek project and many other occasions when we want to compare the performance of a new system with previous results. 
For example, it is interesting to know that the online chain modle system trained and tested only on European  French achieved a WER of 21.24%. 
When that same system was tested on CA16 its WER scores went up to 69.48%. 
Future experiments will test what happens when increasing amounts of in-domain data are added to the training set to reduce mismatch between training and test sets. 
There is optimism in the ASR community that deep learning approaches produce systems that are more robust to the mismatch between training and test data folds. 

** Publication of Heroico Recipe 
In the past year I have become familiar with the concept of a "recipe" in the context of the Kaldi ASR toolkit. 
A recipe is a small set of publically available scripts that can easily be run to reproduce an ASR experiment. 
It is usually associated with a corpus of speech, a lexicon and text data that can be processed by the scripts to build acoustic models, a language model and a pronouncing dictionary. 
It also includes scripts that run tests that show WER scores. 
This year, I have written my own recipes for several languages including: Arabic, French, German, Russian and Spanish. 
It takes time and effort to learn how to write concise recipe scripts. 
A major achievement this year is getting my recipe for the Heroico Spanish Corpus accepted in the Kaldi repository. 
My recipe  exemplefies what I have learned in the past year about building state-of-the-art ASR systems with Kaldi. 
First, I have scripts that perform the tedious but nonetheless important task of preparing the acoustic and text data in the Heroico corpus for processing by the Kaldi training and evaluation tools. 
I am especially proud of this work since I was involved in every aspect of the creation of the Heroico Corpus which is available from the LDC. 
After data preparation, I go through the standard Kaldi steps of progressively building context dependent (CD) gaussian mixture (GMM) hidden markov model (HMM) acoustic models. 
Then I have scripts that use the CD GMM HMM models to train an I-Vector extractor. 
In the last training step, I train chain models which are the currently best performing neural network acoustic models. 
Chain models  are a kind of time delayed neural network (TDNN) that are trained on speed perturbed (SP) acoustic data. 
The I-Vector extraction and TDNN SP models are part of a third generation of neural networks that Kaldi calls nnet3. 
I mention TDNN SP and nnet3 because I had stated as a goal to investigate these kinds of models.
Finally, I have scripts that evaluate the performance of the systems at each step of the building process. 
Word Error Rates are generated for each of these steps. 

** Neural Networks and Deep Learning
I explored other neural network approaches for ASR in my work on the African Accented French. 
Among those approaches I will mention that I tried the rnn lstm models with the Eesen add on to Kaldi. 
The character-base version of this approach was fairly easy to implement since it does not use a lexicon. 
However, the results were not encouraging. 
The WER scores were in the mid forties while comparable scores for the cd gmm hmm approach were in the mid twenties. 
The rnn lstm approach might have advantages that are not apparent to me at the moment (perhaps online decoding), but I drop the exploration of this approach because of its bad performance. 

I also explored several other approaches including Deep Neural Network (DNN), Deep Belief Networks and Restricted Boltzmann Machines (RBMs). 
The WER scores for these approaches were good, but not quite as good as those for the CD GMM HMM and SGMM approaches. 
Since these neural network models also involved longer and more  complicated training regimes, I also dropped them and focused my attention on chain models. 

One kind of model I had mentioned in my goals that I did not get to are bottleneck neural networks. 
However, I have not ignored these kinds of models. 
In fact, recently, most of my work has involved preparing for next year's project that will involve Bottleneck neural networks. 

** Tunisian Arabic:
Obviously, language technologies that are targeted to perform well when used by Soldiers will benefit the Army.
ASR components that are adapted to perform well on key languages like Arabic and its North African accented flavor are such technologies. 
The ATeam has access to a government-owned corpus of Arabic speech that was collected from speakers in Tunisia. 
I was involved in the creation of this corpus when I worked at West Point. 
A portion of this corpus was not labeled since it was given by informants as answers to questions. 
This year I began a project to use the expertise gained  with Kaldi so far to automatically transcribe the unlabeled data. 
I Built on what I learned last year about semi supervised training  for African Accented French. 
However, instead of stopping after obtaining automatic transcriptions of the unlabeled data as in that case , I introduced a human in the loop step to the labeling process. 
I was able to do this since the ATeam counts with an Arabic speaking expert. 
I built an ASR system with supervised training on the available transcribed speech. 
Then I ran the system on the data from a small set of speakers to get a "rough draft" transcription of the data. 
I handed this to our Arabic expert for correction. 
Once he performed his corrections, he handed the transcripts back to me and I incorporated them into the training set for a new ASR system.
I limited the training to only one pass of Speaker Adapted Training (SAT) which can be performed in around 4 hours. 
then I repeated the process on the next batch of data. 
We are about 80 percent done with the transcription. 
We measured WER scores of the automatic transcription as the process moved forward. 
We observed some frustrating trends that we do not under stand yet. 
The WER scores would make large improvements, then they would progressively deteriorate. 

** Language Modeling with the SUBS corpus:
I had set a goal last year of improving the Language Modeling (LM) in our French and Arabidc systems. 
The speech work done by the ATeam is mostly directed to future implementation in S2S apps. 
S2S apps are meant to facilitate dialogues. 
The S2S components will work best when there is a match between their training data and the data they experience when they are used in the field. 
Training data includes the text that is  used to train the language model (LM)s.
This year we started using the SUBS corpus for training our LMs for Arabic and French. 
The SUBS corpus is a collection of movie subtitles. 
Most of these subtitles are transcriptions of dialogues between characters in movies. 
This turns out to be a good source of data for training LMs that are intended to be used in S2S systems. 

** Colloquium Presentation:
Two projects I worked on this year involved a set of pre transcribed data and a set of unlabled data that came from the same speakers. 
I performed semi supervised training on the French data from Cameroon to get automatic transcriptions for the unlabled data. 
I got an unexpectedly large improvement in WER scores by using these automatically generated transcriptions as training data. 
I believe the improvement stems from the fact that the unlabeled data comes from speakers that the models have already been trained on. 
I presented this idea at the ARL Colloquium. 

* Accomplishments <2017-04-04 Tue>
- Extended minimal Tunisian ASR System to QCRI 2 million word arabic dictionary.
Used the Levenshtein distance to propose to a human expert pronunciations that are close to out of vocabulary words.
Converted buckwalter entries in QCRI dictionary to unicode utf8.
Prepared acoustic and text data for ASR model building with Kaldi on SOFTunis corpus.
- Attended IARPA Babel workshop on Kaldi and pyspeech.
Contributed improvement to Kaldi scripts used in IARPA workshop.
- Wrote scripts to Implement a multi-step process to perform semi-supervised training of acoustic models using unlabeled speech data from the Yaounde corpus.
Achieved improvements in WER score with semi-supervised training regime.
- Wrote scripts for processing all the African Accented French Data to  make an i-vector extractor for African Accented Speech.
- Wrote scripts for end to end  model training and testing with the Kaldi toolkit on several French corpora including:
globalPhone (GP)
GP + Yaounde African French ( chain model ) .
Gabon and Niger 2016 data collection. 
Models include:
Monophone (mono)
Triphones
Subspace Gaussian Mixture Models
- Prepared acoustic data and text labels for Yaounde African French and SOFTunis corpora.
- Setup training with eesen (recurrent neural network) toolkit on gp + Yaounde  corpus.
-Prepared data for lab test of VoxTek device.
- Pass Cyber security Fundamentals Course.
