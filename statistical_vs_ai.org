AI, Machine Learning, Deep Learning and Neural Networks  have surged in the past decade as a paradigm for modeling the processes of Automatic Speech Recognition (ASR), Machine Translation  and Text to Speech. 
The new paradigm has replaced the previous statistical method. 
However, there is research that shows that in some cases the statistical method remains dominant. 

Reference ASR systems are now trained on tens of thousands of hours of speech data. 
Neural Network models perform well  when these large amounts of data are available. 
However, in situations where training data is scarce statistical methods sometimes outperform the best neural network  solutions. 

The US Army frequently finds itself in the situation  where it needs help communicating with people who speak a low resource language. 
It often can benefit from speech to speech applications  that work well for populations of speakers who speak with very special phonological inventories in conversations  about very special topics. 
In this paper we will investigate a way to leverage the benefits of the latest advances in Neural Networks despite a severe paucity of training data in the target linguistic environment . 
We will compare a Multi-task Learning (MTL) neural network method with the best Single Task Learning (STL) statistical methods applied to ASR under conditions of severely restricted speech and text  resources. 

We set as a goal to produce components of an ASR   system  for African Accented French. 
These components include acoustic models, a lexicon and a language model.
They will be designed to be embedded in   one side of a speech to speech device. 
The language model  will be  trained   on conversations about military issues. 
Our neural network acoustic models will be trained with MTL on speech data from varying numbers of corpora in varying numbers of languages including varying amounts of data from African Accented French. 
Our baseline systems will use the statistical methods including gaussian mixture model (GMM) hidden markov models (HMM) with speaker adaptive training (SAT) and subspace GMMs. 
We will also compare the MTL trained models with other STM trained neural network models. 

We hypothesize that the MTL method achieves comparable accuracy  to the statistical methods for low resource and other specialized conditions. 
Additionally, we claim that the MTL models are more robust, i.e., they achieve comparable performance to the statistical methods for a special target linguistic environment while outperforming them on a broader range of linguistic environments. 
The MTL and Statistical models will get similar WER scores on African Accented French, however, the MTL models will get better WER scores than the statistical models on European French.

So far, we have demonstrated that we can build systems with the MTL method by producing a minimal example that uses only two languages. 
We built baseline SAT GMM HMM acoustic models for Japanese and Mandarin Chinese with data from the GlobalPhone speech corpus. 
We implemented MTL as follows:

-  Alignments generated by the Japanese and Mandarin  SAT GMM HMMs were used to train a 7-layer bilingual neural network acoustic model on the combined set of training examples from the Japanese and Mandarin corpora. 
Note that instead of considering this as a bilingual model it can be viewed as a Mandarin model whose parameters are shared with a Japanese model.  
The data from the Mandarin corpus was used to readjust the parameters in the last two layers of the bilingual neural network model to produce two models: a new monolingual Mandarin acoustic model and a new Japanese acoustic model. 
These two models share the parameters in their first five layers.

- The monolingual Mandarin acoustic model was used to decode a test set. 
Preliminary WER scores showed a slight improvement over the WER scores for the Mandarin SAT GMM HMMs. 
Although this is good news, we do not expect the new monolingual Mandarin acoustic models to yield better WER scores than models trained with the state-of-the-art chain  model objective. 

Our minimal example did not include a prefinal layer of bottleneck (BN) features and it was not trained using i-vectors. 
BN features and i-vector training have been shown to lower WER scores. 
We are expecting our MTL trained models to yield lower WER scores once they use i-vectors and BN features. 

For our experiments we plan on applying MTL to up to 30 languages and up to 300 hours of speech.
The GlobalPhone corpus has data for 17 languages. 
We also have 9 government-owned speech corpora that we plan on incorporating into the MTL training project. 

