The AI/Machine Learning/Deep Learning/Neural Network paradigm has surged in the past decade for Automatic Speech Recognition (ASR), Machine Translation  and Text to Speech applications. 
It has taken over the previous statistical methods paradigm. 
However, there is research that shows that in some cases the statistical method remains dominant. 

Reference ASR systems are now trained on tens of thousands of hours of speech data. 
In situations where training data is scarce statistical methods sometimes outperform the best neural network  solutions. 
The Army often finds itself in this situation. 
It requires speech to speech devices that work well for populations of speakers who speak with very special phonological characteristics and it needs to engage with them in conversations  about very special topics. 
In this paper we will investigate a way to leverage the benefits of the latest advances in Neural Networks despite a severe pocity of training data in the target linguistic environment . 
We will compare a Multi-task Learning (MTL) neural network method with the best Single Task Learning (STL) statistical methods applied to ASR under conditions of severely restricted speech and text  resources. 

We set as a goal to produce components of one side of an ASR   system of a speech to speech device for African Accented French and conversations about military issues. 
Our neural network acoustic models will be trained with MTL on speech data from varying numbers of corpora in varying numbers of languages including varying amounts of data from African Accented French. 
Our baseline systems will use the statistical methods including gaussian mixture model (GMM) hidden markov models (HMM) with speaker adaptive training (SAT) and subspace GMMs. 
We will also compare the MTL trained models with other STM trained neural network models. 

So far, I have demonstrated that I can build systems with the MTL method by producing a minimal example that uses only two languages. 
I built SAT GMM HMM acoustic models for Japanese and Mandarin Chinese with the GlobalPhone speech corpus. 
I implemented MTL as follows:
The alignments generated by the Japanese and Mandarin  SAT GMM HMMs were used to train a 5-layer multilingual neural network acoustic model on the combined set of training examples from the Japanese and Mandarin corpora. 
The data from the Mandarin corpus was used to readjust the parameters in the last two layers of the multilingual neural network model to produce the new monolingual Mandarin acoustic model. 
The monolingual Mandarin acoustic models were used to decode a test set. 
Preliminary WER scores showed a slight improvement over the WER scores for the Mandarin SAT GMM HMMs. 
My minimal example did not include a prefinal layer of bottleneck (BN) features and it was not trained using i-vectors. 
BN features and i-vector training have been shown to lower WER scores. 
For our experiments we plan on applying MTL to up to 30 languages and up to 300 hours of speech.
The GlobalPhone corpus has data for 17 languages. 
We also have 9 government-owned speech corpora that we plan on incorporating into the MTL training project. 
Most of the data in these corpora were collected in recitative mode through close mounted microphones. 
We also plan on experimenting with incorporating speech from other sources, such as broadcast news; other speaking styles , such as prompted and spontaneous speech; and other recording conditions, such as noisy  telephone conversations. 
We hypothesize that the MTL method achieves comparable WER scores to the statistical methods for low resource and other specialized conditions. 
Additionally, we claim that the MTL models are more robust, i.e., they achieve comparable performance to the statistical methods for a special target linguistic environment while outperforming them on a broader range of linguistic environments. 
The MTL and Statistical models will get similar WER scores on African Accented French, however, the MTL models will get better WER scores than the statistical models on European French.

We want to answer the following research questions:
- Given a fixed military relevant task and test set, is there a threshold beyond which more training data favors one method over the others?
- Can military relevant ASR tasks under very low resource conditions benefit from the incorporation of speech data from diverse sources into the MTL method? 
For example, can a corpus of broadcast news data be used to improve the ASR component of an African Accented French /English speech to speech device that was previously trained only on read and prompted speech? 
