* <2018-04-13 Fri>
** Goals for Friday set Thursday:
- TODO Multilang: Write Tech Note on librispeech  Tunisian MSA experiment.
I should get results for GALE Arabic run on the Tunisian MSA test set:
Right now I only have tri2b models available:
%WER 36.74 [ 2419 / 6584, 263 ins, 228 del, 1928 sub ] exp/tri2b/decode_Tunisian_MSA_test/wer_14_0.5

- TODO Multilang: Finish librispeech yaounde experiment.
The training and decoding finished last night.
Here is the result:

%WER 45.10 [ 1016 / 2253, 200 ins, 114 del, 702 sub ] exp/multi/yaounde/decode_test/wer_16_0.0

Here are the yaounde results by themselves:
%WER 90.68 [ 2043 / 2253, 80 ins, 728 del, 1235 sub ] exp/tri2b/decode_test/wer_17_1.0
%WER 89.53 [ 2017 / 2253, 139 ins, 439 del, 1439 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 88.37 [ 1991 / 2253, 96 ins, 689 del, 1206 sub ] exp/tri1/decode_test/wer_17_0.5
%WER 87.35 [ 1968 / 2253, 59 ins, 569 del, 1340 sub ] exp/mono/decode_test/wer_14_1.0
%WER 77.76 [ 1752 / 2253, 192 ins, 290 del, 1270 sub ] exp/tri3b/decode_test/wer_17_1.0

These are really bad WERs.

- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.
- DONE Tunisian_MSA: Follow up with Yenda on openslr.org posting request.
It has the identifier slr46.
https://www.openslr.org/resources/46/about.html

** Goals for Next Week:
- TODO Multilang: Write tn in LaTeX.
- TODO Tunisian_MSA: Write kaldi recipe.
- TODO Multilang: librispeech yaounde: write tn for this experiment.
- TODO Multilang: librispeech heroico: write tn for this experiment.

* <2018-04-12 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Get Minimal example working on Librispeech and Yaounde. Look at file: local/nnet3/combine_egs.sh
There were updates in  the 2 scripts udner steps/nnet3/multilingual
babel_multilang uses them.
I made a  copy of combine_egs.sh under local/nnet3 so that I could use it with modifications that I needed.
combine_egs.sh calls the other script allocate_egs.py under steps/nnet3/multilingual
There were recent updates to allocate_egs.py.
My copy of combine_egs.sh calls the allocate_egs.sh that was updated under steps/nnet3/multilingual.
I should have made a copy of allocate_egs.py to avoid problems.
Anyway, I seem to have solved the problem by just using the combine_egs.sh under steps/nnet3/multilingual instead of my copy under local/nnet3.
I had to remove options and add the --num-archives option.
It is now running on the GPU.

- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.
- TODO Tunisian_MSA: Follow up with Yenda on openslr.org posting request.

** Goals for Friday:
- TODO Multilang: Write Tech Note on librispeech  Tunisian MSA experiment.
- TODO Multilang: Finish librispeech yaounde experiment.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.
- TODO Tunisian_MSA: Follow up with Yenda on openslr.org posting request.

* <2018-04-11 Wed>
** Goals for Wednesday set Tuesday:
- DONE Multilang: Get a minimal example working.

Tasks:
1. Librispeech: 960 hours of read English.
2. Tunisian_MSA: 10 hours of recitations and answers to questions in Modern Standard Arabic spoken by Tunisians

Tested on 4 speakers, 3 Libyan males and 1 Tunisian female.

Tunisian_MSA Baseline:
%WER 11.03 [ 726 / 6584, 61 ins, 237 del, 428 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_13_0.0

Test Results:
%WER 7.12 [ 469 / 6584, 100 ins, 124 del, 245 sub ] exp/multi/Tunisian_MSA/decode_test/wer_13_0.0

- DONE Tunisian MSA: Package corpus for openslr.org.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.

** Goals for Thursday:
- TODO Multilang: Get Minimal example working on Librispeech and Yaounde. Look at file: local/nnet3/combine_egs.sh
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.
- TODO Tunisian_MSA: Follow up with Yenda on openslr.org posting request.

* <2018-04-10 Tue>
**  Goals for Tuesday set Monday:
- TODO Multilang: Build without hires features.
I am running the mtl build with 4 languages:
1. Librispeech English
2. Tunisian MSA
3. Heroico Spanish
4. Yaounde African Accented French.

This setup fails miserably.


I started a new build with only:
1. librispeech
2. Tunisian MSA

The build should be done tomorrow morning.

- TODO Tunisian MSA: Package corpus for openslr.org.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.

** Goals for Wednesday:
- TODO Multilang: Get a minimal example working.
- TODO Tunisian MSA: Package corpus for openslr.org.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.
- TODO A-Team GPU workstation: Install my fork of kaldi on this machine.

* <2018-04-09 Mon>
** Goals for Next Week:
- TODO Multilang: Use hires features in training and testing.
This gave really bad results.
I am going to try to completely remove hires from the builds.

- TODO Tunisian MSA: Package corpus for openslr.org.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.

** Goals for Tuesday:
- TODO Multilang: Build without hires features.
- TODO Tunisian MSA: Package corpus for openslr.org.
- TODO Heroico: Make a pull request  to incorporate openslr.org changes into kaldi master repo
- TODO Multilang: Do some writing with Michelle.

* <2018-04-06 Fri>
** Goals for Friday set Thursday:
- TODO Multilang: Tune, Evaluate and analyze latest system trained on librispeech, heroico and Tunisian MSA.
here are the results I get for Tunisian MSA:
%WER 7.87 [ 518 / 6584, 109 ins, 131 del, 278 sub ] exp/multi/Tunisian_MSA/decode_test/wer_13_0.0
This is tested on 444 utterances from 4 speakers.
cls, lfi, mbt, and srj.

Here are the results for the Tunisian MSA system alone:
%WER 20.96 [ 1380 / 6584, 144 ins, 360 del, 876 sub ] exp/mono/decode_test/wer_12_0.0
%WER 19.14 [ 1260 / 6584, 208 ins, 207 del, 845 sub ] exp/tri3b/decode_test.si/wer_16_0.0
%WER 16.02 [ 1055 / 6584, 253 ins, 186 del, 616 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 13.82 [ 910 / 6584, 174 ins, 192 del, 544 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 12.11 [ 797 / 6584, 62 ins, 274 del, 461 sub ] exp/chain/tdnn1a_sp/decode_test/wer_13_0.0
%WER 11.13 [ 733 / 6584, 177 ins, 132 del, 424 sub ] exp/tri3b/decode_test/wer_17_0.0
%WER 11.03 [ 726 / 6584, 61 ins, 237 del, 428 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_13_0.0

| model | WER |
| mono | 20.96 |
| tri1 | 16.02 |
| tri2b | 13.82 |
| tri3b | 11.13 |
| chain | 12.11 |
| chain online | 11.03 |
| MTL librispeech heroico Tunisian MSA | 7.87 |

- High Resolution Features.
I have to take a step back because I found that I am not using high resolution features in training and testing the neural network.
I have been extracting the hires features, but I have not been using them. 
I spent the afternoon trying to fix this problem.
The low resolution features are only used to extract alignments.
I have  moved to plp pitch features for the low resolution feature extraction because the babel scripts were doing this.
But now I'm not sure it is worth the trouble.

- TODO Heroico: Modify scripts to incorporate into mtl training.
- DONE Heroico: Modify scripts to handle downloading corpus from openslr.org.
I put this on my fork of kaldi.
https://github.com/johnjosephmorgan/kaldi.git
- DONE Heroico: Update scripts in my kaldi fork.
- TODO Tunisian MSA: Package corpus for submitting to openslr.org.
- TODO Multilang: Get builds to run on a-team GPU workstation.
- TODO Kaldi: Install new version on B-Team and A-team GPU workstations and on my laptop.

** Goals for Next Week:
- TODO Multilang: Use hires features in training and testing.
 
* <2018-04-05 Thu>
**  Goals for Thursday set Wednesday:
- TODO Multilang: Run small build on A-Team GPU machine.
- TODO Tunisian MSA: Package data for openslr.org.

- Multilang: Minimal example:

Source Languages:
1. Librispeech English Read  (960 hours)
2. Heroico Spanish (10 hours)

Results on USMA test set :
%WER 58.50 [ 4475 / 7650, 779 ins, 485 del, 3211 sub ] exp/multi/heroico/decode_devtest/wer_7_1.0
%WER 57.67 [ 5314 / 9215, 896 ins, 486 del, 3932 sub ] exp/multi/heroico/decode_nonnative/wer_7_1.0
%WER 55.41 [ 9261 / 16713, 1614 ins, 817 del, 6830 sub ] exp/multi/heroico/decode_test/wer_7_1.0
%WER 52.81 [ 3960 / 7498, 634 ins, 385 del, 2941 sub ] exp/multi/heroico/decode_native/wer_8_1.0


| fold | heroico Chain  WER | librispeech heroico  MTL WER |
| devtest | 52.21 | 58.50 |
| native | 53.43 | 52.81 |
| nonnative | 61.03 | 57.67 |
| test | 57.70 | 55.41 |

** Goals for Friday:
- TODO Multilang: Tune, Evaluate and analyze latest system trained on librispeech, heroico and Tunisian MSA.
- TODO Heroico: Modify scripts to incorporate into mtl training.
- TODO Heroico: Modify scripts to handle downloading corpus from openslr.org.
- TODO Heroico: Update scripts in my kaldi fork.
- TODO Tunisian MSA: Package corpus for submitting to openslr.org.
- TODO Multilang: Get builds to run on a-team GPU workstation.
- TODO Kaldi: Install new version on B-Team and A-team GPU workstations and on my laptop.

* <2018-04-04 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Tunisian MSA: Package data for openslr.org.
- TODO Tunisian MSA: Tune chain models.
Here are the results after lowering epochs from 10 to 6.
%WER 20.96 [ 1380 / 6584, 144 ins, 360 del, 876 sub ] exp/mono/decode_test/wer_12_0.0
%WER 20.03 [ 1319 / 6584, 213 ins, 206 del, 900 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 16.02 [ 1055 / 6584, 253 ins, 186 del, 616 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 14.82 [ 976 / 6584, 183 ins, 184 del, 609 sub ] exp/tri2b/decode_test/wer_16_0.0
%WER 13.47 [ 887 / 6584, 91 ins, 264 del, 532 sub ] exp/chain/tdnn1a_sp/decode_test/wer_11_0.0
%WER 12.36 [ 814 / 6584, 91 ins, 237 del, 486 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_11_0.0
%WER 11.95 [ 787 / 6584, 189 ins, 131 del, 467 sub ] exp/tri3b/decode_test/wer_17_0.0

| model | WER     |
| mono  | 20.96 |
| tri1 | 16.02 |
| tri2b | 14.82 |
| tri3b | 11.95 |
| chain | 13.47 |
| chain online | 12.36 |

so there was a slight improvement, but we still  do not beat the tri3b models.

I lowered leaves and gaussians to 400 and 4000 and I got  the following for tri2b:
%WER 16.02 [ 1055 / 6584, 251 ins, 225 del, 579 sub ] exp/tri2b/decode_test/wer_17_0.0
I lowered leaves and gaussians to 500 and 4000 and I got  the following for tri2b:
%WER 15.51 [ 1021 / 6584, 168 ins, 225 del, 628 sub ] exp/tri2b/decode_test/wer_17_0.0
I set leaves and gaussians to 600 and 4000 and I got  the following for tri2b:
%WER 15.36 [ 1011 / 6584, 188 ins, 189 del, 634 sub ] exp/tri2b/decode_test/wer_16_0.0
I set leaves and gaussians to 700 and 4000 and I got  the following for tri2b:
%WER 16.86 [ 1110 / 6584, 272 ins, 198 del, 640 sub ] exp/tri2b/decode_test/wer_17_0.0

| leaves | tri2b WER |
| 400 | 16.02 |
| 500 | 15.51 |
| 550 | 16.10 |
| 600 | 15.36 |
| 700  | 16.86 |

I set the number of gaussians to 10000 and I got the following for tri2b:
%WER 13.90 [ 915 / 6584, 175 ins, 184 del, 556 sub ] exp/tri2b/decode_test/wer_17_0.0
I set the number of gaussians to 20000 and I got the following for tri2b:
%WER 13.82 [ 910 / 6584, 174 ins, 192 del, 544 sub ] exp/tri2b/decode_test/wer_17_0.0
I set the number of gaussians to 25000 and I got the following for tri2b:
%WER 14.52 [ 956 / 6584, 191 ins, 193 del, 572 sub ] exp/tri2b/decode_test/wer_16_0.0
I set the number of gaussians to 15000 and I got the following for tri2b:
%WER 14.47 [ 953 / 6584, 180 ins, 197 del, 576 sub ] exp/tri2b/decode_test/wer_17_0.0

Here are all the results so far:
%WER 20.96 [ 1380 / 6584, 144 ins, 360 del, 876 sub ] exp/mono/decode_test/wer_12_0.0
%WER 20.41 [ 1344 / 6584, 208 ins, 213 del, 923 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 16.02 [ 1055 / 6584, 253 ins, 186 del, 616 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 14.06 [ 926 / 6584, 169 ins, 197 del, 560 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 13.47 [ 887 / 6584, 91 ins, 264 del, 532 sub ] exp/chain/tdnn1a_sp/decode_test/wer_11_0.0
%WER 12.36 [ 814 / 6584, 91 ins, 237 del, 486 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_11_0.0
%WER 11.62 [ 765 / 6584, 177 ins, 138 del, 450 sub ] exp/tri3b/decode_test/wer_17_0.0

The tri3b got better.


- TODO SofTunisia: Tune tri2b models.
Here are the results:
%WER 9.81 [ 915 / 9323, 178 ins, 159 del, 578 sub ] exp/tri3b/decode_test/wer_17_0.0
%WER 8.71 [ 812 / 9323, 87 ins, 243 del, 482 sub ] exp/chain/tdnn1a_sp/decode_test/wer_11_0.0
%WER 8.27 [ 771 / 9323, 94 ins, 221 del, 456 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_11_0.0
%WER 18.31 [ 1707 / 9323, 219 ins, 320 del, 1168 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 15.95 [ 1487 / 9323, 134 ins, 397 del, 956 sub ] exp/mono/decode_test/wer_12_0.0
%WER 13.19 [ 1230 / 9323, 240 ins, 225 del, 765 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 12.77 [ 1191 / 9323, 178 ins, 259 del, 754 sub ] exp/tri2b/decode_test/wer_17_0.0

| model | WER |
| mono | 15.95 |
| tri1 | 13.19 |
| tri2b | 12.77 |
| tri3b | 9.81 |
| chain | 8.71 |
| chain online | 8.27 |

- TODO Multilang: Back off to fewer languages  and get them to work well before moving on.

** Goals for Thursday:
- TODO Multilang: Run small build on A-Team GPU machine.
- TODO Tunisian MSA: Package data for openslr.org.

* <2018-04-03 Tue>
**  Goals for Tuesday set Monday:
- TODO Tunisian MSA: Run recipe with Zac's changes through chain models.
Here are the results:
%WER 20.96 [ 1380 / 6584, 144 ins, 360 del, 876 sub ] exp/mono/decode_test/wer_12_0.0
%WER 20.03 [ 1319 / 6584, 213 ins, 206 del, 900 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 16.02 [ 1055 / 6584, 253 ins, 186 del, 616 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 14.82 [ 976 / 6584, 183 ins, 184 del, 609 sub ] exp/tri2b/decode_test/wer_16_0.0
%WER 13.68 [ 901 / 6584, 106 ins, 272 del, 523 sub ] exp/chain/tdnn1a_sp/decode_test/wer_12_0.0
%WER 12.83 [ 845 / 6584, 109 ins, 239 del, 497 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_12_0.0
%WER 11.95 [ 787 / 6584, 189 ins, 131 del, 467 sub ] exp/tri3b/decode_test/wer_17_0.0

| model | WER     |
| mono  | 20.96 |
| tri1 | 16.02 |
| tri2b | 14.82 |
| tri3b | 11.95 |
| chain | 13.68 |
| chain online | 12.83 |


I nned to tune the chain models.

- TODO multilang: Decode with latest models.
- DONE Paper: Meet with Michelle at 9 am.
- TODO Mac Enterprise Machine: Meet in the afternoon with Andrew and Justin to install Mac Mini.
Looks like this won't happen today.


** Goals for Wednesday:
- TODO Tunisian MSA: Package data for openslr.org.
- TODO Tunisian MSA: Tune chain models.
- TODO SofTunisia: Tune tri2b models.
- TODO Multilang: Back off to fewer languages  and get them to work well before moving on.

* <2018-04-02 Mon>
** Goals for April:
- TODO Multilang: Write a paper or a report.
- TODO Tunisian_MSA: Publish recipe on kaldi repo.
- TODO Tunisian_MSA: Incorporate Zac's work into the current recipe.
Zac wrote a script for the Answers Test.
I need to incorporate this into the run.sh script.

** Goals for Tuesday:
- TODO Tunisian MSA: Run recipe with Zac's changes through chain models.
- TODO multilang: Decode with latest models.
- TODO Paper: Meet with Michelle at 9 am.
- TODO Mac Enterprise Machine: Meet in the afternoon with Andrew and Justin to install Mac Mini.

* <2018-03-23 Fri>
** Goals for Friday:
- TODO Multilang: Setup training run with many languages so it runs while I'm gone.

** Goals for April:
- TODO Tunisian_MSA: Incorporate Zac's work into the current recipe.
- TODO Multilang: Write a report.

* 
* <2018-03-22 Thu>
** Goals for Wednesday set Tuesday:
- TODO Multilang: Setup training run with many languages so it runs while I'm gone.

** Goals for Friday:
- TODO Multilang: Setup training run with many languages so it runs while I'm gone.

* <2018-03-20 Tue>
** Goals for Tuesday set Monday:
- TODO Librispeech: Run the kaldi recipe.
- TODO Librispeech: Incorporate into multilang.
- TODO Multilang: Go end to end with the added corpora.

** Goals for Wednesday:
- TODO Multilang: Setup training run with many languages so it runs while I'm gone.

* <2018-03-19 Mon>
** Goals for this Week:
- TODO Multilang: Go end to end with minimal example.
I am incorporating more data, but I still consider this a minimal example.
The recipe is a minimal example.
I am trying to incorporate as much as possible into the run.sh script file.
I am not going to use  iVectors for this minimal example.
Eventually, I want to use only corpora that available from openslr.org.
- TODO Multilang: Incorporate gp_arabic into minimal example.

** Goals for Tuesday:
- TODO Librispeech: Run the kaldi recipe.
- TODO Librispeech: Incorporate into multilang.
- TODO Multilang: Go end to end with the added corpora.

* DAR <2018-03-16 Fri>
** Goals for Friday set Thursday:
- TODO Multilang: Go end to end with minimal example.
- TODO Multilang: Incorporate gp_arabic into minimal example.

* DAR <2018-03-15 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Get minimal example running end 2 end.
I am at the ivector extraction stage.

- DONE CQL Student: Interview Karl.

** Goals for Friday:
- TODO Multilang: Go end to end with minimal example.
- TODO Multilang: Incorporate gp_arabic into minimal example.

* DAR <2018-03-14 Wed>
Goals for Thursday:
- TODO Multilang: Get minimal example running end 2 end.
- TODO CQL Student: Interview Karl.

* DAR <2018-03-08 Thu>
** Goals for Thursday set Tuesday:
- DONE CRADA Meeting at 10 am.
- TODO Listen to Steve's practice talk.
- TODO gp_arabic: Tune tri3b.

I succeeded at making tri3b better than tri2b.

Here are all the dev WERs:
%WER 61.09 [ 5524 / 9043, 351 ins, 1134 del, 4039 sub ] exp/mono/decode_dev/wer_17_0.0
%WER 57.67 [ 5215 / 9043, 482 ins, 756 del, 3977 sub ] exp/tri1/decode_dev/wer_17_0.5
%WER 56.78 [ 5135 / 9043, 537 ins, 614 del, 3984 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 56.24 [ 5086 / 9043, 519 ins, 653 del, 3914 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 55.69 [ 5036 / 9043, 536 ins, 595 del, 3905 sub ] exp/tri3b/decode_dev/wer_17_1.0

Here are the eval WERs:

%WER 40.58 [ 6692 / 16489, 604 ins, 1216 del, 4872 sub ] exp/mono/decode_eval/wer_17_0.0s
%WER 37.50 [ 6183 / 16489, 766 ins, 828 del, 4589 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 37.11 [ 6119 / 16489, 920 ins, 662 del, 4537 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 36.01 [ 5938 / 16489, 863 ins, 704 del, 4371 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 35.53 [ 5859 / 16489, 923 ins, 542 del, 4394 sub ] exp/tri3b/decode_eval/wer_17_1.0

| model | fold | WER |
| mono | dev | 61.09 |
| mono | eval | 40.58 |
| tri1 | dev | 57.67 |
| tri1 | eval | 37.50 |
| tri2b | dev | 56.24 |
| tri2b | eval | 36.01 |
| tri3b | dev | 55.69 |
| tri3b | eval | 35.53 |
| chain tdnn | 34.86 |
| chain tdnn online | 34.70 |

Slightly better WER with chain models:
%WER 61.09 [ 5524 / 9043, 351 ins, 1134 del, 4039 sub ] exp/mono/decode_dev/wer_17_0.0
%WER 57.67 [ 5215 / 9043, 482 ins, 756 del, 3977 sub ] exp/tri1/decode_dev/wer_17_0.5
%WER 56.78 [ 5135 / 9043, 537 ins, 614 del, 3984 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 56.24 [ 5086 / 9043, 519 ins, 653 del, 3914 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 55.69 [ 5036 / 9043, 536 ins, 595 del, 3905 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 51.64 [ 4670 / 9043, 201 ins, 2073 del, 2396 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_17_0.0
%WER 51.50 [ 4657 / 9043, 234 ins, 1917 del, 2506 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_15_0.0
%WER 40.58 [ 6692 / 16489, 604 ins, 1216 del, 4872 sub ] exp/mono/decode_eval/wer_17_0.0
%WER 37.50 [ 6183 / 16489, 766 ins, 828 del, 4589 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 37.11 [ 6119 / 16489, 920 ins, 662 del, 4537 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 36.01 [ 5938 / 16489, 863 ins, 704 del, 4371 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 35.53 [ 5859 / 16489, 923 ins, 542 del, 4394 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 34.86 [ 5748 / 16489, 437 ins, 1183 del, 4128 sub ] exp/chain/tdnn1c_sp/decode_eval/wer_16_0.5
%WER 34.70 [ 5722 / 16489, 516 ins, 1028 del, 4178 sub ] exp/chain/tdnn1c_sp_online/decode_eval/wer_17_0.0

- TODO Multilang: Go end to end with single run.sh script.
I worked a lot on this today.
I am making slow progress.
Ivectors were  missing
Ivector dimension was a problem.
At the end of the day I am at the raw nnet training stage.
The dimension of the data and the dimension specification in the config file are different.
I need to fix this.

- TODO Babel: Investigate Guarani

** Goals for Friday:
- TODO Multilang: Figure out why there is a dimension mismatch between the egs and the confi file.
- TODO Multilang: Go end to end with my new run.sh script.
- TODO GP Arabic: Incorporate GP Arabic back into the multilang build.
- TODO Babel: Investigate Guarani (upsampling?).

* DAR <2018-03-07 Wed>
Goals for Tuesday set Monday:
- DONE ARL Colloquium: Write slides on MTL.
- DONE ARL-Colloquium: Post slides.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

- GP Arabic:
Here are the current results:
%WER 61.00 [ 5516 / 9043, 318 ins, 1086 del, 4112 sub ] exp/mono/decode_dev/wer_17_0.0
%WER 57.33 [ 5184 / 9043, 615 ins, 486 del, 4083 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 57.09 [ 5163 / 9043, 486 ins, 662 del, 4015 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 56.23 [ 5085 / 9043, 614 ins, 459 del, 4012 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 55.89 [ 5054 / 9043, 518 ins, 576 del, 3960 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 45.84 [ 7559 / 16489, 503 ins, 1206 del, 5850 sub ] exp/mono/decode_eval/wer_16_0.0
%WER 42.87 [ 7069 / 16489, 912 ins, 540 del, 5617 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 42.21 [ 6960 / 16489, 694 ins, 703 del, 5563 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 41.45 [ 6835 / 16489, 902 ins, 437 del, 5496 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 41.19 [ 6792 / 16489, 732 ins, 606 del, 5454 sub ] exp/tri2b/decode_eval/wer_17_1.0

I am going to try to improve these scores.
I am experimenting with removing the word position dependence in the phone modeling.

- Babel: 
I looked at the Babel Guarani corpus.
It looks like it's all 8k and alaw?
Should I work with this?

** Goals for Thursday:
- TODO CRADA Meeting at 10 am.
- TODO Listen to Steve's practice talk.
- TODO gp_arabic: Tune tri3b.
- TODO Multilang: Go end to end with single run.sh script.
- TODO Babel: Investigate Guarani

* DAR <2018-03-06 Tue>
** Goals for Tuesday set Monday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL-Colloquium: Post slides.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

* DAR <2018-03-05 Mon>
**  Goals for Friday set Monday:
No work Friday because of weather.
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO ARL-Colloquium: Post slides.
- DONE ARL-Colloquium: Meet with Phil David to discuss slides.
I had a good meeting with Phil.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

** Goals for Tuesday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL-Colloquium: Post slides.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

* DAR <2018-03-01 Thu>
**  Goals for Thursday set Wednesday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- DONE Meet with candidate at 9 am.
- DONE GXM phone meeting 11 am.

** Goals for Friday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO ARL-Colloquium: Post slides.
- TODO ARL-Colloquium: Meet with Phil David to discuss slides.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

* DAR <2018-02-28 Wed>
**  Goals for Wednesday set Tuesday:
- DONE ARL Colloquium: Meet with Judith and Michelle to practice.
Got good feedback from both.
- DONE ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
Hazrat says he  sent them to me.
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO Computer: Get JAWS installed.
Reggie is working on get this done.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

** Goals for Thursday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- TODO Meet with candidate at 9 am.
- TODO GXM phone meeting 11 am.

* DAR <2018-02-27 Tue>
**  Goals for Tuesday set Monday:
- TODO Multilang: Fix the command that prepares the multilingual examples.
-TODO Tunisian MSA: Get this running end to end all the way to chain models.
%WER 76.09 [ 5010 / 6584, 169 ins, 996 del, 3845 sub ] exp/mono/decode_test/wer_13_1.0
%WER 72.08 [ 4746 / 6584, 341 ins, 558 del, 3847 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 70.03 [ 4611 / 6584, 284 ins, 661 del, 3666 sub ] exp/tri1/decode_test/wer_17_1.0
%WER 68.67 [ 4521 / 6584, 304 ins, 589 del, 3628 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 66.89 [ 4404 / 6584, 376 ins, 471 del, 3557 sub ] exp/tri3b/decode_test/wer_17_1.0
%WER 66.30 [ 4365 / 6584, 125 ins, 910 del, 3330 sub ] exp/chain/tdnn1a_sp/decode_test/wer_10_1.0
%WER 65.64 [ 4322 / 6584, 212 ins, 657 del, 3453 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_9_0.5


| model | WER |
| mono | 76.09 |
| tri1 | 70.03 |
| tri2b | 68.67 |
| tri3b | 66.89 |
| chain tdnn | 66.30 |
| chain tdnn online | 65.64 |

- TODO GALE Arabic: Get this working well.
%WER 58.17 [ 40523 / 69668, 1791 ins, 9294 del, 29438 sub ] exp/mono/decode/wer_12_0.0
%WER 39.58 [ 27578 / 69668, 2040 ins, 6382 del, 19156 sub ] exp/tri1/decode/wer_15_0.5
%WER 38.25 [ 26645 / 69668, 2102 ins, 6090 del, 18453 sub ] exp/tri2a/decode/wer_15_0.5
%WER 35.83 [ 24962 / 69668, 2339 ins, 5303 del, 17320 sub ] exp/tri2b/decode/wer_14_0.5
%WER 35.53 [ 24756 / 69668, 2399 ins, 5009 del, 17348 sub ] exp/tri3b/decode.si/wer_14_0.5
%WER 33.66 [ 23451 / 69668, 2503 ins, 4815 del, 16133 sub ] exp/tri3b/decode/wer_15_1.0

- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

** Goals for Wednesday:
- TODO ARL Colloquium: Meet with Judith and Michelle to practice.
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write slides explaining invariants.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

* DAR <2018-02-26 Mon>
**  Goals for Next Week set last Friday:
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
Hazrat and Steve are helping me with this.
- TODO ARL Colloquium: Write slides on MTL.
I worked on understanding the spectogram patterns that are going to be modeled.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- TODO Multilang: Work on a minimal example with Tunisian_MSA and Libyan_MSA.
Libyan MSA is unfortunately too small for the current set up.
I am going back to work with Gale Arabic and Tunisian MSA.


** Goals for Tuesday:
- TODO Multilang: Fix the command that prepares the multilingual examples.
-TODO Tunisian MSA: Get this running end to end all the way to chain models.
- TODO GALE Arabic: Get this working well.
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO ARL Colloquium: Write sllides explaining invarints.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

* DAR <2018-02-23 Fri>
**  Goals for Friday set Thursday:
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO Computer: Get JAWS installed.
I contacted the help desk.

- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- DONE GALE Arabic: What is going on with the feature extraction?
I think the only problem was that /mnt/corpora had gone stale on the B-team GPU machine.
Justin refreshed it and things seem to be working now.

- TODO Multilang: Simplify scripts.
This is moving forward.
- TODO Multilang: Run with only Gale Arabic and SOFTunisia.
%WER 4.90 [ 202 / 4125, 18 ins, 57 del, 127 sub ] exp/nnet3/multi_bnf/softunisia/decode_test/wer_17_0.0
Recall that when I ran this with  the 3 corpora: Gp Arabic, Gale Arabic and Softunisia I got: 
%WER 4.61 [ 190 / 4125, 21 ins, 48 del, 121 sub ] exp/nnet3/multi_bnf/softunisia/decode_test/wer_17_0.0

So GP Arabic actually helped slightly.

- TODO Tunisian_MSA: Run recipe and get results to Zac.
%WER 76.26 [ 5021 / 6584, 197 ins, 928 del, 3896 sub ] exp/mono/decode_test/wer_12_1.0
%WER 72.45 [ 4770 / 6584, 343 ins, 556 del, 3871 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 70.44 [ 4638 / 6584, 285 ins, 664 del, 3689 sub ] exp/tri1/decode_test/wer_17_1.0
%WER 69.11 [ 4550 / 6584, 306 ins, 592 del, 3652 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 67.33 [ 4433 / 6584, 378 ins, 472 del, 3583 sub ] exp/tri3b/decode_test/wer_17_1.0
%WER 66.78 [ 4397 / 6584, 164 ins, 799 del, 3434 sub ] exp/chain/tdnn1a_sp/decode_test/wer_9_1.0
%WER 65.90 [ 4339 / 6584, 144 ins, 841 del, 3354 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_10_1.0

| model | WER |
| mono | 76.26 |
| tri1 | 70.44 |
| tri2b | 69.11 |
| tri3b | 67.33 |
| chain tdnn | 66.78 |
| chain tdnn online | 65.90 |

- TODO Tunisian_MSA: Check for OOVs and get them to ZAc.

** Goals for Next Week:
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- TODO Multilang: Work on a minimal example with Tunisian_MSA and Libyan_MSA.

* DAR <2018-02-22 Thu>
** Goals for Thursday set Wednesday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Write recipe without Government test set.
Here are the WER scores:
%WER 79.38 [ 4123 / 5194, 175 ins, 689 del, 3259 sub ] exp/mono/decode_test/wer_14_0.5
%WER 74.99 [ 3895 / 5194, 297 ins, 428 del, 3170 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 73.64 [ 3825 / 5194, 250 ins, 493 del, 3082 sub ] exp/tri1/decode_test/wer_17_1.0
%WER 73.22 [ 3803 / 5194, 243 ins, 510 del, 3050 sub ] exp/tri2b/decode_test/wer_17_0.5
%WER 71.64 [ 3721 / 5194, 351 ins, 357 del, 3013 sub ] exp/tri3b/decode_test/wer_17_1.0
%WER 71.33 [ 3705 / 5194, 117 ins, 753 del, 2835 sub ] exp/chain/tdnn1a_sp/decode_test/wer_10_1.0
%WER 70.10 [ 3641 / 5194, 122 ins, 662 del, 2857 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_10_1.0

| model | WER |
| mono | 79.38 |
| tri1 | 73.64 |
| tri2b | 73.22 |
| tri3b | 71.64 |
| chain | 71.33 |
| chain online | 70.10 |

I was not using the Tunisian Female test speaker.
I incorporated her into the test set today.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.

- TODO AIShell: Chain models. Did they finish training?
Yes.
I only got dev scores for some models so far.
It was set up to test hires data.
Here are the WER scores.
%WER 47.16 [ 30384 / 64428, 1905 ins, 4888 del, 23591 sub ] exp/mono/decode_test/wer_12_0.0
%WER 42.41 [ 54161 / 127698, 3394 ins, 8668 del, 42099 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 31.23 [ 20118 / 64428, 1617 ins, 3165 del, 15336 sub ] exp/tri1/decode_test/wer_13_0.5
%WER 31.07 [ 20020 / 64428, 1659 ins, 3097 del, 15264 sub ] exp/tri2/decode_test/wer_14_0.5
%WER 30.95 [ 19942 / 64428, 1640 ins, 2896 del, 15406 sub ] exp/tri4a/decode_test.si/wer_17_0.5
%WER 28.28 [ 18220 / 64428, 1530 ins, 2761 del, 13929 sub ] exp/tri3a/decode_test/wer_15_0.5
%WER 27.94 [ 18002 / 64428, 1772 ins, 2336 del, 13894 sub ] exp/tri5a/decode_test.si/wer_14_0.5
%WER 27.26 [ 34811 / 127698, 3101 ins, 5101 del, 26609 sub ] exp/tri1/decode_dev/wer_13_0.0
%WER 27.07 [ 34562 / 127698, 3231 ins, 4869 del, 26462 sub ] exp/tri2/decode_dev/wer_13_0.0
%WER 26.73 [ 34129 / 127698, 3406 ins, 4259 del, 26464 sub ] exp/tri4a/decode_dev.si/wer_13_0.0
%WER 24.75 [ 31605 / 127698, 2940 ins, 4270 del, 24395 sub ] exp/tri3a/decode_dev/wer_14_0.0
%WER 23.92 [ 15413 / 64428, 1530 ins, 2137 del, 11746 sub ] exp/tri4a/decode_test/wer_15_0.5
%WER 23.85 [ 30462 / 127698, 3072 ins, 3678 del, 23712 sub ] exp/tri5a/decode_dev.si/wer_13_0.0
%WER 22.04 [ 14203 / 64428, 1408 ins, 1998 del, 10797 sub ] exp/tri5a/decode_test/wer_15_0.5
%WER 21.48 [ 27432 / 127698, 2821 ins, 3507 del, 21104 sub ] exp/tri4a/decode_dev/wer_14_0.0
%WER 19.53 [ 24940 / 127698, 2591 ins, 3183 del, 19166 sub ] exp/tri5a/decode_dev/wer_14_0.0
%WER 16.78 [ 10814 / 64428, 1157 ins, 1438 del, 8219 sub ] exp/chain/tdnn_1a_sp/decode_test_hires/wer_13_0.0

| Model | fold | WER |
| mono | dev | 42.41 |
| mono | test | 47.16 |

- TODO GALE Arabic: What is going on with the feature extraction?
- TODO Multilang: Did the build finish?
Yes,.
** Goals for Friday:
- TODO ARL-Colloquium: Get spectogram for some phonemes from Hazrat.
- TODO ARL Colloquium: Write slides on MTL.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- TODO GALE Arabic: What is going on with the feature extraction?
- TODO Multilang: Simplify scripts.
- TODO Tunisian_MSA: Run recipe and get results to Zac.
- TODO Tunisian_MSA: Check for OOVs and get them to ZAc.

* DAR <2018-02-21 Wed>
** Goals for Wednesday set Tuesday:
-TODO ARL Colloquium: Write slides on MTL.
- TODO AIShell: Run chain models.
This is running.
The following settings were problematic:
num_jobs_initial=2
num_jobs_final=12
I reset them to:
num_jobs_initial=1
num_jobs_final=1

Before resetting, the GPU would throw an out of memory error.


- TODO Multilang: Run with only Gale Arabic and SOFTunisia.
It looks like this is currently running on the GPU.
- DONE: SOFTunisia: Incorporate Zac's corrections to the lexicon.

- DONE SOFTunisia: Incorporate Zac's corrections to the reference transcripts.

- Multilang On GALE Arabic test:
%WER 24.39 [ 16991 / 69668, 1853 ins, 3378 del, 11760 sub ] exp/nnet3/multi_bnf/my_gale_arabic/decode_test/wer_11_0.5

I need to get the chain model WER for GALE Arabic.

** Goals for Thursday:
- TODO ARL Colloquium: Write slides on MTL.
- TODO Computer: Get JAWS installed.
- TODO Tunisian_MSA: Prepare data for transfer to openslr.org.
- TODO Tunisian_MSA: Write recipe without Government test set.
- TODO Tunisian_MSA: Contact Yenda about submitting recipe to kaldi repo.
- TODO AIShell: Chain models. Did they finish training?
- TODO GALE Arabic: What is going on with the feature extraction?
- TODO Multilang: Did the build finish?

* DAR <2018-02-20 Tue>
** Goals for Next Week:
- TODO ARL-Colloquium: Write slides on AI in ASR.
- TODO AIShell: Incorporate into multilang.
- TODO AISHELL: Build chain models.
I am trying to get the kaldi scripts to run.
I am trying to add _hires to the online ivector directory path.
It looks like the nnet3 tdnn script is running.
Later, I have to get the chain model script running.
I have this running for chain models.

- TODO  EESEN: Get it installed on the b-team  workstation.

- Multilan Softunisia:
%WER 51.30 [ 4639 / 9043, 341 ins, 796 del, 3502 sub ] exp/nnet3/multi_bnf/gp_arabic/decode_dev/wer_16_0.0
%WER 4.61 [ 190 / 4125, 21 ins, 48 del, 121 sub ] exp/nnet3/multi_bnf/softunisia/decode_test/wer_17_0.0

I ran multilang again on the following 3 corpora:
1. GALE Arabic
2. Global Phone Arabic
3. Sof Tunisia

Do I have chain model results for these 3 recipes separately?

Here are the results for Softunisia:

Single Task Learning stl:

%WER 9.36 [ 386 / 4125, 8 ins, 145 del, 233 sub ] exp/chain/tdnn1c_sp/decode_test/wer_16_0.0
%WER 8.22 [ 339 / 4125, 7 ins, 106 del, 226 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_17_0.0
%WER 24.00 [ 990 / 4125, 47 ins, 283 del, 660 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 15.42 [ 636 / 4125, 19 ins, 218 del, 399 sub ] exp/mono/decode_test/wer_17_0.0
%WER 15.30 [ 631 / 4125, 19 ins, 202 del, 410 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 15.08 [ 622 / 4125, 31 ins, 171 del, 420 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 12.95 [ 534 / 4125, 37 ins, 138 del, 359 sub ] exp/tri3b/decode_test/wer_17_0.0

mtl:
%WER 4.61 [ 190 / 4125, 21 ins, 48 del, 121 sub ] exp/nnet3/multi_bnf/softunisia/decode_test/wer_17_0.0

| model | WER |
| mono | 15.42 |
| tri1 | 15.08 |
| tri2b | 15.30 |
| tri3b | 12.95 |
| chain online | 8.22 |
| mtl | 4.61 |

I think I can get better scores by tuning the tri2b models.

I did some triphone tuning and  got the following results:

%WER 8.70 [ 359 / 4125, 29 ins, 84 del, 246 sub ] exp/tri3b/decode_test/wer_16_0.0
%WER 8.34 [ 344 / 4125, 10 ins, 105 del, 229 sub ] exp/chain/tdnn1c_sp/decode_test/wer_14_0.0
%WER 7.30 [ 301 / 4125, 15 ins, 69 del, 217 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_11_0.0
%WER 16.63 [ 686 / 4125, 27 ins, 174 del, 485 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 15.42 [ 636 / 4125, 19 ins, 218 del, 399 sub ] exp/mono/decode_test/wer_17_0.0
%WER 12.78 [ 527 / 4125, 29 ins, 162 del, 336 sub ] exp/tri1/decode_test/wer_16_0.0
%WER 12.22 [ 504 / 4125, 23 ins, 148 del, 333 sub ] exp/tri2b/decode_test/wer_16_0.0

| model | WER |
| mono |  15.42 |
| tri1 | 12.78 |
| tri2b | 12.22 |
| tri3b | 8.70      |
| chain | 8.34 |
| chain online |  7.30 |
| mtl |  |

** Goals for Wednesday:
-TODO ARL Colloquium: Write slides on MTL.
- TODO AIShell: Run chain models.
- TODO Multilang: Run with only Gale Arabic and SOFTunisia.
- TODO: SOFTunisia: Incorporate Zac's corrections to the lexicon.
- TODO SOFTunisia: Incorporate Zac's corrections to the reference transcripts.

* DAR <2018-02-16 Fri>
**  Goals for Friday set Thursday:
- TODO Libyan: Write and run kaldi recipe.
I am working on the ivector training step.
I had to lower the number of jobs from 10 to 4 in ivector extractor training.
Training goes all the way through chain models.
I am now tuning the number of gaussians and leaves.
The WERs are very sensitive to these parameters.

Here are the WER results so far:

%WER 43.34 [ 1146 / 2644, 89 ins, 236 del, 821 sub ] exp/chain/tdnn1a_sp/decode_test/wer_8_0.0
%WER 42.59 [ 1126 / 2644, 82 ins, 243 del, 801 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_9_0.0
%WER 35.78 [ 946 / 2644, 50 ins, 275 del, 621 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 33.55 [ 887 / 2644, 32 ins, 314 del, 541 sub ] exp/mono/decode_test/wer_15_0.0
%WER 33.09 [ 875 / 2644, 46 ins, 254 del, 575 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 32.03 [ 847 / 2644, 32 ins, 280 del, 535 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 22.50 [ 595 / 2644, 27 ins, 172 del, 396 sub ] exp/tri3b/decode_test/wer_17_0.0

Works needs to be done on the chain models.

- TODO Libyan: Incorporate into multilang setup.
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.
- TODO Multilang: Run cross task (GALE Arabic, SOFTunisia and GP Arabic) build again with new SOFTunisia test set.
- TODO GALE Arabic: Get recipe running on B-Team workstation.
This is running.

** Goals for Next Week:
- TODO ARL-Colloquium: Write slides on AI for ASR.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.

* DAR <2018-02-15 Thu>
** Goals for Thursday set Wednesday:
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.
Michelle and I had a good look at the slides.
- TODO Multilang: Run cross task (GALE Arabic, SOFTunisia and GP Arabic) build again with new SOFTunisia test set.


** Goals for Friday:
- TODO Libyan: Write and run kaldi recipe.
- TODO Libyan: Incorporate into multilang setup.
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.
- TODO Multilang: Run cross task (GALE Arabic, SOFTunisia and GP Arabic) build again with new SOFTunisia test set.
- TODO GALE Arabic: Get recipe running on B-Team workstation.
- TODO 
* DAR <2018-02-14 Wed>
** Goals for Wednesday set Tuesday:
- TODO SOFTunisia: Consolidate scores for eesen run.
The training is still running this morning.
Training finished.
I fixed the decoding  scripts.
A lot of scripts were missing.
I guess I had them from the kaldi directories.
Here is the WER score for the eesen run:
%WER 27.90 [ 1151 / 4125, 63 ins, 266 del, 822 sub ] exp/train_char_l4_c320/decode/wer_7_0.0 

size of the fst tlg.fst: 7107678 
size of the acousti model: 34197673 
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.

** Goals for Thursday:
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.
- TODO Multilang: Run cross task (GALE Arabic, SOFTunisia and GP Arabic) build again with new SOFTunisia test set.

* DAR <2018-02-13 Tue>
** Goals for Tuesday set Monday:
- TODO SOFTunisia: Get pronunciations for 66 qcri-OOVs from Zac and incorporate into lexicon.
After fixing attached arabic commas and dos newlines there are only 18 OOVS.
- TODO SOFTunisia Consolidate Results of latest  run.
Here are the results from the morning:

%WER 9.02 [ 372 / 4125, 15 ins, 99 del, 258 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_14_0.0
%WER 24.00 [ 990 / 4125, 47 ins, 283 del, 660 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 15.42 [ 636 / 4125, 19 ins, 218 del, 399 sub ] exp/mono/decode_test/wer_17_0.0
%WER 15.30 [ 631 / 4125, 19 ins, 202 del, 410 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 15.08 [ 622 / 4125, 31 ins, 171 del, 420 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 12.95 [ 534 / 4125, 37 ins, 138 del, 359 sub ] exp/tri3b/decode_test/wer_17_0.0
%WER 10.52 [ 434 / 4125, 16 ins, 115 del, 303 sub ] exp/chain/tdnn1c_sp/decode_test/wer_11_0.0

| model | WER |
| mono | 15.42 |
| tri1 | 15.08 |
| tri2b | 15.30 |
| tri3b | 12.95 |
| chain | 10.52 |
| chain online | 9.02 |

Here are the results from the afternoon:
%WER 9.36 [ 386 / 4125, 8 ins, 145 del, 233 sub ] exp/chain/tdnn1c_sp/decode_test/wer_16_0.0
%WER 8.22 [ 339 / 4125, 7 ins, 106 del, 226 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_17_0.0
%WER 24.00 [ 990 / 4125, 47 ins, 283 del, 660 sub ] exp/tri3b/decode_test.si/wer_17_0.0
%WER 15.42 [ 636 / 4125, 19 ins, 218 del, 399 sub ] exp/mono/decode_test/wer_17_0.0
%WER 15.30 [ 631 / 4125, 19 ins, 202 del, 410 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 15.08 [ 622 / 4125, 31 ins, 171 del, 420 sub ] exp/tri1/decode_test/wer_17_0.0
%WER 12.95 [ 534 / 4125, 37 ins, 138 del, 359 sub ] exp/tri3b/decode_test/wer_17_0.0

| model | morning WER | afternoon WER |
| mono | 15.42 | 15.42 |
| tri1 | 15.08 | 15.08 |
| tri2b | 15.30 | 15.30 |
| tri3b | 12.95 | 12.95 |
| chain | 10.52 | 9.36 |
| chain online | 9.02 | 8.22 |

- TODO SOFTunisia: Set up eesen end to end character system.
I worked all day on this.
The analyze-counts probram requires the text file to have the utterance id and text separated by a white space not a tab.
The training step is running on the GPU.

- TODO EESEN: Install on b-team workstation.
I let Justin know about the compile problems I had with eesen on the B-Team workstation.

- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.

** Goals for Wednesday:
- TODO SOFTunisia: Consolidate scores for eesen run.
- TODO SOFTunisia: get pronunciations for 18 OOVs from Zac and incorporate into lexicon.
- TODO  EESEN: Get it installed on the b-team  workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.

* DAR <2018-02-12 Mon>
** Previous Goals:
- TODO SOFTunisia: Incorporate Libyan data into test set.
Zac wanted me to try volume augmented data.
%WER 67.66 [ 228 / 337, 20 ins, 15 del, 193 sub ] exp/tri3b/decode_adel_augmented_volume/wer_17_0.5
%WER 67.06 [ 226 / 337, 21 ins, 13 del, 192 sub ] exp/tri3b/decode_adel_vol/wer_15_0.5


%WER 69.73 [ 235 / 337, 13 ins, 21 del, 201 sub ] exp/tri3b/decode_adel_vol.si/wer_17_0.5
%WER 68.84 [ 232 / 337, 12 ins, 22 del, 198 sub ] exp/tri3b/decode_adel.si/wer_17_0.5
%WER 68.84 [ 232 / 337, 12 ins, 22 del, 198 sub ] exp/tri3b/decode_adel_augmented_volume.si/wer_17_0.5
%WER 67.66 [ 228 / 337, 20 ins, 15 del, 193 sub ] exp/tri3b/decode_adel/wer_17_0.5
%WER 67.66 [ 228 / 337, 20 ins, 15 del, 193 sub ] exp/tri3b/decode_adel_augmented_volume/wer_17_0.5
%WER 67.06 [ 226 / 337, 21 ins, 13 del, 192 sub ] exp/tri3b/decode_adel_vol/wer_15_0.5
%WER 64.57 [ 2665 / 4127, 168 ins, 461 del, 2036 sub ] exp/tri3b/decode_test.si/wer_16_0.0
%WER 63.63 [ 2626 / 4127, 95 ins, 463 del, 2068 sub ] exp/mono/decode_test/wer_12_0.0
%WER 59.83 [ 2469 / 4127, 121 ins, 459 del, 1889 sub ] exp/tri2b/decode_test/wer_16_0.0
%WER 59.80 [ 2468 / 4127, 129 ins, 416 del, 1923 sub ] exp/tri1/decode_test/wer_17_0.5
%WER 59.07 [ 2438 / 4127, 113 ins, 498 del, 1827 sub ] exp/chain/tdnn1c_sp/decode_test/wer_9_0.5
%WER 58.44 [ 2412 / 4127, 176 ins, 371 del, 1865 sub ] exp/tri3b/decode_test/wer_17_0.5
%WER 57.62 [ 2378 / 4127, 131 ins, 411 del, 1836 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_10_0.0

We figured out that there were a decent amount of OOVs.
178 words in training that were not in our smaller lexicon.
These probably came from the sarraj data.
407 words in the new Libyan test set that were not in our smaller lexicon.
There were another 66 words that were neither in the bigger qcri dictionary nor our smaller lexicon.
Zac is going to work on the 66 words.

- TODO Multilang: Run aishell kaldi recipe.
%WER 47.16 [ 30384 / 64428, 1905 ins, 4888 del, 23591 sub ] exp/mono/decode_test/wer_12_0.0
%WER 42.41 [ 54161 / 127698, 3394 ins, 8668 del, 42099 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 31.23 [ 20118 / 64428, 1617 ins, 3165 del, 15336 sub ] exp/tri1/decode_test/wer_13_0.5
%WER 31.07 [ 20020 / 64428, 1659 ins, 3097 del, 15264 sub ] exp/tri2/decode_test/wer_14_0.5
%WER 30.95 [ 19942 / 64428, 1640 ins, 2896 del, 15406 sub ] exp/tri4a/decode_test.si/wer_17_0.5
%WER 28.28 [ 18220 / 64428, 1530 ins, 2761 del, 13929 sub ] exp/tri3a/decode_test/wer_15_0.5
%WER 27.94 [ 18002 / 64428, 1772 ins, 2336 del, 13894 sub ] exp/tri5a/decode_test.si/wer_14_0.5
%WER 27.26 [ 34811 / 127698, 3101 ins, 5101 del, 26609 sub ] exp/tri1/decode_dev/wer_13_0.0
%WER 27.07 [ 34562 / 127698, 3231 ins, 4869 del, 26462 sub ] exp/tri2/decode_dev/wer_13_0.0
%WER 26.73 [ 34129 / 127698, 3406 ins, 4259 del, 26464 sub ] exp/tri4a/decode_dev.si/wer_13_0.0
%WER 24.75 [ 31605 / 127698, 2940 ins, 4270 del, 24395 sub ] exp/tri3a/decode_dev/wer_14_0.0
%WER 23.92 [ 15413 / 64428, 1530 ins, 2137 del, 11746 sub ] exp/tri4a/decode_test/wer_15_0.5
%WER 23.85 [ 30462 / 127698, 3072 ins, 3678 del, 23712 sub ] exp/tri5a/decode_dev.si/wer_13_0.0
%WER 22.04 [ 14203 / 64428, 1408 ins, 1998 del, 10797 sub ] exp/tri5a/decode_test/wer_15_0.5
%WER 21.48 [ 27432 / 127698, 2821 ins, 3507 del, 21104 sub ] exp/tri4a/decode_dev/wer_14_0.0
%WER 19.53 [ 24940 / 127698, 2591 ins, 3183 del, 19166 sub ] exp/tri5a/decode_dev/wer_14_0.0

| model | WER|

- TODO GALE Arabic: run s5b recipe to include chain models.
- TODO GALE Mandarin: Run recipe.
- TODO ARL Colloquium: Write slides.

** Goals for Tuesday:
- TODO SOFTunisia: Get pronunciations for 66 qcri-OOVs from Zac and incorporate into lexicon.
- TODO SOFTunisia Consolidate Results of latest  run.
- TODO SOFTunisia: Set up eesen end to end character system.
- TODO EESEN: Install on b-team workstation.
- TODO AISHELL: Build chain models.
- TODO AISHELL: Incorporate into multilang.
- TODO ARL Colloquium: Write about Neural Networks.

* DAR <2018-02-09 Fri>
** Goals for Friday set Thursday:
- TODO SOFTunisia: Incorporate Libyan data into test set.
- TODO Multilang: Run aishell kaldi recipe.
- TODO GALE Arabic: run s5b recipe to include chain models.
- TODO GALE Mandarin: Run recipe.
- TODO ARL Colloquium: Write slides.

* DAR <2018-02-08 Thu>
** Goals for Thursday set Wednesday:
- TODO Inuktitut: Get phone 2 word system recipe running.
I am going to leave this for now.
- TODO Colloquium: Write slides.

** Old Goals:
- TODO Multilang: Setup experiment for training chain models on GALE Arabic and testing on SOFTunis and GP_Arabic.
I tested the GALE Arabic on the Sarraj data:
%WER 39.43 [ 27471 / 69668, 2105 ins, 5890 del, 19476 sub ] exp/tri1/decode/wer_15_0.5
%WER 38.35 [ 26718 / 69668, 2177 ins, 5703 del, 18838 sub ] exp/tri2a/decode/wer_15_0.5
%WER 35.91 [ 25018 / 69668, 2221 ins, 5338 del, 17459 sub ] exp/tri2b/decode/wer_15_0.5
%WER 35.65 [ 24838 / 69668, 2368 ins, 5141 del, 17329 sub ] exp/tri3b/decode.si/wer_15_0.5
%WER 33.81 [ 23557 / 69668, 2664 ins, 4636 del, 16257 sub ] exp/tri3b/decode/wer_17_0.5


DETAILED OVERALL REPORT FOR THE SYSTEM: ./sarraj_hyps.txt

SENTENCE RECOGNITION PERFORMANCE

 sentences                                          38
 with errors                            100.0%   (  38)

   with substitions                     100.0%   (  38)
   with deletions                        39.5%   (  15)
   with insertions                      100.0%   (  38)


WORD RECOGNITION PERFORMANCE

Percent Total Error       =   51.2%   ( 340)

Percent Correct           =   59.2%   ( 393)

Percent Substitution      =   36.4%   ( 242)
Percent Deletions         =    4.4%   (  29)
Percent Insertions        =   10.4%   (  69)
Percent Word Accuracy     =   48.8%


Ref. words                =           ( 664)
Hyp. words                =           ( 704)
Aligned words             =           ( 733)

CONFUSION PAIRS                  Total                 (227)
                                 With >=  1 occurances (227)

NOTE: The 'Substitution' words are those reference words
        for which the recognizer supplied an incorrect word.


FALSELY RECOGNIZED               Total                 (212)
                                 With >=  1 occurances (212)

** Goals for Friday:
- TODO SOFTunisia: Incorporate Libyan data into test set.
- TODO Multilang: Run aishell kaldi recipe.
- TODO GALE Arabic: run s5b recipe to include chain models.
- TODO GALE Mandarin: Run recipe.
- TODO ARL Colloquium: Write slides.

* DAR <2018-02-07 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Inuktitut: Train prototype acoustic models.
I spent all day on this.
The main problem was cleaning the transcripts in the 3 sub corpora.
1. Jeffrey's word to morphemes list.
2. Tusaalanga' dialogues.
3. Tusaalanga's dialect transcripts.

The transcripts are now at the word level.
If we want to recognize at the morpheme level, we need to analyze  the words into morphemes.

** Goals for Thursday:
- TODO Inuktitut: Get phone 2 word system recipe running.
- TODO Colloquium: Write slides.

* DAR <2018-02-06 Tue>
** Goals for Wednesday:
- TODO Inuktitut: Train prototype acoustic models.

* DAR <2018-02-05 Mon>
* DAR <2018-02-01 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Setup experiment for training chain models on GP Arabic, Softunisia and GALE Arabic.
This turns out to be much more involved than expected.
I will use the GALE Arabic grapheme to grapheme dictionary for all the languages.

- DONE AMTA: Get data from endangered languages.
Steve and Judith took care of this.
Zac will take it from here to extract data from the youtube pages.
- TODO Inuktitut: Investigate pronunciation.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
I prepared  recording data for Adel, Anwar, Bubaker and Hisham.

- DONE SOFTunisia: Ask Zac if he can run the recipe.
He says he is willing to try it on linux.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
I downloaded the ashell corpus instead.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

- Multilang: 
%WER 9.34 [ 62 / 664, 2 ins, 15 del, 45 sub ] exp/nnet3/multi_bnf/softunisia/decode_test/wer_11_0.0

| language   | tri3b WER | chain WER | MTL gp_arabic softunisia | gp_arabic softunisia GALE Arabic|
| GP Arabic dev |     55.98 |     51.17 | 51.73 | 50.95 |
| softunisia sarraj test | 26.20 | 13.40 | 9.34 |

This is really good news!

Yesterday I went to tell Michelle the good news that the MTL method had beaten the chain model method on the gp_arabic dev set. 
I was excited because this was the first time I got the MTL method to give the best results.
Michelle threw a big bucket of ice water over my head by reminding me that critics will say "you just added more data" and you got better results.
So the experiment we need to run is to combine all the data and train a monolingual chain model system. 
I ran the experiment with threee corpora: Globalphone Arabic, Tuniseen and GALE Arabic. 
This morning I started working on reconciling the three corpora so that we can train one chain model on all the data.
Meanwhile,  I ran the MTL trained  models on the softunisia sarraj test set and the results are even more impressive: 13.40 versus 9.34.
Reconciling the three corpora is much easier said than done.
Each corpora uses a different dictionary.
Globalphone Arabic uses a vocalized romanization for the words and an ascii encoded IPA labeling for the phone.
The Globalphone documentation provides a mapping for most but not all the Arabic Characters. 
The vowels are not mapped and it is not clear how to map to tah mar buta alif maqsura, etc.
I do not have access to the mapping between IPA and the ascii encoding used by Globalphone for the phones.
The GALE Arabic system uses a grapheme to grapheme dictionary.
In other words, the model labels are graphemes instead of phones.
The graphemes use the buckwalter encoding. 
Our tuniseen corpus uses utf8 encoded Arabic characters written by Zac. 
The phone set was borrowed from the QCRI dictionary.
I do not know if there is a mapping between our phone set and IPA.
All this points out how difficult it is to reconcile corpora. 
None of this was an obstacle for MTL. 
I guess an experiment we should run is train GALE Arabic chain models and test on the Globalphone  Arabic dev set and softuniseen test set.
Also train a Globalphone arabic monolingual chain model system and test on softuniseen and vice versa.

** Goals for Friday:
- TODO Multilang: Setup experiment for training chain models on GP Arabic and testing on SOFTunis and GP_Arabic.
- TODO Inuktitut: Investigate pronunciation.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
- TODO SOFTunisia: Send instructions to Zac for installing kaldi.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- TODO Mandarin: Run ashell kaldi recipe.  
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Incorporate ashell mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* DAR <2018-01-31 Wed>
** Goals for Wednesday set Tuesday:
- DONE AMTA: Meeting with Judith to discuss paper ideas.
- TODO AMTA: Get data from endangered languages.
- TODO Inuktitut: Investigate pronunciation.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- DONE Multilang: Incorporate GALE Arabic.
I ran a small multilang experiment.
I ran it using gp_arabic, softunisia and GALE Arabic.
Here is the result on gp_arabic dev:
%WER 50.95 [ 4607 / 9043, 338 ins, 787 del, 3482 sub ] exp/nnet3/multi_bnf/gp_arabic/decode_dev/wer_16_0.0 
This is really good news!.
This is the first time the multilang method beats chain models and we are using data from different sources.
We are combining read speech with Broadcast News.

| language   | tri3b WER | chain WER | MTL gp_arabic softunisia | gp_arabic softunisia GALE Arabic|
| GP Arabic dev |     55.98 |     51.17 | 51.73 | 50.95 |

- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.


- Multilang Mini Experiment:
I ran the multilang recipe on gp_arabic and softunisia.
Here are the results on the gp_arabic dev data:
%WER 51.73 [ 4678 / 9043, 370 ins, 856 del, 3452 sub ] exp/nnet3/multi_bnf/gp_arabic/decode_dev/wer_15_0.0


| language   | tri3b WER | chain WER | MTL gp_arabic softunisia |
| GP Arabic dev |     55.98 |     51.17 | 51.73 |

** Goals for Thursday:
- TODO Multilang: Setup experiment for training chain models on GP Arabic, Softunisia and GALE Arabic.
- TODO AMTA: Get data from endangered languages.
- TODO Inuktitut: Investigate pronunciation.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* DAR <2018-01-30 Tue>
** Goals for Tuesday set Monday:
- DONE ARL Colloquium: Decide on February 13 or 27.
I will present on March 6.
Mary will present on February 27.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
This is turning out to be a lot of work.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate i-vecbtors.
I am working on this.
It looks promising. 
I think it is done.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Inuktitut: Investigate pronunciation.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

** Goals for Wednesday:
- TODO AMTA: Meeting with Judith to discuss paper ideas.
- TODO AMTA: Get data from endangered languages.
- TODO Inuktitut: Investigate pronunciation.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* <2018-01-29 Mon>
I took sick leave last Friday because I cut my index finger.
**  Goals for Friday set Thursday:
- TODO Softunisia: Get results of chain model decoding on train data to Zac.
The results using the chain models were not as good as the results using the tri3b (sat mllt lda) models.
%WER 7.14 [ 13430 / 188004, 49 ins, 251 del, 13130 sub ] exp/chain/tdnn1c_sp/decode_train/wer_8_0.0
Zac is going to use the transcripts obtained with the tri3b models.
- TODO Softunisia: Tune system.
- DONE SOFTunisia: Get Recordings data to Zac.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Watch the run.
I have not been able to build the GALE Mandarin tri3b models yet.
The B-team workstation is the right place to do this work.

- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate i-vecbtors.
- TODO Multilang: Incorporate GALE Arabic.
I'll do this when I finish decoding the latest multilang build.
Probably tomorrow.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
| language | tri3b WER | chain WER | MTL WER 7 languages   | mtl 17 languages  | 17 languages second try |
| Arabic dev | 55.98 | 51.17 | | 53.18 | 52.50 |
| Bulgarian dev | 24.78      | 19.47 | 22.33 | 23.81 | 22.28 |
| Croatian dev | 28.53 | 27.57 | 28.77 | 33.02 | 27.68 |
| Czech dev | 43.72 | 50.14 | | 46.70 | 43.33 |
| French dev | 24.21       | | | 91.36 | 24.68
| German dev | 38.04 | | | 39.24 | 36.30 |
| Hausa dev | 24.64 | 23.56 | 21.77 | 27.99 | 22.31 |
| Japanese dev | 6.15 | | 4.97 | 5.13 | 5.01 |
| Korean dev | 25.64 | | 24.28 | 27.81 | 24.92 |
| Mandarin dev | 19.07 | 15.52 | 17.94 | | 19.02 |
| Polish dev | 32.62 | | | 50.41 | 47.96 |
| Portuguese dev | 24.11 | | 21.30 | 23.47 | 22.11 |
| Russian dev | 55.81 | 49.23 | | 54.83 | 53.04 |
| Spanish dev | 33.36 | | | 43.95 | 41.10 |
| Swedish dev | 62.07 | | | 65.53 | 62.43 |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | | 73.51 | 71.09 |
| Vietnamese dev | 37.49 | | | 38.67 | 35.20 |

- TODO Multilang: Write some paragraphs for paper.
- TODO Inuktitut: Investigate pronunciation.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

** Goals for Tuesday:
- TODO ARL Colloquium: Decide on February 13 or 27.
- TODO ARL Colloquium: Prepare slides.
- TODO Softunisia: Write recipe scripts to prepare Libyan MSA data.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Build tri3b models. 
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate i-vecbtors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Write some paragraphs for paper.
- TODO Inuktitut: Investigate pronunciation.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* DAR <2018-01-25 Thu>
** Goals for Thursday set Wednesday:
- TODO GALE Mandarin: Watch the run.
- TODO Multilang: Incorporate i-vecbtors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
- TODO Inuktitut: Investigate pronunciation.
- TODO Softunisia: Prepare recipe for kaldi repo.
I ran the chain models for Softunisia.
Good results.

Here are all the results:
%WER 42.92 [ 285 / 664, 7 ins, 120 del, 158 sub ] exp/tri3b/decode_test.si/wer_11_0.0
%WER 34.34 [ 228 / 664, 8 ins, 83 del, 137 sub ] exp/mono/decode_test/wer_12_0.0
%WER 33.13 [ 220 / 664, 9 ins, 91 del, 120 sub ] exp/tri2b/decode_test/wer_10_0.0
%WER 30.72 [ 204 / 664, 7 ins, 62 del, 135 sub ] exp/tri1/decode_test/wer_11_0.0
%WER 26.20 [ 174 / 664, 8 ins, 62 del, 104 sub ] exp/tri3b/decode_test/wer_15_0.0
%WER 15.81 [ 105 / 664, 2 ins, 31 del, 72 sub ] exp/chain/tdnn1c_sp/decode_test/wer_12_0.0

| model | WER |
| mono | 34.34 |
| tri2b tri mllt lda | 33.13 |
| tri1 | 30.72 |
| tri3b mllt lda sat | 26.20 |
| tdnn chain | 15.81 |
| tdnn chain online | 13.40 |

Why are the tri1 models better than the tri2b models?
What happened to the monophones?

- TODO African French ditto for African French (Yaounde)
- DONE SOFTUNISIA: Contact Zac about dictionary.
Zac is going to work on the sarraj test data first.
He will get me more test data Monday.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

** Goals for Friday:
- TODO Softunisia: Get results of chain model decoding on train data to Zac.
- TODO Softunisia: Tune system.
- TODO SOFTunisia: Get Recordings data to Zac.
- TODO SOFTunisia: Ask Zac if he can run the recipe.p
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Watch the run.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate i-vecbtors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
- TODO Multilang: Write some paragraphs for paper.
- TODO Inuktitut: Investigate pronunciation.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.
- TODO SOFTunisia: Ask Zac if he can run the recipe.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO GALE Mandarin: Watch the run.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Incorporate i-vecbtors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.

* DAR <2018-01-24 Wed>
** Tuesday:
- DONE TARP training.

Nothing else was accomplished.
All the mandatory training pages are not accessible to JAWS. 
The standown was a total waste of time for me.

** Goals for This Week:
- TODO GALE Mandarin: Set up recipe.
I have it running on my laptop and the B-team workstation.
Still no kaldi on the B-team workstation.
- TODO Multilang: Incorporate i-vectors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
- TODO Inuktitut: Investigate pronunciation.
- TODO Softunisia: Prepare recipe for kaldi repo.
I spent most of the afternoon working on this goal.
It is starting to look good.
Once Zac is done with the dictionary, I think we should contact Yenda about submitting it.
We'll want to submit the dictionary to the openslr.org webpage.
openslr.org is basically a data repo for kaldi.
- TODO African French ditto for African French (Yaounde)
- TODO Multilang: Write some paragraphs for paper.
I wrote some words.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

** Goals for Thursday:
- TODO GALE Mandarin: Watch the run.
- TODO Multilang: Incorporate i-vectors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
- TODO Inuktitut: Investigate pronunciation.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO SOFTUNISIA: Contact Zac about dictionary.
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* DAR <2018-01-23 Tue>
** Goals for This Week:
- TODO GALE Mandarin: Set up recipe.
- TODO Multilang: Incorporate i-vectors.
- TODO Multilang: Incorporate GALE Arabic.
- TODO Multilang: Incorporate GALE Mandarin.
- TODO Mandarin: Search for CASS corpus.
- TODO Multilang: Finish decoding languages with latest build that incorporated bottlenecks.
- TODO Inuktitut: Investigate pronunciation.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
- TODO GALE Arabic: Run s5b recipe with possibly more corpora instead of s5 recipe.

* dar <2018-01-19 Fri>
**  Goals for Friday set Thursday:
- DONE GALE Mandarin: Copy data from DVDs to workstation.
Justin moved all the LDC GALE Mandarin corpora to /mnt/corpora
- DONE GALE Mandarin: Ask Justin to put data on /mnt/corpora
- TODO GALE Mandarin: Run kaldi script (modify if needed).
I am setting up the script on my laptop.
A python 2 module is required that was not on anyt of our machines.
It is called mmseg.
I'm not sure what this module does.
 
- TODO MTL Paper: Write some paragraphs on data section.

* DAR <2018-01-18 Thu>
**  Goals for Thursday set Wednesday:
- TODO Multilang: Incorporate Bottlenecks.
- TODO Multilang: Incorporate I-vectors.
The main script is still running this morning. 
It it currently adjusting the priors on the Korean network.
Raw Neural Network training  is done.

- GALE Arabic: Run chain models on GPU workstation.
The main script is still running.
It is training SAT tri3b models.

- TODO Multilang: Run with GALE Arabic.
- TODO Softunisia: Work with Zac on dictionary.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- Yaounde: Investigate why WERs are so low.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 


** Goals for Friday:
- TODO GALE Mandarin: Copy data from DVDs to workstation.
- TODO GALE Mandarin: Ask Justin to put data on /mnt/corpora
- TODO GALE Mandarin: Run kaldi script (modify if needed).
- TODO MTL Paper: Write some paragraphs on data section.

* DAR <2018-01-17 Wed>
**  Goals for Next Week set Friday:
I forgot to set goals yesterday.
- TODO Multilang: Incorporate Bottlenecks.
I started a multilang run yesterday and it it still running this morning.
It is on iteration 270 of the neural network training.
It will run for 480 iterations.
- TODO Multilang: Incorporate I-vectors.
- TODO Multilang: Run with GALE Arabic.
I started building the GALE Arabic on the A-team workstation yesterday since we do not have kaldi compiled with GPUs on theB-team workstation yet. 
It is still running this morning.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 

** Goals for Thursday:
- TODO Multilang: Incorporate Bottlenecks.
- TODO Multilang: Incorporate I-vectors.
- GALE Arabic: Run chain models on GPU workstation.
- TODO Multilang: Run with GALE Arabic.
- TODO Softunisia: Work with Zac on dictionary.
- TODO Softunisia: Prepare recipe for kaldi repo.
- TODO African French ditto for African French (Yaounde)
- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- Yaounde: Investigate why WERs are so low.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 

* DAR <2018-01-16 Tue>
** Goals for Next Week set Last Friday:
- TODO Multilang: Incorporate Bottlenecks.
I started working on this today.
I only added a couple of lines to the run_multilingual.sh script referring to bnf.
I added a line indicating the bottleneck layer dimension.
I added a line to the neural network config file indicating information about the bottleneck layer.
This line also creates the bottleneck  layer which is the 7th layer.
I also had to indicate that the next layer -- the prefinal affine layer -- should take as input the bottleneck layer instead of layer 6.
Other than that I did not do anyting.
The rest seems to be taken care of by other scripts and c++ programs (I hope ). 

- TODO Multilang: Incorporate I-vectors.
- TODO Multilang: Run with GALE Arabic.
- TODO Softunisia: Prepare recipe for kaldi repo.
Here are the WER scores for the Sarraj test data:

%WER 94.28 [ 626 / 664, 3 ins, 99 del, 524 sub ] exp/mono/decode_test/wer_7_0.0
%WER 89.91 [ 597 / 664, 11 ins, 89 del, 497 sub ] exp/tri3b/decode_test.si/wer_15_0.5
%WER 88.10 [ 585 / 664, 12 ins, 98 del, 475 sub ] exp/tri1/decode_test/wer_15_0.0
%WER 87.95 [ 584 / 664, 13 ins, 75 del, 496 sub ] exp/tri2b/decode_test/wer_12_0.0
%WER 86.14 [ 572 / 664, 11 ins, 79 del, 482 sub ] exp/tri3b/decode_test/wer_16_0.5

Since these scores look really bad, I decided to check how the models do on the training data:

%WER 5.50 [ 3444 / 62668, 252 ins, 1155 del, 2037 sub ] exp/mono/decode_train/wer_17_0.0
%WER 2.14 [ 1341 / 62668, 239 ins, 271 del, 831 sub ] exp/tri1/decode_train/wer_16_0.0
%WER 1.79 [ 1124 / 62668, 216 ins, 240 del, 668 sub ] exp/tri2b/decode_train/wer_17_0.0
%WER 1.77 [ 1112 / 62668, 201 ins, 212 del, 699 sub ] exp/tri3b/decode_train.si/wer_17_0.0
%WER 1.52 [ 955 / 62668, 185 ins, 172 del, 598 sub ] exp/tri3b/decode_train/wer_17_0.0

| model | test WER | train WER | |
| mono  | 94.28 | 5.50 |
| tri1 | 88.10 | 2.14 |
| tri2b | 87.95 | 1.79 |
| tri3b | 86.14 | 1.52 |

From these results it looks like the problem is overfitting.

- TODO African French ditto for African French (Yaounde)
I worked a little on the yaounde recipe today.
There was a non breaking white space  aka hard space in the test data transcription.
This now make the validation fail.
It insists only on the simpl space  for white space.

- TODO Multilang: Write some paragraphs for paper.
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO GP: GEt chain model results for all languages in gp. 
* DAR <2018-01-12 Fri>
**  Goals for Friday set Thursday:
- TODO Multilang: Compile results after training on 17 languages

| language | tri3b WER | chain WER | MTL WER 7 languages   | mtl 17 languages  | 17 languages second try |
| Arabic dev | 55.98 | 51.17 | | 53.18 | 52.50 |
| Bulgarian dev | 24.78      | 19.47 | 22.33 | 23.81 | 22.28 |
| Croatian dev | 28.53 | 27.57 | 28.77 | 33.02 | 27.68 |
| Czech dev | 43.72 | 50.14 | | 46.70 | 43.33 |
| French dev | 24.21       | | | 91.36 |
| German dev | 38.04 | | | 39.24 |
| Hausa dev | 24.64 | 23.56 | 21.77 | 27.99 |
| Japanese dev | 6.15 | | 4.97 | 5.13 |
| Korean dev | 25.64 | | 24.28 | 27.81 |
| Mandarin dev | 19.07 | 15.52 | 17.94 | |
| Polish dev | 32.62 | | | 50.41 |
| Portuguese dev | 24.11 | | 21.30 | 23.47 |
| Russian dev | 55.81 | 49.23 | | 54.83 |
| Spanish dev | 33.36 | | | 43.95 |
| Swedish dev | 62.07 | | | 65.53 |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | | 73.51 |
| Vietnamese dev | 37.49 | | | 38.67 |

- TODO Incorporate i-vectors.
- TODO Multilang: Incorporate Bottlenecks.
- TODO GP: Get chain model baseline WER scores for all languages (German and Japanese first)
- TODO AMPTA: Meet with Judith (data?)
- TODO GP: Fix Portuguese.

- Baseline WER scores: 
These are the best scores achieved so far: 

| language | tri3b WER | chain WER | MTL WER 7 languages   | | mtl 17 languages |
| Arabic dev | 55.98 | 51.17 | | 53.18 |
| Bulgarian dev | 24.78      | 19.47 | 22.33 | 23.81 |
| Croatian dev | 28.53 | 27.57 | 28.77 | 33.02 |
| Czech dev | 43.72 | 50.14 | | 46.70 |
| French dev | 24.21| | | 91.36 |
| German dev | 38.04 | | | 39.24 |
| Hausa dev | 24.64 | 23.56 | 21.77 | 27.99 |
| Japanese dev | 6.15 | | 4.97 | 5.13 |
| Korean dev | 25.64 | | 24.28 | 27.81 |
| Mandarin dev | 19.07 | 15.52 | 17.94 | |
| Polish dev | 32.62 | | | 50.41 |
| Portuguese dev | 24.11 | | 21.30 | 23.47 |
| Russian dev | 55.81 | 49.23 | | 54.83 |
| Spanish dev | 33.36 | | | 43.95 |
| Swedish dev | 62.07 | | | 65.53 |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | | 73.51 |
| Vietnamese dev | 37.49 | | | 38.67 |

* DAR <2018-01-11 Thu>
**  Goals for Thursday set Wednesday:
- DONE Softunisia: Fix problems with new test set. Why is tri3b failing?
This was just a bug in the run.sh script.
- TODO Multilang: Get WER scores for 17 languages.

- Baseline WER scores: 
These are the best scores achieved so far: 

| language | tri3b WER | chain WER | MTL WER 7 languages   | | mtl 17 languages |
| Arabic dev | 55.98 | 51.17 | | 53.18 |
| Bulgarian dev | 24.78      | 19.47 | 22.33 | 23.81 |
| Croatian dev | 28.53 | 27.57 | 28.77 | 33.02 |
| Czech dev | 43.72 | 50.14 | | 46.70 |
| French dev | 93.41 | | | 91.36 |
| German dev | 38.04 | | | 39.24 |
| Hausa dev | 24.64 | 23.56 | 21.77 | 27.99 |
| Japanese dev | 6.15 | | 4.97 | 5.13 |
| Korean dev | 25.64 | | 24.28 | 27.81 |
| Mandarin dev | 19.07 | 15.52 | 17.94 | |
| Polish dev | 32.62 | | | 50.41 |
| Portuguese dev | 24.11 | | 21.30 | 23.47 |
| Russian dev | 55.81 | 49.23 | | 54.83 |
| Spanish dev | 33.36 | | | 43.95 |
| Swedish dev | 62.07 | | | 65.53 |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | | 73.51 |
| Vietnamese dev | 37.49 | | | 38.67 |

The French results should be ignored. The alignments were radom.

- TODO Multilang: I-vectors (should I incorporate this now or wait?)
- TODO Multilang: ditto for bottleneck layer.
- TODO GP: Get Portuguese working. I should be able to do this since I've done it before. (copy build on a-team workstation?
I worked on gp_french instead.
I found a major problem. I think it is the reason the WER are so low.
I had copied a script to make list from my African Accented French recipe.
I had to be modified to work with gp_french.
Basically, the alignments were randomly assigned to labels.
 
Here are the cd gmm hmm WER scores:
%WER 48.54 [ 10824 / 22297, 442 ins, 2641 del, 7741 sub ] exp/mono/decode_dev/wer_10_0.0
%WER 26.88 [ 5994 / 22297, 639 ins, 885 del, 4470 sub ] exp/tri1/decode_dev/wer_16_0.0
%WER 26.11 [ 5822 / 22297, 642 ins, 878 del, 4302 sub ] exp/tri2b/decode_dev/wer_17_0.0
%WER 11.65 [ 2598 / 22297, 226 ins, 694 del, 1678 sub ] exp/tri3b/decode_dev.si/wer_17_0.0
john@I3916:~/gp_french/s5$ 
- TODO GP: Get chain model WER scores for Japanese and German.
The GPU was down today because of a security patch.

- TODO AMPTA: LRL data.

** Goals for Friday:
- TODO Multilang: Compile results after training on 17 languages
- TODO Incorporate i-vectors.
- TODO Multilang: Incorporate Bottlenecks.
- TODO GP: Get chain model baseline WER scores for all languages (German and Japanese first)
- TODO AMPTA: Meet with Judith (data?)
- TODO GP: Fix Portuguese.

* DAR <2018-01-10 Wed>
** Goals for Wednesday set Tuesday:
- TODO Multilang: Get MTL WER results  after training on 17 languages.
The training is not done yet as of Wednesday morning.
The training iterations are done after a total of 479.
Model combination is being performed.
Training is done.
Decoding is a problem.
I am skipping i-vectors again.
I-vectors will have  to be left for later.
I can run decoding with hard-wired values passed as arguments to the decoder command line.
I should have WER scores tmorrow.
- TODO GALE Arabic: Get tri3b WER results.

%WER 39.43 [ 27471 / 69668, 2105 ins, 5890 del, 19476 sub ] exp/tri1/decode/wer_15_0.5
%WER 38.35 [ 26718 / 69668, 2177 ins, 5703 del, 18838 sub ] exp/tri2a/decode/wer_15_0.5
%WER 35.91 [ 25018 / 69668, 2221 ins, 5338 del, 17459 sub ] exp/tri2b/decode/wer_15_0.5
%WER 35.65 [ 24838 / 69668, 2368 ins, 5141 del, 17329 sub ] exp/tri3b/decode.si/wer_15_0.5
%WER 33.81 [ 23558 / 69668, 2664 ins, 4635 del, 16259 sub ] exp/tri3b/decode/wer_17_0.5

| model | WER |
| tri1 | 39.43 |
| tri2a | 38.35 |
| tri2b | 35.91 |
| tri3b | 33.81 |

- TODO GALE Arabic: Get chain model WER results.
- TODO GP: Get WERs for chain models (Hausa first)
Here are Hausa WER scores:
%WER 36.84 [ 616 / 1672, 48 ins, 148 del, 420 sub ] exp/tri1/decode_dev/wer_11_0.0
%WER 36.48 [ 610 / 1672, 26 ins, 136 del, 448 sub ] exp/mono/decode_dev/wer_13_0.0
%WER 32.30 [ 540 / 1672, 53 ins, 96 del, 391 sub ] exp/tri2b/decode_dev/wer_10_0.0
%WER 27.39 [ 458 / 1672, 33 ins, 90 del, 335 sub ] exp/tri3b/decode_dev.si/wer_13_0.0
%WER 24.64 [ 412 / 1672, 30 ins, 76 del, 306 sub ] exp/tri3b/decode_dev/wer_17_0.0
%WER 23.56 [ 394 / 1672, 21 ins, 58 del, 315 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_13_0.5
%WER 23.50 [ 393 / 1672, 21 ins, 58 del, 314 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_13_0.5

- TODO GP: Improve tri3b baselines (How?)

- Baseline WER scores: 
These are the best scores achieved so far: 

| language | tri3b WER | chain WER | MTL WER on 17 languages |
| Arabic dev | 55.98 | 51.17 | |
| Bulgarian dev | 24.78      | 19.47 | 22.33 |
| Croatian dev | 28.53 | 27.57 | 28.77 |
| Czech dev | 43.72 | 50.14 | |
| French dev | 93.41 | | |
| German dev | 38.04 | | |
| Hausa dev | 24.64 | 23.56 | 21.77 |
| Japanese dev | 6.15 | | 4.97 |
| Korean dev | 25.64 | | 24.28 |
| Mandarin dev | 19.07 | 15.52 | 17.94 |
| Polish dev | 32.62      | | |
| Portuguese dev | 24.11 | | 21.30 |
| Russian dev | 55.81 | 49.23 | |
| Spanish dev | 33.36 | | |
| Swedish dev | 62.07 | | | |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | |
| Vietnamese dev | 37.49 | | |

gp_portuguese is failing on the b-team workstation.
this is the only language (other than tamil and thai) that is failing to build.

- Softunisia: 
There was a problem with the latest run of the Softunisia system.
I ran it on the b-team workstation.
I get much better WER scores:
%WER 49.78 [ 28782 / 57818, 1651 ins, 4719 del, 22412 sub ] exp/mono/decode_test/wer_17_1.0
%WER 37.52 [ 4186 / 11156, 252 ins, 620 del, 3314 sub ] [PARTIAL] exp/tri3b/decode_test.si/wer_10_1.0
%WER 35.11 [ 18826 / 53624, 1021 ins, 2989 del, 14816 sub ] [PARTIAL] exp/tri2b/decode_test/wer_9_1.0
%WER 33.81 [ 19546 / 57818, 1120 ins, 2959 del, 15467 sub ] exp/tri1/decode_test/wer_11_1.0


There are still problems.
The tri3b system failed.
These scores could be better.

** Goals for Thursday:
- TODO Softunisia: Fix problems with new test set. Why is tri3b failing?
- TODO Multilang: Get WER scores for 17 languages.
- TODO Multilang: I-vectors (should I incorporate this now or wait?)
- TODO Multilang: ditto for bottleneck layer.
- TODO GP: Get Portuguese working. I should be able to do this since I've done it before. (copy build on a-team workstation?
- TODO GP: Get chain model WER scores for Japanese and German.
- TODO AMPTA: LRL data.

* DAR <2018-01-09 Tue>
**  Goals for Tuesday set Monday:
- TODO AMPTA: Investigate Data availability.
- TODO GP: Get chain model results for gp languages.
- TODO GALE Arabic: Build cd gmm hmm (tri3b) system.
- TODO Multilang: Run with more languages.

- WER scores 
| language | tri3b WER | chain WER | MTL WER on 7 languages |
| Arabic dev | 55.98 | 51.17 | |
| Bulgarian dev | 24.78      | 19.47 | 22.33 |
| Croatian dev | 28.53 | 27.57 | 28.77 |
| Czech dev | 43.72 | 50.14 | |
| French dev | 93.41 | | |
| German dev | 38.04 | | |
| Hausa dev | 24.64 | | 21.77 |
| Japanese dev | 6.15 | | 4.97 |
| Korean dev | 25.64 | | 24.28 |
| Mandarin dev | 19.07 | 15.52 | 17.94 |
| Polish dev | 48.23 | | |
| Portuguese dev | 24.11 | | 21.30 |
| Russian dev | 55.81 | 49.23 | |
| Spanish dev | 42.97 | | |
| Swedish dev | 62.07 | | | |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | |
| Vietnamese dev | 37.49 | | |

- Multilang:
I started a new run with 17 languages: 
gp_arabic gp_bulgarian gp_croatian gp_czech gp_french gp_german gp_hausa gp_japanese gp_korean gp_mandarin gp_polish gp_portuguese gp_russian gp_spanish gp_swedish gp_turkish gp_vietnamese 
I want to know if I can run with all these languages. 
For some of these  languages (French) the baseline tri3b systems are not ready yet.
Results should be ready tomorrow.

- GALE Arabic:
I am working with this corpus because it comes from broadcast news.
I am building the tri3b system on the b-team workstation.
GALE Arabic is a pretty big corpus.
It might take a while to build  the  tri3b system.
I just looked at the alignments analysis file.
There are 319 hours of speech in the GALE Arabic  corpus.

Experiment:
Run multilang on gp_arabic and gale_arabic.
What happens?
Does gp_arabic improve?
Does it beat chain models?

** Goals for Wednesday:
- TODO Multilang: Get MTL WER results  after training on 17 languages.
- TODO GALE Arabic: Get tri3b WER results.
- TODO GALE Arabic: Get chain model WER results.
- TODO GP: Get WERs for chain models (Hausa first)
- TODO GP: Improve tri3b baselines (How?)

* DAR <2018-01-08 Mon>
** Goals for Next Week:
- TODO Multilang: Write ideas for Paper. 
- TODO Multilang: Look for data from different sources in our languages. (gale arabic)
I am starting to train a GALE Arabic system.
I'd like to do this on the b-team GPU machine.
- TODO Softunisia: Write an end 2 end recipe suitable for submission to the kaldi repository.
- TODO African French ditto for African French (Yaounde)
- TODO Heroico: Write tn.
- TODO Heroico: Contact Dan and Yenda about publishing the recipe.
- TODO African French: Write outline of paper.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO Multilang: Extend build script to use i-vectors and bottleneck features.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO Multilang: Add well behaving GP languages  to build.
- TODO GP: GEt chain model results for all languages in gp. 
I got chain model results for Croatian.

| language | tri3b WER | chain WER | MTL WER |
| Arabic dev | 55.98 | 51.17 | |
| Bulgarian dev | 24.78      | 19.47 | 22.33 |
| Croatian dev | 28.53 | 27.57 | 28.77 |
| Czech dev | 43.72 | | |
| French dev | 93.41 | | |
| German dev | 38.04 | | |
| Hausa dev | 24.64 | | 21.77 |
| Japanese dev | 6.15 | | 4.97 |
| Korean dev | 25.64 | | 24.28 |
| Mandarin dev | 19.07 | 15.52 | 17.94 |
| Polish dev | 48.23 | | |
| Portuguese dev | 24.11 | | 21.30 |
| Russian dev | 55.81 | 49.23 | |
| Spanish dev | 42.97 | | |
| Swedish dev | 62.07 | | | |
| tamil dev | | | |
| Thai dev | | | |
| Turkish dev | 75.25 | | |
| Vietnamese dev | 37.49 | | |


**
* DAR <2018-01-05 Fri>
** Goals for Friday set Thursday:
- TODO MTL Paper: Write background.
- TODO Multilang: Investigate tree file.
nnet3-copy --binary=false exp/nnet3/multi/final.raw text.raw
this command converts the final.raw file from binary to text and outputs it into text.raw.
text.raw is the file containing the neural network.
The parameters of the components of the nn are stored in this file.

- TODO Multilang: Decode with new system trained on 7 languages.
It looks like the HCLG.fst file from the tri3b system can be used for decoding with the new system.

* DAR <2018-01-04 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Get results for run with i-vectors and 7 languages.
The training finished, but decoding failed.
I had this problem before.
The decoding graph is not built.
The tree file is missing.
Where is the tree file?
When does it get built?
What is it for?

- TODO MTL Paper: Write background.
- TODO gp_french: Get results after running with lm trained on prompts.
The results are still horrible.
I'm ready to give up on this.

** Goals for Friday:
- TODO MTL Paper: Write background.
- TODO Multilang: Investigate tree file.
- TODO Multilang: Decode with new system trained on 7 languages.

* DAR <2018-01-02 Tue>
** Goals for Wednesday set Tuesday:
- TODO Work with Michelle's draft of the MTL paper.
- TODO Get results from multilang trilingual run.
%WER 18.19 [ 3324 / 18274, 367 ins, 695 del, 2262 sub ] exp/nnet3/multi_bnf/gp_mandarin/decode_dev/wer_13_0.0
%WER 17.94 [ 3278 / 18274, 396 ins, 667 del, 2215 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_13_0.0

So the WER went up aft adding the Bulgarian.
Did I do anything else?
18.19 is still better than the tri3b 19.07.

Here are the gp_mandarin WERs:

%WER 46.38 [ 9973 / 21502, 623 ins, 2029 del, 7321 sub ] exp/mono/decode_eval/wer_13_0.0
%WER 36.63 [ 6694 / 18274, 351 ins, 1489 del, 4854 sub ] exp/mono/decode_dev/wer_15_0.0
%WER 32.78 [ 7049 / 21502, 676 ins, 1358 del, 5015 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 32.02 [ 6886 / 21502, 747 ins, 1223 del, 4916 sub ] exp/tri3b/decode_eval.si/wer_16_0.0
%WER 31.17 [ 6703 / 21502, 725 ins, 1244 del, 4734 sub ] exp/tri2b/decode_eval/wer_17_0.0
%WER 27.48 [ 5909 / 21502, 631 ins, 1170 del, 4108 sub ] exp/tri3b/decode_eval/wer_17_0.5
%WER 23.89 [ 4365 / 18274, 407 ins, 850 del, 3108 sub ] exp/tri3b/decode_dev.si/wer_17_0.5
%WER 23.51 [ 4296 / 18274, 400 ins, 825 del, 3071 sub ] exp/tri1/decode_dev/wer_17_0.5
%WER 22.54 [ 4119 / 18274, 475 ins, 724 del, 2920 sub ] exp/tri2b/decode_dev/wer_17_0.0
%WER 21.52 [ 4628 / 21502, 587 ins, 893 del, 3148 sub ] exp/chain/tdnn1a_sp/decode_eval/wer_11_0.0
%WER 19.07 [ 3484 / 18274, 435 ins, 644 del, 2405 sub ] exp/tri3b/decode_dev/wer_17_0.5
%WER 15.52 [ 2836 / 18274, 359 ins, 585 del, 1892 sub ] exp/chain/tdnn1a_sp/decode_dev/wer_11_0.5

| model | dev WER |
| mono | 36.63 |
| tri1 | 23.51 |
| tri2b | 22.54 |
| tri3b | 19.07 |
| chain | 15.52 |

- TODO Investigate Paper publication venues.

** Goals for Thursday:
- TODO Multilang: Get results for run with i-vectors and 7 languages.
- TODO MTL Paper: Write background.
- TODO gp_french: Get results after running with lm trained on prompts.

* DAR <2018-01-02 Tue>
** Goals for January:
- TODO Setup work environment on b-team GPU workstation.
- TODO Multilang: Flesh out paper idea. Search for publication venue.
- TODO Multilang: Extend build script to use i-vectors and bottleneck features.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO Multilang: Add well behaving GP languages  to build.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO Softunisia: Write an end 2 end recipe suitable for submission to the kaldi repository.
- TODO African French ditto for African French (Yaounde)
- TODO Heroico: Write tn.
- TODO Heroico: Contact Dan and Yenda about publishing the recipe.
- TODO African French: Write outline of paper.

- Current WER scores for GP:
| language | tri3b| chain |
| Arabic dev | 55.98 | 51.17 |
| Bulgarian dev | 24.78      | 19.47 |
| Croatian dev | 28.53 | |
| Czech dev | 43.72 | |
| French dev | 93.41 | |
| German dev | 38.04 | |
| Hausa dev | 24.64 | |
| Japanese dev | 6.15 | |
| Korean dev | 25.64 | |
| Mandarin dev | 19.07 | |
| Polish dev | 48.23 | |
| Portuguese dev | 24.11 | |
| Russian dev | 55.81 | 49.23 |
| Spanish dev | 42.97 | |
| Swedish dev | 62.07 | |
| tamil dev | | |
| Thai dev | | |
| Turkish dev | 75.25 | |
| Vietnamese dev | 37.49 | |

** Goals for Wednesday
- TODO Work with Michelle's draft of the MTL paper.
- TODO Get results from multilang trilingual run.
- TODO Investigate Paper publication venues.

* DAR <2017-12-13 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Multilang: Write a script that runs end to end and uses the above setup commands:
The commands were:
mkdir -p data/gp_japanese data/gp_mandarin
#  link source data/ train directories into building directory:
ln -s ~/gp_japanese/s5/data/train data/gp_japanese
ln -s ~/gp_mandarin/s5/data/train data/gp_mandarin
# make experiment directories under multilang building directory:
mkdir -p exp/gp_japanese exp/gp_mandarin
# link source tri3b alignment directories into multilang building directory:
ln -s ~/gp_japanese/s5/exp/tri3b_ali exp/gp_japanese
ln -s ~/gp_mandarin/s5/exp/tri3b_ali exp/gp_mandarin

Here are the WER scores I got after decoding:
%WER 17.94 [ 3278 / 18274, 396 ins, 667 del, 2215 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_13_0.0

I am going to work on building chain models for gp_mandarin.
then I can compare the multilang results with a neural network model.

- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- DONE African French: Write outline of paper.
- TODO Softunisia: Chain models.
I worked on the lexicon.
I restricted the words to only those appearing in the training set.
I think I need to include the test data in the list of words.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

** Goals for Thursday:
- TODO Softunisia: Incorporate test set words into lexicon.
- TODO Softunisia:  Convert phones in  lexicon to IPA utf8.
- TODO Softunisia: Test new lexicon with gmm hmms and chain models.
- TODO Multilang: Get gp_mandarin chain model results.
- TODO Multilang: Incorporate i-vectors and bottlenect layer into neural network model.
- TODO Multilang: Refine paper outline ( focus on incorporating diverse genres: Broadcast News and Read Speech).
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

* DAR <2017-12-12 Tue>
** Goals for Tuesday set Monday:
- TODO Multilang: Minimal example. Get scripts to do the right thing with the decoding graph.
I'm going to try to list the steps for a minimal exapmle:
- Languages: 
gp_japanese and gp_mandarin

- Locations:
~/gp_japanese/s5 and ~/gp_mandarin/s5

These two directories are the locations where I built the tri3b models.

- multilang Building directory:
~/multilang/s5
This is where we will build new multilang models.

- move to the multilang building directory
cd ~/multilang/s5

- make data directories in building directory:
mkdir -p data/gp_japanese data/gp_mandarin

- link source data/ train directories into building directory:
ln -s ~/gp_japanese/s5/data/train data/gp_japanese
ln -s ~/gp_mandarin/s5/data/train data/gp_mandarin

- make experiment directories under multilang building directory:
mkdir -p exp/gp_japanese exp/gp_mandarin

- link source tri3b alignment directories into multilang building directory:
ln -s ~/gp_japanese/s5/exp/tri3b_ali exp/gp_japanese
ln -s ~/gp_mandarin/s5/exp/tri3b_ali exp/gp_mandarin

- feature extraction:
I wanted to avoid plp features and speed perturbation, but it is happening automatically somehow.


- link dev data:
ln -s ~/gp_mandarin/s5/data/dev data/gp_mandarin/
This and the following commands are  for linking the directory that is needed for decoding

- link lang directory
ln -s ~/gp_mandarin/s5/data/lang_test data/gp_mandarin/

- link tree directory
ln -s ~/gp_mandarin/s5/exp/tri3b/tree  exp/nnet3/multi/gp_mandarin/

- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

** Goals for Wednesday:
- TODO Multilang: Write a script that runs end to end and uses the above setup commands
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?


* DAR <2017-12-11 Mon>
**  Goals for Monday set Friday:
- DONE Multilang: Decode the target Russian and Spanish with the new hybrid multilang system. 
I am marking this goal as DONE even though it was modified.
I had a misunderstanding about how multilang works and I might still have a misunderstandig. 
I thought the source and target languages were supposed to be different.
My understanding now is that the input and output languages are the same.
The process is meant to improve all(?) the input languages. 
In particular it improves the low resource language.
But the low resource language is one of the input languages.
So I modified my minimal example (and now it is really minimal) to only work with japanese and mandarin.
The idea is that the parameters from these two languages are shared. 
One model set is output (is this right?).
That model can be used to decode both Japanese and mandarin (really?).
Anyway, I am not sure about all the steps in this multilang process. 
I ran the multilang training scripts without any bells and whistles. 
No i-vectors, no pitch and no bottleneck features; I don't even thik I did speed perturbation. 
A neural network was trained. 
Then, I think what happens is that the final layer is adjusted for each language.
So  2 sllightly different models are written, one for Japanese and one for Mandarin?
Then I decoded the Mandaring dev data with the new mandarin model.
I had to do some of this by hand, the scripts did not make sense to me.
Specifically, the scripts wanted to make the decoding graph in the old tri3b directory.
This can't be right.
Anyway, I copied the tree and the lang_test directories from the tri3b model set to the new nnet3/multi model directories.
I ran mkgraph in the new mandarin directory.
Then I decoded with this graph.
The results:
%WER 18.17 [ 3320 / 18274, 373 ins, 697 del, 2250 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_14_0.0

I don't know yet if this is good or not, but at least it is better than the tri3b WER wich was 19.07.
I would have to train the neural network models without the multilang than compare them.
But this seems pretty good since I am not using pitch which was used in the tri3b results.
Anyway, I am happy since I got a minimal example to run.
I'll have to work on the script to make it run without any intervention. 

- TODO Softunisia: Write an end 2 end recipe suitable for submission to the kaldi repository.
I made some progress on this today.
I got the sarraj data from Zac.
I am working on incorporating this into the test set.
I worked a lot on getting the westpoint data incorporated as test data.
I'd rather use data that we can put on openslr.org as test data.

- TODO African French ditto for African French (Yaounde)
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- DONE Softunisia Recipe: Test set. (Zac's transcription of Libian data, Westpoint?)
I got a lot done on this today.
In fact I am probably almost finished with this, so I'm going to mark it DONE.

- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.


** Goals for Tuesday:
- TODO Multilang: Minimal example. Get scripts to do the right thing with the decoding graph.
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

* DAR <2017-12-08 Fri>
**  Goals for Friday set Thursday:
- TODO Multilang: Simplify multilang recipe. Hard code sp, hires, pitch, bnf.
I did not do this.
Instead I reverted back to the original script.
I set bnf, speed perturb, ivector and ivector to false.
I am trying to get a minimal system to run.
I also restricted the source languages to only the  two languages Japanese and Mandarin.
I made a lot of progress today with this strategy.
In fact, I got passed the training stage and adjustment of priors.
I am trying to get the decoding to run.
I am decoding on Russian for now.

- TODO Softunisia: Write official recipe with chain models (test set?).
I also made a lot of progress on this goal today.
I am working on a test set.
For now I am working with the West Point test data.
I trained up to tri1.
- TODO GP: take a pass on all the languages to try to improve WERs.
- TODO S2S: minimal example.
- TODO Write tn for Heroico.

* DAR <2017-12-07 Thu>
** Goals for Thursday set Wednesday:
- DONE Multilang: Train the i-vector extractor.
I am going back to the beginning.
I need to get the variables set correctly.
Variables include the suffixes for directories.
I am including pitch, speed perturbation and bottlenec features all of which makes suffixes get appended to directory names.
The suffixes and affixes are a mess.
I got pretty far with this today, but the directory names is too messed up.
I am only working with 5 languages:
gp_hausa gp_japanese gp_korean gp_mandarin  gp_portuguese            
- TODO Softunisia: Train chain models.
- TODO SofTunisia: Make an official kaldi recipe.
I got started on this.
- TODO Heroico: Contact Dan and Yenda.
I sent a message.
Have not heard back yet.

- TODO S2S: Minimal example.
- TODO GP: Take another pass on each language to try to improve WER scores.

** Goals for Friday:
- TODO Multilang: Simplify multilang recipe. Hard code sp, hires, pitch, bnf.
- TODO Softunisia: Write official recipe with chain models (test set?).
- TODO GP: take a pass on all the languages to try to improve WERs.
- TODO S2S: minimal example.
- TODO Write tn for Heroico.
* DAR <2017-12-06 Wed>
** Goals for Wednesday: set Tuesday
- DONE Brief Reggie on Multilang project.
- DONE GP Russian: Train through tri3b_ali.

%WER 73.73 [ 13468 / 18266, 933 ins, 2027 del, 10508 sub ] exp/mono/decode_eval/wer_8_1.0
%WER 69.25 [ 13131 / 18962, 644 ins, 2626 del, 9861 sub ] exp/mono/decode_dev/wer_9_1.0
%WER 60.97 [ 11137 / 18266, 1537 ins, 1140 del, 8460 sub ] exp/tri3b/decode_eval.si/wer_16_1.0
%WER 59.76 [ 10916 / 18266, 1053 ins, 1508 del, 8355 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 59.58 [ 10883 / 18266, 1277 ins, 1282 del, 8324 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 58.33 [ 10655 / 18266, 1538 ins, 1059 del, 8058 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 57.71 [ 10943 / 18962, 1422 ins, 1346 del, 8175 sub ] exp/tri3b/decode_dev.si/wer_16_1.0
%WER 56.92 [ 10793 / 18962, 1085 ins, 1678 del, 8030 sub ] exp/tri1/decode_dev/wer_16_1.0
%WER 56.58 [ 10729 / 18962, 1217 ins, 1557 del, 7955 sub ] exp/tri2b/decode_dev/wer_16_1.0
%WER 55.81 [ 10582 / 18962, 1479 ins, 1274 del, 7829 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 49.23 [ 8993 / 18266, 642 ins, 1329 del, 7022 sub ] exp/chain/tdnn1c_sp/decode_eval/wer_11_1.0
%WER 49.21 [ 8988 / 18266, 787 ins, 1160 del, 7041 sub ] exp/chain/tdnn1c_sp_online/decode_eval/wer_10_1.0
%WER 46.87 [ 8888 / 18962, 883 ins, 1214 del, 6791 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_9_1.0
%WER 46.78 [ 8870 / 18962, 855 ins, 1233 del, 6782 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_9_1.0


- Current WER scores for GP:
| language | tri3b| chain |
| Arabic dev | 70.73 | 64.57 |
| Bulgarian dev | 24.78      | 19.47 |
| Croatian dev | 28.53 | |
| Czech dev | 43.72 | |
| French dev | 93.41 | |
| German dev | 38.04 | |
| Hausa dev | 24.64 | |
| Japanese dev | 6.15 | |
| Korean dev | 25.64 | |
| Mandarin dev | 19.07 | |
| Polish dev | 48.23 | |
| Portuguese dev | 24.11 | |
| Russian dev | 55.81 | 49.23 |
| Spanish dev | 42.97 | |
| Swedish dev | 62.07 | |
| tamil dev | | |
| Thai dev | | |
| Turkish dev | 75.25 | |
| Vietnamese dev | 37.49 | |

- TODO Multilang: Fix problem with alignment file location.
I had to soft link the Russian directories.
Here is the command I am running:

steps/train_lda_mllt.sh --cmd run.pl --num-iters 3 --splice-opts "--left-context=3 --right-context=3" --boost-silence 1.5 6000 75000 data/gp_russian/train_sp_hires data/gp_russian/lang exp/gp_russian/tri3b_ali exp/gp_russian/nnet3_pitch/tri3b 

That command ran, but when I go to run the master script it requires other arguments:
This command seems to run:
steps/train_lda_mllt.sh --cmd run.pl --num-iters 13 --splice-opts --left-context=3 --right-context=3 --boost-silence 1.5 6000 75000 data/gp_russian/train_sp_hires_pitch data/gp_russian/lang exp/gp_russian/tri3b_ali_sp exp/gp_russian/nnet3_pitch/tri3b
Notice the _pitch suffix.
I've got to fix these problems in the script.

- TODO Multilang: Minimal example.
- TODO Multilang: Train a global i-vecgor extractor on pooled data.
I think the lda_mllt training above is a step towards training the i-vector extractor.

- TODO S2S Demo: Write files to correct locations for decoding.
- TODO Heroico: Contact Dan and Yenda ( I think I'm done).
- DONE Softunisia: Get stage 19 hypotheses to Zac. (6 speakers)

** Goals for Thursday:
- TODO Multilang: Train the i-vector extractor.
- TODO Softunisia: Train chain models.
- TODO SofTunisia: Make an official kaldi recipe.
- TODO Heroico: Contact Dan and Yenda.
- TODO S2S: Minimal example.
- TODO GP: Take another pass on each language to try to improve WER scores.
  
* DAR <2017-12-05 Tuef>
**  Goals for Tuesday set Monday:
- DONE GP Russian: Convert Romanized Russian to UTF8 with charmap given in documentation.
Here are the WERs I have so far:
%WER 73.73 [ 13468 / 18266, 933 ins, 2027 del, 10508 sub ] exp/mono/decode_eval/wer_8_1.0
%WER 69.25 [ 13131 / 18962, 644 ins, 2626 del, 9861 sub ] exp/mono/decode_dev/wer_9_1.0
%WER 59.76 [ 10916 / 18266, 1053 ins, 1508 del, 8355 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 56.92 [ 10793 / 18962, 1085 ins, 1678 del, 8030 sub ] exp/tri1/decode_dev/wer_16_1.0
%WER 56.58 [ 10729 / 18962, 1217 ins, 1557 del, 7955 sub ] exp/tri2b/decode_dev/wer_16_1.0

- Current WER scores for GP:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | 26.24 | 24.11 | | |
| Russian dev | 69.25 | 56.92 | 56.58 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

- TODO S2S Demo: Decode input with tri2b English models.
Worked on this, but still have problems writing files to the correct place. 
- TODO S2S Demo: Repeat with tri2b Spanish models what was done for English.
- TODO Heroico: Contact Dan and Yenda about recipe.
- TODO Multilang: Minimal Example with 14 GP languages.
The scripts expect an alignment file under tri3b_ali_sp.
The alignments are actually under tri3b_ali_train_sp.


** Goals for Wednesday:
- TODO Brief Reggie on Multilang project.
- TODO GP Russian: Train through tri3b_ali.
- TODO Multilang: Fix problem with alignment file location.
- TODO Multilang: Minimal example.
- TODO Multilang: Train a global i-vecgor extractor on pooled data.
- TODO S2S Demo: Write files to correct locations for decoding.
- TODO Heroico: Contact Dan and Yenda ( I think I'm done).
- TODO Softunisia: Get stage 19 hypotheses to Zac. (6 speakers)

* DAR <2017-12-04 Mon>
** Goals for Next Week:
- TODO Multilang: Train SAT models for all gp languages.
GP Russian is still a mystery.
The dictionary and the transcriptions seem to come from different places.
I think the problem is with the transcripts.
Heer is what the GP documentation says:
I. Dictionary Generation and Format
The phone-based pronunciation dictionary for Russian contains the pronunciations of all word forms found in the transcription data of the GlobalPhone audio recordings of this language. 
The word forms are given in original Russian Cyrillic script in UTF-8 encoding like it appears in the transcription of the GlobalPhone speech and  text corpus in the directory /trl. 

No, they are not.

The dictionary can also be provided in Romanized script using ASCII encoding as appearing in the directory /rmn in the speech & text corpus. 
The conversion between the Roman and the Cyrillic script is given in the section Romanization below.

Maybe I can convert from the romanization to utf8?
Yes, I think this is what I need to do.
II. Romanization
The following list describes the original Cyrillic characters used in Russian script, the corresponding UTF-8 code point, and the Romanized form as used in the Romanized transcription files. 
The Romanization and Back-transformation can be achieved by simple one-to-one reversible substitution rules based on regular expressions, e.g. in case of tcl use the following line to convert
w into : regsub -all {w} $temp "\u0432" temp
| Original Russian character | Romanized Character | Unicode code point | Description |
|  | a | U+0430 | CYRILLIC SMALL LETTER A |
| \ b | U+0431 | CYRILLIC SMALL LETTER BE |
 w U+0432 CYRILLIC SMALL LETTER VE
 g U+0433 CYRILLIC SMALL LETTER GHE
 d U+0434 CYRILLIC SMALL LETTER DE
 ye U+0435 CYRILLIC SMALL LETTER IE
 jscH U+0436 CYRILLIC SMALL LETTER ZHE
 z U+0437 CYRILLIC SMALL LETTER ZE
 i U+0438 CYRILLIC SMALL LETTER I
 j U+0439 CYRILLIC SMALL LETTER SHORT I
 k U+043A CYRILLIC SMALL LETTER KA
 l U+043B CYRILLIC SMALL LETTER EL
 m U+043C CYRILLIC SMALL LETTER EM
 n U+043D CYRILLIC SMALL LETTER EN
 o U+043E CYRILLIC SMALL LETTER O
 p U+043F CYRILLIC SMALL LETTER PE
 r U+0440 CYRILLIC SMALL LETTER ER
 s U+0441 CYRILLIC SMALL LETTER ES
 t U+0442 CYRILLIC SMALL LETTER TE
 u U+0443 CYRILLIC SMALL LETTER U
 f U+0444 CYRILLIC SMALL LETTER EF
 h U+0445 CYRILLIC SMALL LETTER HA
 tS U+0446 CYRILLIC SMALL LETTER TSE
 tscH U+0447 CYRILLIC SMALL LETTER CHE
 sch U+0448 CYRILLIC SMALL LETTER SHA
 schTsch U+0449 CYRILLIC SMALL LETTER SHCHA
 Q U+044A CYRILLIC SMALL LETTER HARD SIGN
 i2 U+044B CYRILLIC SMALL LETTER YERU
 ~ U+044C CYRILLIC SMALL LETTER SOFT SIGN
 e U+044D CYRILLIC SMALL LETTER E
 yu U+044E CYRILLIC SMALL LETTER YU
 ya U+044F CYRILLIC SMALL LETTER YA
 A U+0410 CYRILLIC CAPITAL LETTER A
 B U+0411 CYRILLIC CAPITAL LETTER BE
 W U+0412 CYRILLIC CAPITAL LETTER VE
 G U+0413 CYRILLIC CAPITAL LETTER GHE
 D U+0414 CYRILLIC CAPITAL LETTER DE
 YE U+0415 CYRILLIC CAPITAL LETTER IE
 JscH U+0416 CYRILLIC CAPITAL LETTER ZHE
 Z U+0417 CYRILLIC CAPITAL LETTER ZE
 I U+0418 CYRILLIC CAPITAL LETTER I
 J U+0419 CYRILLIC CAPITAL LETTER SHORT I
 K U+041A CYRILLIC CAPITAL LETTER KA
 L U+041B CYRILLIC CAPITAL LETTER EL
 M U+041C CYRILLIC CAPITAL LETTER EM
 N U+041D CYRILLIC CAPITAL LETTER EN
 O U+041E CYRILLIC CAPITAL LETTER O
 P U+041F CYRILLIC CAPITAL LETTER PE
 R U+0420 CYRILLIC CAPITAL LETTER ER
 S U+0421 CYRILLIC CAPITAL LETTER ES
 T U+0422 CYRILLIC CAPITAL LETTER TE
 U U+0423 CYRILLIC CAPITAL LETTER U
 F U+0424 CYRILLIC CAPITAL LETTER EF
 H U+0425 CYRILLIC CAPITAL LETTER HA
 TS U+0426 CYRILLIC CAPITAL LETTER TSE
 TscH U+0427 CYRILLIC CAPITAL LETTER CHE
 Sch U+0428 CYRILLIC CAPITAL LETTER SHA
 SchTsch U+0429 CYRILLIC CAPITAL LETTER SHCHA
 Q U+042A CYRILLIC CAPITAL LETTER HARD SIGN
 I2 U+042B CYRILLIC CAPITAL LETTER YERU
 ~ U+042C CYRILLIC CAPITAL LETTER SOFT SIGN
 E U+042D CYRILLIC CAPITAL LETTER E
 Yu U+042E CYRILLIC CAPITAL LETTER YU
 Ya U+042F CYRILLIC CAPITAL LETTER YA

- TODO Multilang: USE alignments from SAT models to start multilang building process.
I am starting with 14 gp languages.
- TODO Heroico: Contact Dan Povey and Yenda about next step (am I finished? Is the recipe ready?)
- TODO Write TN.
- TODO S2S: Minimal example using English mini_librispeech and Heroico Spanish.
I worked a little on this today.
I have a script that invokes a recording program called rec.
I can extract mfcc features and cmvn them.

- TODO Softunisia: Retrain and get transcripts to Zac.
Zac called me.
He had problems with the transcripts  I sent him.
He was able to retrieve them intact from my github webpage.


** Goals for Tuesday:
- TODO GP Russian: Convert Romanized Russian to UTF8 with charmap given in documentation.
- TODO S2S Demo: Decode input with tri2b English models.
- TODO S2S Demo: Repeat with tri2b Spanish models what was doen for English.
- TODO Heroico: Contact Dan and Yenda about recipe.
- TODO Multilang: Minimal Example with 14 GP languages.

* DAR <2017-12-01 Fri>
** oals for Friday set Thursday:
- DONE Heroico: 1e experiment with 7 epochs instead of 10 to avoid overfitting.
./local/chain/compare_wer.sh exp/chain/tdnn1d_sp exp/chain/tdnn1e_sp
System                tdnn1d_sp tdnn1e_sp
WER devtest       52.78     52.21
WER native       55.32     53.43
nonnative     64.35     61.03
WER test       60.28     57.70
 Final train prob        -0.0229   -0.0250
 Final valid prob        -0.0683   -0.0678
 Final train prob (xent)   -0.7525   -0.7887
 Final valid prob (xent)   -1.0296   -1.0419

-  info
exp/chain/tdnn1e_sp:
 num-iters=105
 nj=1..1
 num-params=6.6M
 dim=40+100->1392
 combine=-0.036->-0.033
 xent:train/valid[69,104,final]=(-1.20,-0.917,-0.789/-1.35,-1.16,-1.04)
 logprob:train/valid[69,104,final]=(-0.049,-0.030,-0.025/-0.082,-0.075,-0.068)

- Word Error Rates on folds
%WER 61.03 [ 5624 / 9215, 630 ins, 727 del, 4267 sub ] exp/chain/tdnn1e_sp/decode_nonnative/wer_8_1.0
%WER 57.70 [ 9644 / 16713, 1249 ins, 1040 del, 7355 sub ] exp/chain/tdnn1e_sp/decode_test/wer_7_1.0
%WER 53.43 [ 4006 / 7498, 558 ins, 408 del, 3040 sub ] exp/chain/tdnn1e_sp/decode_native/wer_7_1.0
%WER 52.21 [ 3994 / 7650, 585 ins, 456 del, 2953 sub ] exp/chain/tdnn1e_sp/decode_devtest/wer_9_1.0

| fold | 1a | 1b | 1c | 1d | 1e |
| devtest | 54.46 | 54.20 | 54.16 | 52.78 | 52.21 |
| native |  62.14 | 62.32 | 61.70 | 55.32 | 53.43 |
| nonnative | 70.58 | 71.20 | 71.68 | 64.35 | 61.03 |
| test | 66.85 | 67.21 | 67.25 | 60.28 | 57.70 |

- TODO Heroico: Write tn .
- TODO AMTA2018: Work on Minimal Example (English mini_librispeechSpanish GP/Heroico?)
- TODO Multilang: Train up to Russian tri3b_ali.

* DAR <2017-11-30 Thu>
** Goals for Thursday set Wednesday:
- DONE SofTunisia: Retrain with Zac's new dictionary.
The tri3b decoding finished.
I have to get it to Zac.
I tried to send him the file in an email.

- DONE Heroico: 1d remove proportional shrinking.
I  removed the proportional shrinking option but I also added the l2 regularization on the 8 layer setup.

%WER 64.35 [ 5930 / 9215, 726 ins, 734 del, 4470 sub ] exp/chain/tdnn1d_sp/decode_nonnative/wer_7_1.0
%WER 60.28 [ 10074 / 16713, 1324 ins, 1175 del, 7575 sub ] exp/chain/tdnn1d_sp/decode_test/wer_7_1.0
%WER 55.32 [ 4148 / 7498, 600 ins, 435 del, 3113 sub ] exp/chain/tdnn1d_sp/decode_native/wer_7_1.0
%WER 52.78 [ 4038 / 7650, 708 ins, 401 del, 2929 sub ] exp/chain/tdnn1d_sp/decode_devtest/wer_8_1.0

# | fold | 1a | 1b | 1c | 1d | 1e |
#| devtest | 54.46 | 54.20 | 54.16 | 52.78 |
#| native |  62.14 | 62.32 | 61.70 | 55.32 |
#| nonnative | 70.58 | 71.20 | 71.68 | 64.35 |
#| test | 66.85 | 67.21 | 67.25 | 60.28 |

The change made a big difference.
There are now 8 layers that use l2 regularization.

- DONE Multilang: Train tri3b models for Portuguese
%WER 43.56 [ 2752 / 6318, 185 ins, 672 del, 1895 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 28.00 [ 1769 / 6318, 265 ins, 279 del, 1225 sub ] exp/tri3b/decode_dev.si/wer_17_0.0
%WER 27.45 [ 1734 / 6318, 274 ins, 240 del, 1220 sub ] exp/tri1/decode_dev/wer_16_0.0
%WER 26.24 [ 1658 / 6318, 244 ins, 252 del, 1162 sub ] exp/tri2b/decode_dev/wer_17_0.0
%WER 24.11 [ 1523 / 6318, 249 ins, 225 del, 1049 sub ] exp/tri3b/decode_dev/wer_17_0.0


- TODO Write paper.
I wrote a little bit more on the tn and a very rough draft of the AMTA abstract.

- Current WER scores for GP:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | 26.24 | 24.11 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |


** oals for Friday:
- TODO Heroico: 1e experiment with 7 epochs instead of 10 to avoid overfitting.
- TODO Heroico: Write tn .
- TODO AMTA2018: Work on Minimal Example (English mini_librispeechSpanish GP/Heroico?)
- TODO Multilang: Train up to Russian tri3b_ali.

* DAR  <2017-11-29 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Write paper.
I wrote some more on the Heroico project.
- DONE Heroico: Next experiment (1c)
1b lowered the number of leaves from 3500 to 200 and 3 out of 4 WERS went up.
1c will set number of leaves to 2500.
# | fold | 1a | 1b | 1c |
| devtest | 54.46 | 54.20 | 54.16 |
| native |  62.14 | 62.32 | 61.70 |
| nonnative | 70.58 | 71.20 | 71.68 |
| test | 66.85 | 67.21 | 67.25 |

Inconclusive.
I am resetting the number of leaves to 3500 and removing the proportional shrink in experiment 1d.
- TODO GP: Train tri3b models for all languages.
I started Korean today.
- TODO Multilang: Get minimal example running.
I am going to wait until I have the tri3b models for most of the languages.
I am missing Korean and Portuguese and probably several more.
Korean should be done soon.
I'll finish Portuguese tomorrow.

** Goals for Thursday:
- TODO SofTunisia: Retrain with Zac's new dictionary.
- TODO Heroico: 1d remove proportional shrinking.
- TODO Multilang: Train tri3b models for Portuguese
- TODO Write paper.

* DAR <2017-11-28 Tue>
** Goals for Tuesday set Monday:
- TODO Write something!
I am writing about the Heroico corpus.
I will probably make this into another TN like the one we wrote for the Yaounde Speech Corpus. 

- TODO multilang: Get minimal example running.
- TODO Multilang: Train all languages up through tri3b_ali. 
- TODO Heroico: Tuning experiments.
I finished the 1a run:
1b is running. It uses a smaller (2000 instead of 3500) number of leaves.

| fold | 1a |
| devtest | 54.46 |
| native |  62.14 |
| nonnative | 70.58 |
| test | 66.85 |

SofTunisia: Zac called me. He reminded me that I owed him for stage 17 done with the new dictionary.
I had run it a couple of weeks ago.
I found some problems with the run. 
I fixed the problems and I reran the old stage with the new dictionary.
Hazrat emailed the results to Zac.

** Goals for Wednesday:
- TODO Write paper.
- TODO Heroico: Next experiment (1c)
- TODO GP: Train tri3b models for all languages.
- TODO Multilang: Get minimal example running.

* <2017-11-27 Mon>
** Goals for After leave:
- TODO GlobalPhone: Make a pass through each language to check if text encoding (including casing) matches between text to train acoustic models, text to train lm and text in dictionary.

Bulgarian: I deleted control m characters. 
Croatian: remove cr. LM is not clear . Possible OOV problem. Dictionary has words with upper case first letter.
Czech: remove cr and down case
Korean: The LM is for hangul characters in UTF8.
Polish: Delete control MM and down case
Russian: Remove cr
Swedish: The 3 components are in different encodings!
The GP dictionary is originally in ASCII.
Thai: Weird line termiators.



| language | am train text  | dict | lm |
| Arabic | ascii | ascii | ascii |
| Bulgarian | UTF8 | UTF8 | UTF8 |
| Croatian | UTF8 | UTF8 | ? |
| Czech | UTF8 | UTF8 | UTF8 |
| French | UTF8 | UTF8 | UTF8 |
| German | UTF8 | UTF8 | UTF8 |
| Hausa | ASCII | ASCII | ASCII |
| Japanese | UTF8 | UTF8 | UTF8 |
| Korean | ASCII | ASCII | ASCII |
| Mandarin | ASCII | ASCII | ASCII |
| Polish | UTF8 | UTF8 | UTF8 |
| Portuguese | UTF8 | UTF8 | UTF8 | 
| Russian | UTF8 | UTF8 | UTF8 |
| Spanish | UTF8 | UTF8 | UTF8 |
| Swedish | ASCII | ASCII | UTF8 (ASCII with the exception of  ) |
| Tamil | UTF8 | UTF8 | UTF8 |
| Thai | UTF8 lf nel terminators | UTF8 | UTF8 |
| Turkish | UTF8 | UTF8 |
| Vietnamese | UTF8 | UTF8 (fortran) | UTF8 |

- TODO Heroico: Address Dan's comments ( try to get l2 regularization working)
- TODO Heroico: Modify layers (from 6 to 8)
I am starting from the very beginning.
- TODO Write paper.
- TODO African French: Work with Steve, Luis and Mike Li to get minimal Ultra example working.


** Goals for Tuesday:
- TODO Write something!
- TODO multilang: Get minimal example running.
- TODO Multilang: Train all languages up through tri3b_ali. 
- TODO Heroico: Tuning experiments.

* DAR <2017-11-16 Thu>
**  Goals for Thursday set Wednesday:
- TODO African French: Miniturize. Build up the lm from a minimal working example.
I included the transcripts  for the ca16 test  set  in the lm training set and here are the WER scores:
%WER 31.59 [ 1008 / 3191, 60 ins, 342 del, 606 sub ] exp/mono/decode_ca16/wer_11_0.0

%WER 20.78 [ 663 / 3191, 79 ins, 163 del, 421 sub ] exp/tri3b/decode_ca16.si/wer_17_0.0

%WER 19.24 [ 614 / 3191, 91 ins, 134 del, 389 sub ] exp/tri1/decode_ca16/wer_14_0.0

%WER 17.80 [ 568 / 3191, 76 ins, 130 del, 362 sub ] exp/tri2b/decode_ca16/wer_17_0.0

%WER 16.14 [ 515 / 3191, 85 ins, 99 del, 331 sub ] exp/tri3b/decode_ca16/wer_17_0.0

| model | lm train | lm train + test  |
| mono | 80.38 | 31.59 |
| tri1 | 62.93 | 19.24 |
| tri2b | 60.61 | 17.80 |
| tri3b | 58.26 | 16.14 |

I tried decoding without retraining.

%WER 99.66 [ 3180 / 3191, 13 ins, 1621 del, 1546 sub ] exp/mono/decode_ca16/wer_16_0.5

%WER 20.78 [ 663 / 3191, 79 ins, 163 del, 421 sub ] exp/tri3b/decode_ca16.si/wer_17_0.0

%WER 16.14 [ 515 / 3191, 85 ins, 99 del, 331 sub ] exp/tri3b/decode_ca16/wer_17_0.0

%WER 101.44 [ 3237 / 3191, 81 ins, 1166 del, 1990 sub ] exp/tri2b/decode_ca16/wer_17_1.0

%WER 100.19 [ 3197 / 3191, 43 ins, 1506 del, 1648 sub ] exp/tri1/decode_ca16/wer_17_1.0

This dos not look good so I'm retraining from the beginning.
- TODO Heroico: Address Dan's comments.  remove proportional shrinking
- TODO Heroico: L2 regularization? Does it work with my current version of kaldi?
- TODO Heroico: Modify layers (from 6 to 8)
- TODO Multilang: Take another pass on each language. Try to get comparable WERs to published scores.

Arabic: 
I found some encoding issues in the text files and dictionary.
I corrected them and I am going to run the system build again.
I am including the dev and eval text in the lm training text.
Just in case here are the WER scores before I start this:
%WER 77.57 [ 7015 / 9043, 349 ins, 1201 del, 5465 sub ] exp/mono/decode_dev/wer_16_0.0

%WER 71.79 [ 6492 / 9043, 685 ins, 580 del, 5227 sub ] exp/tri3b/decode_dev.si/wer_17_1.0

%WER 71.49 [ 6465 / 9043, 530 ins, 773 del, 5162 sub ] exp/tri1/decode_dev/wer_17_1.0

%WER 70.80 [ 6402 / 9043, 586 ins, 727 del, 5089 sub ] exp/tri2b/decode_dev/wer_17_1.0

%WER 70.73 [ 6396 / 9043, 679 ins, 571 del, 5146 sub ] exp/tri3b/decode_dev/wer_17_1.0

%WER 64.95 [ 5873 / 9043, 501 ins, 1325 del, 4047 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_9_0.0

%WER 64.57 [ 5839 / 9043, 501 ins, 1325 del, 4013 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_9_0.0

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

- TODO Writing.

- French: I just found out that the French lexicon is all lower case.
My training text is uppercase.

- Heroico 1b results:
%WER 76.91 [ 7087 / 9215, 680 ins, 1165 del, 5242 sub ] exp/chain/tdnn1b_sp/decode_nonnative/wer_8_1.0

%WER 76.22 [ 7024 / 9215, 811 ins, 1007 del, 5206 sub ] exp/chain/tdnn1b_sp_online/decode_nonnative/wer_7_1.0

%WER 75.78 [ 6983 / 9215, 1377 ins, 507 del, 5099 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0

%WER 74.25 [ 5680 / 7650, 1187 ins, 431 del, 4062 sub ] exp/tri3b/decode_devtest.si/wer_16_1.0

%WER 73.76 [ 12328 / 16713, 2541 ins, 804 del, 8983 sub ] exp/tri3b/decode_test.si/wer_17_1.0

%WER 71.87 [ 12012 / 16713, 1342 ins, 1746 del, 8924 sub ] exp/chain/tdnn1b_sp/decode_test/wer_7_1.0

%WER 71.86 [ 5497 / 7650, 530 ins, 959 del, 4008 sub ] exp/mono/decode_devtest/wer_7_1.0

%WER 71.64 [ 6602 / 9215, 646 ins, 939 del, 5017 sub ] exp/mono/decode_nonnative/wer_7_1.0

%WER 71.26 [ 5343 / 7498, 1159 ins, 293 del, 3891 sub ] exp/tri3b/decode_native.si/wer_17_1.0

%WER 71.09 [ 11882 / 16713, 1162 ins, 1999 del, 8721 sub ] exp/chain/tdnn1b_sp_online/decode_test/wer_8_1.0

%WER 69.59 [ 11630 / 16713, 1153 ins, 1643 del, 8834 sub ] exp/mono/decode_test/wer_7_1.0

%WER 67.09 [ 6182 / 9215, 907 ins, 626 del, 4649 sub ] exp/tri1/decode_nonnative/wer_14_1.0

%WER 66.98 [ 5022 / 7498, 503 ins, 700 del, 3819 sub ] exp/mono/decode_native/wer_7_1.0

%WER 66.78 [ 6154 / 9215, 1048 ins, 537 del, 4569 sub ] exp/tri2b/decode_nonnative/wer_15_1.0

%WER 66.64 [ 6141 / 9215, 1226 ins, 425 del, 4490 sub ] exp/tri3b/decode_nonnative/wer_16_1.0

%WER 66.33 [ 5074 / 7650, 921 ins, 481 del, 3672 sub ] exp/tri1/decode_devtest/wer_11_1.0

%WER 66.30 [ 5072 / 7650, 1198 ins, 328 del, 3546 sub ] exp/tri3b/decode_devtest/wer_11_1.0

%WER 65.88 [ 5040 / 7650, 985 ins, 450 del, 3605 sub ] exp/tri2b/decode_devtest/wer_13_1.0

%WER 65.63 [ 4921 / 7498, 551 ins, 741 del, 3629 sub ] exp/chain/tdnn1b_sp/decode_native/wer_7_1.0

%WER 65.05 [ 10872 / 16713, 1725 ins, 959 del, 8188 sub ] exp/tri1/decode_test/wer_13_1.0

%WER 64.90 [ 4866 / 7498, 543 ins, 739 del, 3584 sub ] exp/chain/tdnn1b_sp_online/decode_native/wer_7_1.0

%WER 64.45 [ 10772 / 16713, 2261 ins, 698 del, 7813 sub ] exp/tri3b/decode_test/wer_16_1.0

%WER 64.33 [ 10751 / 16713, 1955 ins, 845 del, 7951 sub ] exp/tri2b/decode_test/wer_14_1.0

%WER 62.54 [ 4689 / 7498, 781 ins, 379 del, 3529 sub ] exp/tri1/decode_native/wer_13_1.0

%WER 61.66 [ 4623 / 7498, 1038 ins, 267 del, 3318 sub ] exp/tri3b/decode_native/wer_15_1.0

%WER 61.28 [ 4595 / 7498, 899 ins, 309 del, 3387 sub ] exp/tri2b/decode_native/wer_13_1.0

%WER 54.00 [ 4131 / 7650, 731 ins, 389 del, 3011 sub ] exp/chain/tdnn1b_sp_online/decode_devtest/wer_7_1.0

%WER 53.62 [ 4102 / 7650, 720 ins, 383 del, 2999 sub ] exp/chain/tdnn1b_sp/decode_devtest/wer_7_1.0

1c:
%WER 71.45 [ 6584 / 9215, 645 ins, 1079 del, 4860 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_7_1.0

%WER 62.26 [ 4668 / 7498, 515 ins, 667 del, 3486 sub ] exp/chain/tdnn1c_sp/decode_native/wer_7_1.0

%WER 54.03 [ 4133 / 7650, 756 ins, 386 del, 2991 sub ] exp/chain/tdnn1c_sp/decode_devtest/wer_7_1.0

| fold | 1a | 1b | 1c |
| devtest | | 53.62 | 54.03 | 
| native | 64.76 | 65.63 | 62.26 | 
| nonnative | 73.85 | 76.91 | 71.45 |
| test | 69.84 | 71.87 | 67.32 |

The change made in experiment 1b definitly made the chain models worse.

1c improved WER scores with the  exception of devtest.

** Goals for After leave:
- TODO GlobalPhone: Make a pass through each language to check if text encoding (including casing) matches between text to train acoustic models, text to train lm and text in dictionary.
- TODO Heroico: Address Dan's comments ( try to get l2 regularization working)
- TODO Heroico: Modify layers (from 6 to 8)

- TODO Write paper.
- TODO African French: Work with Steve, Luis and Mike Li to get minimal Ultra example working.

* DAR <2017-11-15 Wed>
** Goals for Wednesday set Tuesday:
- DONE Heroico: Run ende to end and adress Dan's comments
I ran end2end once with the new data folds.
There is now a devtext fold.
I separated out the data in Heroico that was read from the same promps that were read at USMA.
- DONE Heroico: Write README
The README might need more work.
- DONE Heroico: Get chain model info 
I did this for the 1a run.
- DONE Heroico: Get WERs
Here are the 1a scores:
%WER 75.78 [ 6983 / 9215, 1377 ins, 507 del, 5099 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 74.25 [ 5680 / 7650, 1187 ins, 431 del, 4062 sub ] exp/tri3b/decode_devtest.si/wer_16_1.0
%WER 73.76 [ 12328 / 16713, 2541 ins, 804 del, 8983 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 71.86 [ 5497 / 7650, 530 ins, 959 del, 4008 sub ] exp/mono/decode_devtest/wer_7_1.0
%WER 71.64 [ 6602 / 9215, 646 ins, 939 del, 5017 sub ] exp/mono/decode_nonnative/wer_7_1.0
%WER 71.26 [ 5343 / 7498, 1159 ins, 293 del, 3891 sub ] exp/tri3b/decode_native.si/wer_17_1.0
%WER 69.59 [ 11630 / 16713, 1153 ins, 1643 del, 8834 sub ] exp/mono/decode_test/wer_7_1.0
%WER 67.09 [ 6182 / 9215, 907 ins, 626 del, 4649 sub ] exp/tri1/decode_nonnative/wer_14_1.0
%WER 66.98 [ 5022 / 7498, 503 ins, 700 del, 3819 sub ] exp/mono/decode_native/wer_7_1.0
%WER 66.78 [ 6154 / 9215, 1048 ins, 537 del, 4569 sub ] exp/tri2b/decode_nonnative/wer_15_1.0
%WER 66.64 [ 6141 / 9215, 1226 ins, 425 del, 4490 sub ] exp/tri3b/decode_nonnative/wer_16_1.0
%WER 66.33 [ 5074 / 7650, 921 ins, 481 del, 3672 sub ] exp/tri1/decode_devtest/wer_11_1.0
%WER 66.30 [ 5072 / 7650, 1198 ins, 328 del, 3546 sub ] exp/tri3b/decode_devtest/wer_11_1.0
%WER 65.88 [ 5040 / 7650, 985 ins, 450 del, 3605 sub ] exp/tri2b/decode_devtest/wer_13_1.0
%WER 65.05 [ 10872 / 16713, 1725 ins, 959 del, 8188 sub ] exp/tri1/decode_test/wer_13_1.0
%WER 64.45 [ 10772 / 16713, 2261 ins, 698 del, 7813 sub ] exp/tri3b/decode_test/wer_16_1.0
%WER 64.33 [ 10751 / 16713, 1955 ins, 845 del, 7951 sub ] exp/tri2b/decode_test/wer_14_1.0
%WER 62.54 [ 4689 / 7498, 781 ins, 379 del, 3529 sub ] exp/tri1/decode_native/wer_13_1.0
%WER 61.66 [ 4623 / 7498, 1038 ins, 267 del, 3318 sub ] exp/tri3b/decode_native/wer_15_1.0
%WER 61.28 [ 4595 / 7498, 899 ins, 309 del, 3387 sub ] exp/tri2b/decode_native/wer_13_1.0

- DONE Heroico: symbolic link to chain model script under tuning with 1a affix.
- DONE African French: Miniturize. Start with small LM.

I trained the lm only on the transcripts used for training the acoustic models plus the bic corpus. 
Here are the cd gmm hmm results:
%WER 80.38 [ 2565 / 3191, 108 ins, 722 del, 1735 sub ] exp/mono/decode_ca16/wer_8_0.0
%WER 62.93 [ 2008 / 3191, 117 ins, 556 del, 1335 sub ] exp/tri1/decode_ca16/wer_13_0.5
%WER 62.36 [ 1990 / 3191, 188 ins, 413 del, 1389 sub ] exp/tri3b/decode_ca16.si/wer_10_0.5
%WER 60.61 [ 1934 / 3191, 160 ins, 449 del, 1325 sub ] exp/tri2b/decode_ca16/wer_12_0.5
%WER 58.26 [ 1859 / 3191, 138 ins, 435 del, 1286 sub ] exp/tri3b/decode_ca16/wer_13_1.0

ls -sh exp/tri2b/graph/HCLG.fst 
11M exp/tri2b/graph/HCLG.fst

I am going to include the text from the test set in the training data for the lm.
My hypothesis is that this will make the WER scores go down.
I propose we make a "canned" demo that only works well on phrases from the CA16 test set.

- TODO SOFTunisia: Contact Zac. Where are we?
- TODO Multilang: Get global phone chain models running (latest version?)
- TODO Writing

** Goals for Thursday:
- TODO African French: Miniturize. Build up the lm from a minimal working example.
- TODO Heroico: Address Dan's comments.  remove proportional shrinking
- TODO Heroico: L2 regularization? Does it work with my current version of kaldi?
- TODO Heroico: Modify layers (from 6 to 8)
- TODO Multilang: Take another pass on each language. Try to get comparable WERs to published scores.
- TODO Writing.

* DAR <2017-11-14 Tue>
- Current WER scores:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.52 | 47.35 | 45.08 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

** Goals for Wednesday:
- TODO Heroico: Run ende to end and adress Dan's comments
- TODO Heroico: Write README
- TODO Heroico: Get chain model info 
- TODO Heroico: Get WERs
- TODO Heroico: symbolic link to chain model script under tuning with 1a affix.
- TODO African French: Miniturize. Start with small LM.
- TODO SOFTunisia: Contact Zac. Where are we?
- TODO Multilang: Get global phone chain models running (latest version?)
- TODO Writing

* DAR <2017-11-13 Mon>
**  Goals set Last Week:
- TODO Multilang: build cd gmm hmm systems for all the GP languages (with reference lm).
- TODO Multilang: Build  chain models for each GP language (baselines?)
- TODO Multilang: Do multilang training?
- TODO Incorporate Government-owned corpora into multilang setup. ( WestPoint, ARL Urdu Pashto, Transtac Babel)
- TODO Babel: Search for data sampled at >= 16khz.

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.52 | 47.35 | 45.08 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

* DAR <2017-11-09 Thu>
**  Goals for Thursday set Wednesday:
- TODO Multilang: Why do some languages not have dev sets?
- TODO Multilang Portuguese: What is wrong with GP Portuguese?
I think there are a lot of bad recordings.
I run the utils/fix_data_dir.sh script after doing plp_pitch feature extraction.
This finds around 3k bad files and it makes lists with only the good files.
It also exits with an error status.
I ignore this error status. 
- TODO Multilang: Russian: What is wrong with Russian?
- TODO Multilang: Tamil: What is wrong with GP Tamil?
- TODO Multilang: Thai: What is wrong with GP Thai?
- TODO SOFTunisia: Run the previous stage with the new lexicon.
- TODO Multilang: Fix feature extraction for nnet3 alignment

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 49.37 | 28.13 | 32.38 | 30.98 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Czech dev | 69.95 | 66.71 | 65.96 | 65.60 |
| French | | | | |
| German dev | 68.19 | 57.64 | 56.59 | 54.63 |
| Hausa dev | 36.48 | | |
| Hausa eval | 49.81 | 44.17 | 40.87      | 29.02 |
| Japanese eval | 41.73 | 26.38 | 25.17 | 23.01 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 48.84 | 33.99 | 32.55 | 28.35 |
| Polish dev | 73.33 | 66.21 | 62.52 | 58.56 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 69.30 | 57.16 | 55.50 | 53.56 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 95.62 | 84.70 | 83.90 | 80.29 |

* DAR <2017-11-08 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Mandatory Training (NDA)
- TODO Read more of Thang disertation.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).
Feature vectors are extracted from the acoustic data in several ways:
1. plp and pitch features to train and test the cd gmm hmm models.
2.  low-resolution plp pitch features to get alignments to do chain models
3. high resolution mfcc features to train and test the ivector extractor.
4. ? for training testing the chain models.

The feature vectors extracted in 2. are done in 3 ways by speed perturbing the data.

- DONE Multilang: Incorporate reference LMs.
LMs for Arabic and Turkish are missing.

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 49.37 | 33.71 | 32.38 | 30.98 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Czech dev | 69.95 | 66.71 | 65.96 | 65.60 |
| French | | | | |
| German dev | 68.19 | 57.64 | 56.59 | 54.63 |
| Hausa eval | 49.81 | 44.17 | 41.45 | 33.03 |
| Japanese eval | 41.73 | 26.38 | 25.17 | 23.01 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 48.84 | 33.99 | 32.55 | 28.35 |
| Polish dev | 73.33 | 66.21 | 62.52 | 58.56 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 69.30 | 57.16 | 55.50 | 53.56 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 95.62 | 84.70 | 83.90 | 80.29 |

- SOFTunisia: Zac gave me the new lexicon.
He wants me to rerun the last stage with the new lexicon so we can compare results.
I'm not sure where the latest batch starts.

** Goals for Thursday:
- TODO Multilang: Why do some languages not have dev sets?
- TODO Multilang Portuguese: What is wrong with GP Portuguese?
- TODO Multilang: Russian: What is wrong with Russian?
- TODO Multilang: Tamil: What is wrong with GP Tamil?
- TODO Multilang: Thai: What is wrong with GP Thai?
- TODO SOFTunisia: Run the previous stage with the new lexicon.
- TODO Multilang: Fix feature extraction for nnet3 alignment

* DAR <2017-11-07 Tue>
**  Goals for Tuesday set Monday:
- TODO Mandatory Training (NDA)
- DONE Read chapter 4 of Thang disertation.
-TODO Multilang: Expand tabs to white space in all dictionaries (start from tamil). 
problems with thai.
There is a bad character somewhere.

- TODO Multilang: convert tab to space in <UNK> entry.
- DONE Multilang: make sure all files are in UTF8 (start from tamil).
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.
Korean:
%WER 51.61 [ 18946 / 36707, 488 ins, 2286 del, 16172 sub ] exp/mono/decode_dev/wer_8_0.0
%WER 47.52 [ 18495 / 38920, 374 ins, 3631 del, 14490 sub ] exp/mono/decode_eval/wer_9_0.0
%WER 30.79 [ 11303 / 36707, 543 ins, 1005 del, 9755 sub ] exp/tri1/decode_dev/wer_13_0.5
%WER 29.71 [ 10907 / 36707, 511 ins, 1029 del, 9367 sub ] exp/tri2b/decode_dev/wer_13_0.5
%WER 29.62 [ 10874 / 36707, 522 ins, 781 del, 9571 sub ] exp/tri3b/decode_dev.si/wer_10_1.0
%WER 25.64 [ 9412 / 36707, 474 ins, 670 del, 8268 sub ] exp/tri3b/decode_dev/wer_13_0.5
%WER 14.28 [ 5559 / 38920, 437 ins, 709 del, 4413 sub ] exp/tri3b/decode_eval.si/wer_10_0.0
%WER 11.84 [ 4610 / 38920, 356 ins, 644 del, 3610 sub ] exp/tri2b/decode_eval/wer_12_0.0
%WER 11.83 [ 4604 / 38920, 398 ins, 623 del, 3583 sub ] exp/tri1/decode_eval/wer_11_0.0
%WER 10.96 [ 4265 / 38920, 391 ins, 569 del, 3305 sub ] exp/tri3b/decode_eval/wer_12_0.0

Mandarin:
%WER 52.13 [ 11209 / 21502, 686 ins, 2250 del, 8273 sub ] exp/mono/decode_eval/wer_12_1.0
%WER 48.84 [ 8925 / 18274, 593 ins, 1547 del, 6785 sub ] exp/mono/decode_dev/wer_11_1.0
%WER 38.76 [ 8334 / 21502, 894 ins, 1434 del, 6006 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 37.84 [ 8137 / 21502, 963 ins, 1262 del, 5912 sub ] exp/tri3b/decode_eval.si/wer_14_1.0
%WER 36.84 [ 7921 / 21502, 826 ins, 1374 del, 5721 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 33.99 [ 6211 / 18274, 692 ins, 1053 del, 4466 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 33.44 [ 6110 / 18274, 769 ins, 893 del, 4448 sub ] exp/tri3b/decode_dev.si/wer_14_1.0
%WER 33.24 [ 7147 / 21502, 881 ins, 1197 del, 5069 sub ] exp/tri3b/decode_eval/wer_16_1.0
%WER 32.55 [ 5949 / 18274, 707 ins, 945 del, 4297 sub ] exp/tri2b/decode_dev/wer_15_1.0
%WER 28.35 [ 5180 / 18274, 821 ins, 723 del, 3636 sub ] exp/tri3b/decode_dev/wer_14_1.0

Polish:
%WER 73.89 [ 11214 / 15176, 706 ins, 2922 del, 7586 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 73.33 [ 13183 / 17977, 628 ins, 4142 del, 8413 sub ] exp/mono/decode_dev/wer_12_0.0
%WER 66.83 [ 10142 / 15176, 713 ins, 3754 del, 5675 sub ] exp/tri1/decode_eval/wer_13_0.0
%WER 66.21 [ 11903 / 17977, 981 ins, 3977 del, 6945 sub ] exp/tri1/decode_dev/wer_12_0.0
%WER 63.62 [ 9655 / 15176, 833 ins, 2940 del, 5882 sub ] exp/tri2b/decode_eval/wer_13_0.5
%WER 62.52 [ 11240 / 17977, 1032 ins, 3366 del, 6842 sub ] exp/tri2b/decode_dev/wer_13_0.0
%WER 62.09 [ 9423 / 15176, 745 ins, 2771 del, 5907 sub ] exp/tri3b/decode_eval.si/wer_15_0.0
%WER 61.56 [ 9342 / 15176, 878 ins, 2603 del, 5861 sub ] exp/tri3b/decode_eval/wer_16_0.5
%WER 60.30 [ 10840 / 17977, 809 ins, 3170 del, 6861 sub ] exp/tri3b/decode_dev.si/wer_14_0.0
%WER 58.56 [ 10527 / 17977, 940 ins, 2922 del, 6665 sub ] exp/tri3b/decode_dev/wer_17_0.5

Spanish:
%WER 69.30 [ 13237 / 19101, 753 ins, 2829 del, 9655 sub ] exp/mono/decode_dev/wer_10_0.0
%WER 59.76 [ 7417 / 12411, 585 ins, 1444 del, 5388 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 57.16 [ 10918 / 19101, 949 ins, 2412 del, 7557 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 56.72 [ 10835 / 19101, 1168 ins, 1812 del, 7855 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 55.50 [ 10601 / 19101, 1084 ins, 1998 del, 7519 sub ] exp/tri2b/decode_dev/wer_16_1.0
%WER 53.56 [ 10231 / 19101, 1299 ins, 1500 del, 7432 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 49.49 [ 6142 / 12411, 1042 ins, 711 del, 4389 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 48.80 [ 6056 / 12411, 802 ins, 981 del, 4273 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 48.09 [ 5969 / 12411, 943 ins, 763 del, 4263 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 48.09 [ 5969 / 12411, 1148 ins, 584 del, 4237 sub ] exp/tri3b/decode_eval/wer_17_1.0

- TODO Multilang: Figure out why chain model training fails (speed perturbed data).

** Goals for Wednesday:
- TODO Mandatory Training (NDA)
- TODO Read more of Thang disertation.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).
- TODO Multilang: Incorporate reference LMs.

* DAR <2017-11-06 Mon>
**  Goals for Next Week:
-TODO Multilang: Expand tabs to white space in all dictionaries.
I worked a lot on this today.
I am working in alphabetical order.
I finished up through gp_spanish.

I should go back and fix the <UNK> entry, it still has a tab.
- TODO Multilang: make sure all files are in UTF8 (or ascii).
Same as above, I finished up through gp_spanish.
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.

Here is an update on WER scores:
Arabic:
%WER 77.57 [ 7015 / 9043, 349 ins, 1201 del, 5465 sub ] exp/mono/decode_dev/wer_16_0.0
%WER 73.09 [ 12048 / 16484, 598 ins, 1511 del, 9939 sub ] exp/mono/decode_eval/wer_13_0.5
%WER 72.07 [ 6517 / 9043, 731 ins, 608 del, 5178 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 71.68 [ 6482 / 9043, 572 ins, 802 del, 5108 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 71.03 [ 6423 / 9043, 746 ins, 574 del, 5103 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 70.64 [ 6388 / 9043, 590 ins, 780 del, 5018 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 66.68 [ 10991 / 16484, 1281 ins, 720 del, 8990 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 66.11 [ 10898 / 16484, 1073 ins, 955 del, 8870 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 65.63 [ 10819 / 16484, 1059 ins, 996 del, 8764 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 65.47 [ 10792 / 16484, 1370 ins, 547 del, 8875 sub ] exp/tri3b/decode_eval/wer_17_1.0

Bulgarian:
%WER 52.68 [ 7312 / 13881, 541 ins, 1575 del, 5196 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 49.37 [ 7464 / 15118, 680 ins, 1425 del, 5359 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 37.98 [ 5272 / 13881, 913 ins, 761 del, 3598 sub ] exp/tri3b/decode_eval.si/wer_16_1.0
%WER 37.82 [ 5250 / 13881, 917 ins, 707 del, 3626 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 36.36 [ 5047 / 13881, 898 ins, 695 del, 3454 sub ] exp/tri2b/decode_eval/wer_17_0.5
%WER 34.37 [ 4771 / 13881, 969 ins, 608 del, 3194 sub ] exp/tri3b/decode_eval/wer_16_1.0
%WER 33.71 [ 5097 / 15118, 887 ins, 693 del, 3517 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 33.31 [ 5036 / 15118, 1005 ins, 625 del, 3406 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 32.38 [ 4895 / 15118, 903 ins, 682 del, 3310 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 30.98 [ 4683 / 15118, 1034 ins, 536 del, 3113 sub ] exp/tri3b/decode_dev/wer_17_1.0

Croatian:
%WER 66.30 [ 5657 / 8533, 380 ins, 1006 del, 4271 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 57.38 [ 4896 / 8533, 484 ins, 999 del, 3413 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 56.89 [ 4854 / 8533, 366 ins, 1294 del, 3194 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 56.55 [ 4825 / 8533, 382 ins, 1306 del, 3137 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 53.65 [ 4578 / 8533, 558 ins, 725 del, 3295 sub ] exp/tri3b/decode_eval/wer_17_1.0

Czech:
%WER 72.32 [ 8565 / 11844, 404 ins, 1743 del, 6418 sub ] exp/mono/decode_eval/wer_11_1.0
%WER 69.95 [ 6312 / 9024, 276 ins, 1516 del, 4520 sub ] exp/mono/decode_dev/wer_14_0.5
%WER 69.30 [ 8208 / 11844, 1641 ins, 520 del, 6047 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 68.92 [ 6219 / 9024, 1344 ins, 541 del, 4334 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 68.10 [ 8066 / 11844, 761 ins, 2364 del, 4941 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 67.11 [ 7949 / 11844, 1658 ins, 767 del, 5524 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 66.77 [ 7908 / 11844, 956 ins, 1951 del, 5001 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 66.71 [ 6020 / 9024, 596 ins, 1774 del, 3650 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 65.96 [ 5952 / 9024, 793 ins, 1593 del, 3566 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 65.60 [ 5920 / 9024, 1381 ins, 642 del, 3897 sub ] exp/tri3b/decode_dev/wer_17_1.0

German:
%WER 77.09 [ 9219 / 11959, 519 ins, 1990 del, 6710 sub ] exp/mono/decode_eval/wer_13_1.0
%WER 68.19 [ 10492 / 15387, 799 ins, 2876 del, 6817 sub ] exp/mono/decode_dev/wer_14_0.5
%WER 63.17 [ 7554 / 11959, 1142 ins, 1587 del, 4825 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 61.66 [ 7374 / 11959, 525 ins, 2845 del, 4004 sub ] exp/tri1/decode_eval/wer_16_1.0
%WER 61.16 [ 7314 / 11959, 544 ins, 2704 del, 4066 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 60.28 [ 7209 / 11959, 1309 ins, 1127 del, 4773 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 57.64 [ 8869 / 15387, 693 ins, 4022 del, 4154 sub ] exp/tri1/decode_dev/wer_15_0.0
%WER 56.59 [ 8707 / 15387, 824 ins, 3682 del, 4201 sub ] exp/tri2b/decode_dev/wer_15_0.0
%WER 56.07 [ 8628 / 15387, 1150 ins, 2603 del, 4875 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 54.63 [ 8406 / 15387, 1852 ins, 1230 del, 5324 sub ] exp/tri3b/decode_dev/wer_17_1.0

Hausa:
%WER 49.81 [ 769 / 1544, 37 ins, 164 del, 568 sub ] exp/mono/decode_eval/wer_11_0.5
%WER 44.17 [ 682 / 1544, 52 ins, 242 del, 388 sub ] exp/tri1/decode_eval/wer_13_0.0
%WER 41.45 [ 640 / 1544, 55 ins, 225 del, 360 sub ] exp/tri2b/decode_eval/wer_15_0.0
%WER 35.56 [ 549 / 1544, 85 ins, 77 del, 387 sub ] exp/tri3b/decode_eval.si/wer_17_0.5
%WER 33.03 [ 510 / 1544, 53 ins, 130 del, 327 sub ] exp/tri3b/decode_eval/wer_17_1.0

Japanese:
%WER 41.73 [ 7476 / 17915, 961 ins, 1342 del, 5173 sub ] exp/mono/decode_eval/wer_10_0.0
%WER 27.67 [ 4957 / 17915, 880 ins, 688 del, 3389 sub ] exp/tri3b/decode_eval.si/wer_17_0.5
%WER 26.38 [ 4726 / 17915, 857 ins, 683 del, 3186 sub ] exp/tri1/decode_eval/wer_16_0.5
%WER 25.17 [ 4509 / 17915, 864 ins, 612 del, 3033 sub ] exp/tri2b/decode_eval/wer_15_0.5
%WER 23.01 [ 4123 / 17915, 829 ins, 588 del, 2706 sub ] exp/tri3b/decode_eval/wer_14_1.0

- TODO Multilang: Run chain model training for all languages (this will help down the line).
I am working on French.
It looks like the problem is with the speed perturbed data.
I think it requires matrices with more rows or columns that for some reason do not exist.
Either I do not use sp data or I figure out how to get the larger matrices.
I can get things to run without the sp data, but I do not think this is what I want.

** Goals for Tuesday:
- TODO Mandatory Training (NDA)
- TODO Read chapter 4 of Thang disertation.
-TODO Multilang: Expand tabs to white space in all dictionaries (start from tamil). 
- TODO Multilang: convert tab to space in <UNK> entry.
- TODO Multilang: make sure all files are in UTF8 (start from tamil).
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).

* DAR <2017-11-03 Fri>
**  Goals for Thursday set Wednesday:
- DONE Workshop. (should take up the whole day)
I thinkthe the workshop was very successful.

* DAR <2017-11-01 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Setup the gp_french and incorporate it into multilang.
I am starting the multilang over again.
I got to the point where the multilang recipe was going to train the ivector extractor.
All the data was pooled.
At this point the utt2spk file failed.
Most of the GP corpora use speaker names like 001.
I have to make them distinct across languages.
So, for example, I'll label them as AR001 and SP001 for Arabic and Spanish respectively.
I also plan on starting small, maybe with 3 languages.
I am starting with Arabic, Bulgarian , Croatian and French. Maybe Turkish too.
Later I'll start over again using the GP LMs, for now I am building the LMS on the training text.

- TODO Study the multilang recipe.
I am reading the Disertation by Ngoc Thang Vu.
It looks like my project this year will consist of replicating some of the work in this disertation and then improving on it with chain models.
The disertation uses DNNS.
My experience with DNNs was a little disappointing.
I think Chain models should do better than the  DNNs. 
- TODO Writing.
The Ngoc Thang Vu disertation is good background for anything we will write about.

** Goals for Thursday:
- TODO Workshop. (should take up the whole day)

* DAR <2017-10-31 Tue>
** Goals for Tuesday set Monday:
- TODO Multilang: Investigate recipe.
My current understanding is that the multilang recipe is going to make a nnet3 model on all the data I feed it, which right now comes from 8 languages.
Then the target language data is used to train/retrain the last layer of the neural net.

- TODO Writing.
Steve and I discussed a mind map for the paper.
I think we can write about the chain model versus tri3b results:
Look at the first 3 columns of the table below.
tri3b on gp: 35.85
Chain model on GP: 51.27
Add 3 hours of Gabon Read: 
tri3b with 3h of babon read: 19.84
chain with 3h of gabon read: 14.78

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 44.59 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 45.28 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 35.85 | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | 52.40 | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | 51.27 | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |


** Goals for Wednesday:
- TODO Setup the gp_french and incorporate it into multilang.
- TODO Study the multilang recipe.
- TODO Writing.

* DAR <2017-10-30 Mon>
** Goals for Monday:
- DONE Check Mandarin dictionary normalization.
The list of phones looks good.
** Goals for Next Week:
- DONE Multilang: Finish dictionary work for all languages.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok
hausa: ok
Japanese: ok
Korean: ok
Mandarin: ok
Polish: ok
Portuguese: ok
Russian: ok
Spanish: ok
Swedish: ok
Tamil: There are backslashes
Thai: ok
Turkish: ok
Vietnamese: ok

- TODO Multilang: Train cd gmm hmm systems for each language.
Arabic: Started
Bulgarian: Started
Croatian: Started
Czech: DONE
German: Done
Hausa: Done
Japanese: Started
Korean: Started
Mandarin: Started
Polish: Started 
Portuguese: Problems with the folds. There are files that don't really have usable data  in them . They are very small.
Russian: Started
Spanish: Done
Swedish: Done
Tamil: Done
Thai: Started
Turkish: Done
Vietnamese: Started

- Polish mfcc:
%WER 71.43 [ 10840 / 15176, 838 ins, 2521 del, 7481 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 64.05 [ 9720 / 15176, 905 ins, 3022 del, 5793 sub ] exp/tri1/decode_eval/wer_13_0.0

- TODO Workshop: (Thursday).
- TODO Writing.



- Multilang recipe:
I started running the multilang recipe.
I am only using the languages that I have built cd gmm hmm systems for so far.
Arabic
Czech
German
Hausa
Spanish
Swedish
Turkish


I had to link the data/train and exp/tri3b_ali directories to the local working directory.
Now I am extracting high resolution mfcc features (and pitch?).

** Goals for Tuesday:
- TODO Multilang: Investigate recipe.
- TODO Writing.

* DAR <2017-10-27 Fri>
** Goals for Friday set Thursday:
- TODO Multilang: Continue checking dictionaries.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok
hausa: ok
Japanese ok
Korean: ok

- TODO Multilang: Get monophone results for each language.

| language | hours | monoWER |
| Arabic | 15.3 | 99.91 |
| Bulgarian | 17.1 | 100.00 |
| Croatian | 7.7 | 77.30 |
| Czech | 16.0 | 88.96 |
| German | 14.8 | 81.46 |
| Hausa| 4.8 | 48.38 |
| Japanese | 28.8 | 100.00 |
| Korean | 18.9 | 100.00 |
| Mandarin | 26.6 | 103.04 |
| Polish | 18.2 | 71.43 |
| Portuguese | 16.0 | 100.0 |
| Russian | 20.9 | 99.89 |
| Spanish | 17.5 | 60.20 |
| Swedish | 17.4 | 81.69 |
| Turkish | 13.2 | 82.91 |
| Vietnamese | 13.6 | 97.80 |

- Swedish MFCC: 
%WER 81.69 [ 14830 / 18154, 826 ins, 3532 del, 10472 sub ] exp/mono/decode_eval/wer_9_1.0
%WER 71.25 [ 12935 / 18154, 1753 ins, 2091 del, 9091 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 69.66 [ 12646 / 18154, 1892 ins, 1931 del, 8823 sub ] exp/tri3b/decode_eval/wer_17_1.0

-Turkish:
%WER 82.91 [ 10400 / 12543, 215 ins, 2685 del, 7500 sub ] exp/mono/decode_eval/wer_10_1.0


- TODO Writing

** Goals for Monday:
- TODO Check Mandarin dictionary normalization.
* DAR <2017-10-26 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Setup all languages to train on plp pitch (start tomorrow with Japanese).
- TODO Multilang: Go through each language and check the state of the dictionary and try to correct problems.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok

- TODO Writing.
- TODO Multilang: What is the next step?


- Bulgarian:
17.1 hours of speech.
I fixed some issues with the dictionary normalization script.
This should work better now.
- Croatian:
7.7 hours of data
%WER 77.30 [ 6596 / 8533, 264 ins, 1480 del, 4852 sub ] exp/mono/decode_eval/wer_13_0.5
This maybe as good as this will get for such a small corpus.
-Czech:
16.0 hours of data
- German:
14.8 hours of data
%WER 81.46 [ 3664 / 4498, 249 ins, 669 del, 2746 sub ] [PARTIAL] exp/mono/decode_eval/wer_13_0.5
I think this should be doing better than this.
- Hausa:
4.8 hours of data

** Goals for Friday:
- TODO Multilang: Continue checking dictionaries.
- TODO Multilang: Get monophone results for each language.
- TODO Writing

* DAR <2017-10-25 Wed>
** Goals for Wednesday set Tuesday:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Thai
Tamil 
tamil does not have a globalphone dictionary.
Babel has a tamil dictionary.
Turkish
Vietnamese
wuu
Wuu has no dictionary

- Swedish: 
17.4 hours of data

I started looking at the multilang recipe.
It suggests using plp + pitch features for each language.
I started doing this for each file.
I copied the plp pitch configuration files to each directory.
I also fixed some problems with dictionaries. 
I am currently working on Japanese.

- TODO African French: Build with some more combinations of data.
- TODO African French: Get hours of speech by running monophone training, alignment and testing for data sets that are missing hours.
- TODO Start writing paper.

** Goals for Thursday:
- TODO Multilang: Setup all languages to train on plp pitch (start tomorrow with Japanese).
- TODO Multilang: Go through each language and check the state of the dictionary and try to correct problems.
- TODO Writing.
- TODO Multilang: What is the next step?
 
* DAR <2017-10-24 Tue>
** Goals set Last Week:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Swedish (This needs to be worked on first. I think it needs a dictionary normalization script.)
Thai
Tamil
Turkish
Vietnamese
wu

I got a lot done on Swedish today.

- DONE African French: Get results for GP aloen and other training sequences.

%WER 53.90 [ 1720 / 3191, 125 ins, 386 del, 1209 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 52.40 [ 1672 / 3191, 159 ins, 443 del, 1070 sub ] exp/chain/tdnn_sp/decode_ca16/wer_17_0.5
%WER 51.27 [ 1636 / 3191, 165 ins, 396 del, 1075 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_17_0.5
%WER 48.82 [ 1558 / 3191, 189 ins, 401 del, 968 sub ] exp/tri3b/decode_ca16.si/wer_17_0.5
%WER 45.28 [ 1445 / 3191, 176 ins, 390 del, 879 sub ] exp/tri2b/decode_ca16/wer_17_0.0
%WER 44.59 [ 1423 / 3191, 220 ins, 262 del, 941 sub ] exp/tri1/decode_ca16/wer_17_0.0
%WER 35.85 [ 1144 / 3191, 162 ins, 254 del, 728 sub ] exp/tri3b/decode_ca16/wer_17_1.0

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 44.59 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 45.28 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 35.85 | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | 52.40 | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | 51.27 | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

** Goals for Wednesday:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Thai
Tamil
Turkish
Vietnamese
wu
- TODO African French: Build with some more combinations of data.
- TODO African French: Get hours of speech by running monophone training, alignment and testing for data sets that are missing hours.
- TODO Start wrigin paper.
* DAR <2017-10-19 Thu>
** Goals for Thursday set Wednesday:
- DONE Multilang: Arabic training and evaluation.
15.35 hours of training data.

I got a lot done on this goal today.
I setup the basic recipe for 13 of the 18 languages:
Arabic
Bulgarian
Croatian
Czech
German
Hausa
Japanese
Korean
Mandarin
Polish
Portuguese
Russian
Spanish
- DONE Multilang: Bulgarian?
- TODO African French: What do we get when we only run on gp?

I am taking Friday and Monday off.

** Goals for Next Week:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Swedish (This needs to be worked on first. I think it needs a dictionary normalization script.)
Thai
Tamil
Turkish
Vietnamese
u

- TODO African French: Get results for GP aloen and other training sequences.

* DAR <2017-10-18 Wed>
** Goals for Wednesday set Tuesday:
- DONE Multilang: Make my own data prep scripts for Arabic.
I made a lot of progress on this goal today.
I have scripts that prepare the data and write the lists for acoustic model training and testing.
I train an LM on the training text.
I am using the dictionary supplied by the Globalphone corpus.
Monophone training is running, but I don not know yet if decoding will work.
- DONE African French: Get current results.
%WER 45.28 [ 1445 / 3191, 107 ins, 337 del, 1001 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 27.08 [ 864 / 3191, 144 ins, 162 del, 558 sub ] exp/tri3b/decode_ca16.si/wer_16_1.0
%WER 25.51 [ 814 / 3191, 157 ins, 129 del, 528 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 21.87 [ 698 / 3191, 142 ins, 93 del, 463 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 19.84 [ 633 / 3191, 133 ins, 91 del, 409 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 14.95 [ 477 / 3191, 75 ins, 89 del, 313 sub ] exp/chain/tdnn_sp/decode_ca16/wer_12_0.0
%WER 14.79 [ 472 / 3191, 72 ins, 94 del, 306 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_13_0.0

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 47.23 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

- TODO African French: Get results when only Training on gp (I probably have these results already, but get them anyway just for completeness).

** Goals for Thursday:
- TODO Multilang: Arabic training and evaluation.
- TODO Multilang: Bulgarian?
- TODO African French: What do we get when we only run on gp?

* DAR <2017-10-17 Tue>
** Goals for Tuesday set Monday:
- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
- TODO African French: Get chain model results and move on to next step by removing more data.
%WER 43.94 [ 1402 / 3191, 117 ins, 288 del, 997 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 23.72 [ 757 / 3191, 154 ins, 100 del, 503 sub ] exp/tri1/decode_ca16/wer_15_0.0
%WER 22.56 [ 720 / 3191, 104 ins, 165 del, 451 sub ] exp/tri2b/decode_ca16/wer_17_1.0
%WER 19.08 [ 609 / 3191, 121 ins, 80 del, 408 sub ] exp/tri3b/decode_ca16/wer_17_0.5
%WER 14.10 [ 450 / 3191, 78 ins, 66 del, 306 sub ] exp/chain/tdnn_sp/decode_ca16/wer_11_0.0
%WER 13.88 [ 443 / 3191, 69 ins, 73 del, 301 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_11_0.5

| model | WER gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

%WER 45.28 [ 1445 / 3191, 107 ins, 337 del, 1001 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 27.08 [ 864 / 3191, 144 ins, 162 del, 558 sub ] exp/tri3b/decode_ca16.si/wer_16_1.0
%WER 25.51 [ 814 / 3191, 157 ins, 129 del, 528 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 21.87 [ 698 / 3191, 142 ins, 93 del, 463 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 19.84 [ 633 / 3191, 133 ins, 91 del, 409 sub ] exp/tri3b/decode_ca16/wer_16_0.5

The chain model results are not ready yet.
- TODO Heroico: Maybe start from beginning since scripts are not moving forward and they die on pca transform estimation.
%WER 23.33 [ 2150 / 9215, 162 ins, 373 del, 1615 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 20.46 [ 3420 / 16713, 288 ins, 533 del, 2599 sub ] exp/mono/decode_test/wer_8_0.0
%WER 18.42 [ 1697 / 9215, 233 ins, 193 del, 1271 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 16.67 [ 1250 / 7498, 123 ins, 181 del, 946 sub ] exp/mono/decode_native/wer_7_0.0
%WER 15.72 [ 1449 / 9215, 147 ins, 224 del, 1078 sub ] exp/tri1/decode_nonnative/wer_17_0.5
%WER 14.60 [ 1345 / 9215, 222 ins, 135 del, 988 sub ] exp/tri2b/decode_nonnative/wer_17_0.0
%WER 14.36 [ 2400 / 16713, 318 ins, 306 del, 1776 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 13.01 [ 2175 / 16713, 230 ins, 320 del, 1625 sub ] exp/tri1/decode_test/wer_16_0.5
%WER 11.70 [ 1078 / 9215, 128 ins, 137 del, 813 sub ] exp/tri3b/decode_nonnative/wer_17_1.0
%WER 11.63 [ 1944 / 16713, 292 ins, 211 del, 1441 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 9.59 [ 719 / 7498, 78 ins, 100 del, 541 sub ] exp/tri1/decode_native/wer_15_0.5
%WER 9.20 [ 690 / 7498, 123 ins, 76 del, 491 sub ] exp/tri3b/decode_native.si/wer_14_0.0
%WER 9.02 [ 1507 / 16713, 192 ins, 166 del, 1149 sub ] exp/tri3b/decode_test/wer_16_0.5
%WER 7.79 [ 584 / 7498, 80 ins, 63 del, 441 sub ] exp/tri2b/decode_native/wer_14_0.0
%WER 5.49 [ 412 / 7498, 34 ins, 52 del, 326 sub ] exp/tri3b/decode_native/wer_14_0.5

The run failed again on the ubm training step.
I enables this line in my path.sh file:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64

Maybe this is what was missing all this time?
No. It failed again :(

** Goals for Wednesday:
- TODO Multilang: Make my own data prep scripts for Arabic.
- TODO African French: Get current results.
- TODO African French: Get results when only Training on gp (I probably have these results already, but get them anyway just for completeness).

* DAR <2017-10-16 Mon>
** Goals set Last Week:
- DONE Objectives (Monday) 
(1) TECHNICAL Objectives (Weight 30)
A. Acoustic Models for Low Resource Languages
Adapt the Kaldi multilang recipe to build acoustic models for a target low resource language given resources from many other source languages. 

Specific Rating tasks:
Modify the Kaldi multilang recipe from its original keyword spotting task to the Speech to Speech (S2S) task.
Build 18 ASR systems from source language resources in the GlobalPhone corpus and government owned corpora (see corpus curation task below).
Setup experiment to evaluate effectiveness of the multilang approach. 

B. Corpus Curation
Curate four government owned speech corpora.

Specific Rating Tasks:
Prepare Arabic, French, German and Russian speech data for use in the multilang project listed above.
Write Kaldi recipes for each language corpus.
Submit recipes for publication in Kaldi repository.
Publish data, lexicons and recipes in ARL NSRL repository.

C. Speech to Speech Technology
Investigate S2S hardware restrictions and software solutions with the goal of contributing optimized components. 

Specific Rating Tasks:
Study methods for performing online or real time ASR processing and produce ASR components that are optimized to work with these methods. 
Study methods for integrating ASR and MT components in S2S applications and tailor our products to conform to these methods.
Study methods for making S2S ASR highly responsive and accurate and use results of investigations to guide our choices of models and algorithms. 



(2) COOPERATION (Weight 10)

A. Cooperate with colleagues.

Specific Rating Tasks:
Collaborate with Steve LaRocca in first quarter to write papers that report on advances made in our projects. 
Collaborate with the Basic Research team and CERDEC by contributing speech recognition components to Human Robot communication efforts. 

(3) COMMUNICATIONS (Weight 30)

A. Publish papers and reports

Specific Rating Tasks:
Write a TR with Steve LaRocca in the first quarter documenting projects. 
Write journal paper with Steve LaRocca that reports on multilang project results.

B. Activity Reports
Write weekly reports to help guide research and to recored progress .

C. Establish Professional Communication Channels with Scientists contributing to Kaldi project.

Specific Rating Tasks:
Contribute algorithm to Kaldi

(4) MGMT. OF TIME & RESOURCES (Weight 15)

A. Curate and archive our own valuable speech and text corpora on our branch storage disks. 


Specific Rating Tasks:
Format the data so that the corpora that can be made publically available are ready to be transfered. 
Organize the data so that it is easy to access from recipes running on connected branch machines.
Stay abreast of possible areas where hardware upgrades could improve work efficiency. 

(5) CUSTOMER RELATIONS (Weight 15)

Establish relationships with MFLTS and CERDEC to remain aware of Army requirements.
Establish contacts with researchers in the ASR and NLP fields. 
Establish contacts with s2s application developers.

(6) TECH TRANSITION (Weight 10)

Contribute recipes for building ASR systems with our corpora to the MFLTS. 
Transition ASR components and our other products to USA Army Africa and MFLTS. 

(7) DIVERSITY: 
Support ARL's diversity initiatives by participating in locally-sponsored diversity training, broad outreach, and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees. 

(8) SHARP: 
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all. 
Demonstrate support for the SHARP program by actively participating in required training and other educational programs. 
Intervene and appropriately respond to any instances of sexual harassment or sexual assault and encourage others to do the same.

- TODO Heroico: Tune Chain Models?
I found more references to the mini_librispeech recipe in the scripts I am using to do the i-vector extraction and chain model training.
I removed the references to the data splitting in the scripts when they are run on the clsp cluster.

- TODO African French: Get WER scores for models trained on progressivley smaller training sets. (try removing yaounde)
%WER 41.99 [ 1340 / 3191, 114 ins, 314 del, 912 sub ] exp/mono/decode_ca16/wer_11_0.0
%WER 24.01 [ 766 / 3191, 154 ins, 105 del, 507 sub ] exp/tri3b/decode_ca16.si/wer_13_0.0
%WER 23.22 [ 741 / 3191, 85 ins, 185 del, 471 sub ] exp/tri1/decode_ca16/wer_14_1.0
%WER 20.78 [ 663 / 3191, 112 ins, 126 del, 425 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 17.05 [ 544 / 3191, 100 ins, 90 del, 354 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 12.28 [ 392 / 3191, 65 ins, 66 del, 261 sub ] exp/chain/tdnn_sp/decode_ca16/wer_13_0.0
%WER 12.28 [ 392 / 3191, 54 ins, 74 del, 264 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_12_0.5

| model | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 12.28 | 12.85 | 11.69 | 12.60 |


%WER 43.94 [ 1402 / 3191, 117 ins, 288 del, 997 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 23.72 [ 757 / 3191, 154 ins, 100 del, 503 sub ] exp/tri1/decode_ca16/wer_15_0.0
%WER 22.56 [ 720 / 3191, 104 ins, 165 del, 451 sub ] exp/tri2b/decode_ca16/wer_17_1.0
%WER 19.08 [ 609 / 3191, 121 ins, 80 del, 408 sub ] exp/tri3b/decode_ca16/wer_17_0.5

- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
I worked a little on this today.
None of the languages work out of the box.
I think I'm going to write my own scripts.
I want to use utf8 and I don't want to mess with converting waveform data.
I will put the waveform data that is ready for processing under /mnt/disk01/globalphone.

** Goals for Tuesday:
- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
- TODO African French: Get chain model results and move on to next step by removing more data.
- TODO Heroico: Maybe start from beginning since scripts are not moving forward and they die on pca transform estimation.

* DAR <2017-10-12 Thu>
** Goals for Thursday:
- TODO Objectives.
- TODO African French: Get tri3b results.
%WER 22.56 [ 720 / 3191, 124 ins, 116 del, 480 sub ] exp/tri3b/decode_ca16.si/wer_14_0.0
%WER 16.61 [ 530 / 3191, 97 ins, 86 del, 347 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 42.09 [ 1343 / 3191, 132 ins, 270 del, 941 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 141 ins, 120 del, 474 sub ] exp/tri1/decode_ca16/wer_13_0.0
%WER 20.90 [ 667 / 3191, 100 ins, 133 del, 434 sub ] exp/tri2b/decode_ca16/wer_15_0.5
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0

| model | WER |
mono | 42.09 |
| tri1 | 23.03 |
tri2b | 20.90 \
| tri3b | 16.61 |
| chain | 11.69 \ |
|chaine online | 11.69 |

- TODO Heroico: Tune chain models.
Here are the WER scores I get on the clsp cluster:
%WER 44.07 [ 4061 / 9215, 121 ins, 1871 del, 2069 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_12_0.0
%WER 41.95 [ 3866 / 9215, 149 ins, 1600 del, 2117 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative/wer_11_0.0
%WER 36.95 [ 6176 / 16713, 269 ins, 2525 del, 3382 sub ] exp/chain/tdnn1c_sp/decode_test/wer_9_0.0
%WER 35.25 [ 5891 / 16713, 251 ins, 2406 del, 3234 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_10_0.0
%WER 28.03 [ 2102 / 7498, 86 ins, 951 del, 1065 sub ] exp/chain/tdnn1c_sp/decode_native/wer_9_0.0
%WER 26.81 [ 2010 / 7498, 83 ins, 873 del, 1054 sub ] exp/chain/tdnn1c_sp_online/decode_native/wer_9_0.0
%WER 23.28 [ 2145 / 9215, 169 ins, 364 del, 1612 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 20.36 [ 3402 / 16713, 266 ins, 590 del, 2546 sub ] exp/mono/decode_test/wer_9_0.0
%WER 19.63 [ 1809 / 9215, 241 ins, 219 del, 1349 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 16.70 [ 1252 / 7498, 97 ins, 221 del, 934 sub ] exp/mono/decode_native/wer_9_0.0
%WER 15.68 [ 1445 / 9215, 162 ins, 202 del, 1081 sub ] exp/tri1/decode_nonnative/wer_17_0.5
%WER 15.23 [ 2545 / 16713, 333 ins, 319 del, 1893 sub ] exp/tri3b/decode_test.si/wer_16_1.0
%WER 15.07 [ 1389 / 9215, 182 ins, 186 del, 1021 sub ] exp/tri2b/decode_nonnative/wer_17_0.5
%WER 12.91 [ 2158 / 16713, 225 ins, 299 del, 1634 sub ] exp/tri1/decode_test/wer_16_0.5
%WER 12.49 [ 1151 / 9215, 133 ins, 153 del, 865 sub ] exp/tri3b/decode_nonnative/wer_16_1.0
%WER 11.92 [ 1992 / 16713, 237 ins, 278 del, 1477 sub ] exp/tri2b/decode_test/wer_17_0.5
%WER 9.47 [ 1583 / 16713, 169 ins, 225 del, 1189 sub ] exp/tri3b/decode_test/wer_16_1.0
%WER 9.43 [ 707 / 7498, 93 ins, 89 del, 525 sub ] exp/tri3b/decode_native.si/wer_17_0.5
%WER 9.32 [ 699 / 7498, 65 ins, 93 del, 541 sub ] exp/tri1/decode_native/wer_14_0.5
%WER 7.94 [ 595 / 7498, 63 ins, 82 del, 450 sub ] exp/tri2b/decode_native/wer_14_0.5
%WER 5.61 [ 421 / 7498, 40 ins, 59 del, 322 sub ] exp/tri3b/decode_native/wer_16_0.5

| model | native | both | nonnative |
| mono | 16.70 | 20.36 | 23.28 |
| tri1 | 9.32 | 12.91 | 15.68 |
| tri2b | 7.94 | 11.92 | 15.07 |
| tri3b | 5.61 | 9.47 | 12.49 |
| chain | 28.03 | 36.95 | 44.07 |
| chain online | 26.81 | 35.25 | 41.95 |

- TODO African French: Run with another training chunk removed.
I am now running with Niger removed. 
- TODO Yaounde: More work to figure out why results are so bad.
I am going to test on the CA16 corpus.

- Hispanic Heritage Month Activity: I attended the presentation by Raquel Tamez.

** Goals for Friday:
- TODO Objectives
- TODO Yaounde: What WER scores do we get for ca16?
%WER 96.96 [ 3094 / 3191, 47 ins, 1382 del, 1665 sub ] exp/mono/decode_ca16/wer_17_0.0
%WER 90.99 [ 2050 / 2253, 39 ins, 971 del, 1040 sub ] exp/mono/decode_test/wer_14_1.0

So the problem is definitely not with the ARTI242 test set. 

- TODO African French: WER scores when srica is removed.
%WER 41.43 [ 1322 / 3191, 117 ins, 272 del, 933 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 133 ins, 124 del, 478 sub ] exp/tri3b/decode_ca16.si/wer_14_0.0
%WER 22.78 [ 727 / 3191, 109 ins, 144 del, 474 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 20.34 [ 649 / 3191, 114 ins, 128 del, 407 sub ] exp/tri2b/decode_ca16/wer_17_0.0
%WER 16.64 [ 531 / 3191, 106 ins, 75 del, 350 sub ] exp/tri3b/decode_ca16/wer_17_0.0
%WER 12.85 [ 410 / 3191, 65 ins, 73 del, 272 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_12_0.5
%WER 12.75 [ 407 / 3191, 77 ins, 56 del, 274 sub ] exp/chain/tdnn_sp/decode_ca16/wer_12_0.0

| model | WER |
mono | 41.43 |
| tri1 | 22.78 |
tri2b | 20.34 \
| tri3b | 16.64 |
| chain | 12.75 \ |
|chaine online | 12.85 |

* DAR <2017-10-11 Wed>
** Goals for Wednesday set Tuesday:
- TODO Objectives
I got the form from Shanel.
- TODO Yaounde: What happens with subs trained lm?
- TODO African French: Complete set of results.
Here is what I have now:
%WER 42.09 [ 1343 / 3191, 132 ins, 270 del, 941 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 141 ins, 120 del, 474 sub ] exp/tri1/decode_ca16/wer_13_0.0
%WER 20.90 [ 667 / 3191, 100 ins, 133 del, 434 sub ] exp/tri2b/decode_ca16/wer_15_0.5
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0


| model | WER |
mono | 42.09 |
| tri1 | 23.03 |
tri2b | 20.90 \
| tri3b | |
| chain | 11.69 \ |
|chaine online | 11.69 |

- TODO Heroico: Results including chain model results and contact Yenda.
I contacted Yenda.
He was not much help.
I fixed a reference to the clsp cluster in the ivector prep script.
It was hard coded to use the mini_librispeech corpus.

%WER 9.47 [ 710 / 7498, 96 ins, 94 del, 520 sub ] exp/tri3b/decode_native.si/wer_17_0.5
%WER 9.44 [ 708 / 7498, 79 ins, 103 del, 526 sub ] exp/tri1/decode_native/wer_14_0.5
%WER 9.24 [ 1544 / 16713, 164 ins, 214 del, 1166 sub ] exp/tri3b/decode_test/wer_17_1.0
%WER 8.27 [ 620 / 7498, 76 ins, 90 del, 454 sub ] exp/tri2b/decode_native/wer_15_0.5
%WER 5.57 [ 418 / 7498, 43 ins, 53 del, 322 sub ] exp/tri3b/decode_native/wer_16_0.5
%WER 27.34 [ 2519 / 9215, 191 ins, 558 del, 1770 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_11_0.0
%WER 26.16 [ 2411 / 9215, 184 ins, 537 del, 1690 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative/wer_11_0.0
%WER 23.13 [ 2131 / 9215, 173 ins, 376 del, 1582 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 22.44 [ 3750 / 16713, 278 ins, 848 del, 2624 sub ] exp/chain/tdnn1c_sp/decode_test/wer_11_0.0
%WER 21.58 [ 3607 / 16713, 273 ins, 819 del, 2515 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_11_0.0
%WER 20.40 [ 3410 / 16713, 273 ins, 610 del, 2527 sub ] exp/mono/decode_test/wer_9_0.0
%WER 19.32 [ 1780 / 9215, 246 ins, 206 del, 1328 sub ] exp/tri3b/decode_nonnative.si/wer_16_1.0
%WER 17.07 [ 1280 / 7498, 98 ins, 231 del, 951 sub ] exp/mono/decode_native/wer_9_0.0
%WER 16.16 [ 1212 / 7498, 77 ins, 306 del, 829 sub ] exp/chain/tdnn1c_sp/decode_native/wer_12_0.0
%WER 15.91 [ 1193 / 7498, 73 ins, 303 del, 817 sub ] exp/chain/tdnn1c_sp_online/decode_native/wer_12_0.0
%WER 15.74 [ 1450 / 9215, 159 ins, 211 del, 1080 sub ] exp/tri1/decode_nonnative/wer_16_0.5
%WER 15.37 [ 1416 / 9215, 218 ins, 155 del, 1043 sub ] exp/tri2b/decode_nonnative/wer_17_0.0
%WER 14.98 [ 2504 / 16713, 382 ins, 278 del, 1844 sub ] exp/tri3b/decode_test.si/wer_17_0.5
%WER 12.91 [ 2158 / 16713, 240 ins, 303 del, 1615 sub ] exp/tri1/decode_test/wer_15_0.5
%WER 12.14 [ 1119 / 9215, 127 ins, 153 del, 839 sub ] exp/tri3b/decode_nonnative/wer_17_1.0
%WER 12.12 [ 2026 / 16713, 303 ins, 232 del, 1491 sub ] exp/tri2b/decode_test/wer_17_0.0

| model | native | both | nonnative |
| mono | 17.07 | 20.40 | 23.13 |
| tri1 | 9.44 | 12.91 | 15.74 |
| tri2b | 8.27 | 12.12 | 15.37 |
| tri3b | 5.57 | 9.24 | 12.14 |
| chain | 16.16 | 22.44 | 27.34 |
| chain online | 15.91 | 21.58 | 26.16 |

Why are the chain models not better than the cd gmm hmm ?

** Goals for Thursday:
- TODO Objectives
- TODO African French: Get tri3b results.
- TODO Heroico: Tune chain models. 
- TODO African French: Run with another training chunk removed 
- TODO Yaounde: More work to figure out why results are so bad.

* DAR <2017-10-10 Tue>
** Goals for Next Week:
- TODO Objectives
- TODO Heroico: Chain model results?
- DONE Heroico: Decide about lm (include simple lm?)
I am going with only subs.
- TODO Yaounde: Chain model results?

- TODO African French: Build system on progressivly smaller training sets.

I removed the ARTI data set of 242 utterances.
So far I only have chain model results.
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0

This is better than the previous result which was 12.63.
Is there something wrong with the ARTI242 data? (Transcripts, recording parameters, ...)

- TODO Multilang: Minimal example
** Goals for Wednesday:
- TODO Objectives
- TODO Yaounde: What happens with subs trained lm?
- TODO African French: Complete set of results.
- TODO Heroico: Results including chain model results and contact Yenda.

* DAR <2017-10-05 Thu>
** Goals for Thursday set Wednesday:
- TODO Objectives:
- DONE SOFTunisia: Finish training with stage 16 speakers and get rough draft to ZAC.
- TODO African French: Build system without ARTI corpus.
- DONE Heroico: Incorporate subs trained lm into system.
I am going to remove the gp lm from the recipe.
I want to use UTF8 as the text encoding.
I am pretty sure the gp lm is not in utf8.
Here are the WER scores for today.
I don't have the chain model results for subs yet.
%WER 67.34 [ 6205 / 9215, 398 ins, 1455 del, 4352 sub ] exp/chain/tdnn1c_sp/decode_nonnative_gplm/wer_8_0.5
%WER 66.61 [ 6138 / 9215, 388 ins, 1417 del, 4333 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_gplm/wer_8_0.5
%WER 65.28 [ 6016 / 9215, 453 ins, 1214 del, 4349 sub ] exp/mono/decode_nonnative_gplm/wer_8_0.0
%WER 62.40 [ 10429 / 16713, 733 ins, 2089 del, 7607 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 6.22 [ 573 / 9215, 28 ins, 182 del, 363 sub ] exp/chain/tdnn1c_sp/decode_nonnative_simple/wer_15_0.0
%WER 60.64 [ 10135 / 16713, 686 ins, 2199 del, 7250 sub ] exp/chain/tdnn1c_sp/decode_test_gplm/wer_8_0.5
%WER 59.98 [ 10024 / 16713, 665 ins, 2155 del, 7204 sub ] exp/chain/tdnn1c_sp_online/decode_test_gplm/wer_8_0.5
%WER 58.98 [ 4422 / 7498, 339 ins, 820 del, 3263 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 57.58 [ 5306 / 9215, 571 ins, 787 del, 3948 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_14_1.0
%WER 57.04 [ 5256 / 9215, 526 ins, 861 del, 3869 sub ] exp/tri1/decode_nonnative_gplm/wer_12_1.0
%WER 55.14 [ 5081 / 9215, 518 ins, 838 del, 3725 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.06 [ 8868 / 16713, 950 ins, 1356 del, 6562 sub ] exp/tri1/decode_test_gplm/wer_12_1.0
%WER 52.74 [ 8815 / 16713, 1047 ins, 1206 del, 6562 sub ] exp/tri3b/decode_test_gplm.si/wer_14_1.0
%WER 52.41 [ 3930 / 7498, 288 ins, 734 del, 2908 sub ] exp/chain/tdnn1c_sp/decode_native_gplm/wer_7_1.0
%WER 51.73 [ 3879 / 7498, 227 ins, 829 del, 2823 sub ] exp/chain/tdnn1c_sp_online/decode_native_gplm/wer_8_1.0
%WER 50.87 [ 4688 / 9215, 588 ins, 673 del, 3427 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_1.0
%WER 50.66 [ 8466 / 16713, 1049 ins, 1184 del, 6233 sub ] exp/tri2b/decode_test_gplm/wer_13_1.0
%WER 48.07 [ 3604 / 7498, 422 ins, 497 del, 2685 sub ] exp/tri1/decode_native_gplm/wer_12_1.0
%WER 47.30 [ 7906 / 16713, 1125 ins, 942 del, 5839 sub ] exp/tri3b/decode_test_gplm/wer_15_1.0
%WER 4.72 [ 435 / 9215, 18 ins, 152 del, 265 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_simple/wer_17_0.0
%WER 46.87 [ 3514 / 7498, 512 ins, 379 del, 2623 sub ] exp/tri3b/decode_native_gplm.si/wer_13_1.0
%WER 45.25 [ 3393 / 7498, 467 ins, 413 del, 2513 sub ] exp/tri2b/decode_native_gplm/wer_13_1.0
%WER 42.92 [ 3218 / 7498, 566 ins, 287 del, 2365 sub ] exp/tri3b/decode_native_gplm/wer_13_1.0
%WER 4.19 [ 700 / 16713, 43 ins, 223 del, 434 sub ] exp/chain/tdnn1c_sp/decode_test_simple/wer_15_0.0
%WER 3.80 [ 350 / 9215, 93 ins, 22 del, 235 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 3.31 [ 553 / 16713, 33 ins, 187 del, 333 sub ] exp/chain/tdnn1c_sp_online/decode_test_simple/wer_17_0.0
%WER 31.57 [ 2909 / 9215, 193 ins, 610 del, 2106 sub ] exp/mono/decode_nonnative_subs/wer_9_0.0
%WER 28.51 [ 4765 / 16713, 401 ins, 880 del, 3484 sub ] exp/mono/decode_test_subs/wer_8_0.0
%WER 2.71 [ 453 / 16713, 121 ins, 37 del, 295 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 25.64 [ 2363 / 9215, 351 ins, 290 del, 1722 sub ] exp/tri3b/decode_nonnative_subs.si/wer_16_0.5
%WER 24.69 [ 1851 / 7498, 178 ins, 310 del, 1363 sub ] exp/mono/decode_native_subs/wer_8_0.0
%WER 22.91 [ 2111 / 9215, 245 ins, 311 del, 1555 sub ] exp/tri1/decode_nonnative_subs/wer_17_0.0
%WER 21.33 [ 1966 / 9215, 164 ins, 361 del, 1441 sub ] exp/tri2b/decode_nonnative_subs/wer_17_1.0
%WER 21.00 [ 3509 / 16713, 427 ins, 510 del, 2572 sub ] exp/tri3b/decode_test_subs.si/wer_17_1.0
%WER 19.26 [ 3219 / 16713, 314 ins, 522 del, 2383 sub ] exp/tri1/decode_test_subs/wer_16_0.5
%WER 18.13 [ 1671 / 9215, 208 ins, 247 del, 1216 sub ] exp/tri3b/decode_nonnative_subs/wer_17_1.0
%WER 17.88 [ 2989 / 16713, 275 ins, 511 del, 2203 sub ] exp/tri2b/decode_test_subs/wer_16_1.0
%WER 1.71 [ 158 / 9215, 44 ins, 15 del, 99 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.5
%WER 1.65 [ 124 / 7498, 12 ins, 42 del, 70 sub ] exp/chain/tdnn1c_sp/decode_native_simple/wer_17_0.0
%WER 1.64 [ 123 / 7498, 15 ins, 38 del, 70 sub ] exp/chain/tdnn1c_sp_online/decode_native_simple/wer_17_0.0
%WER 1.54 [ 142 / 9215, 36 ins, 14 del, 92 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 15.30 [ 1147 / 7498, 149 ins, 154 del, 844 sub ] exp/tri3b/decode_native_subs.si/wer_17_1.0
%WER 14.62 [ 2444 / 16713, 282 ins, 359 del, 1803 sub ] exp/tri3b/decode_test_subs/wer_17_1.0
%WER 14.55 [ 1091 / 7498, 122 ins, 153 del, 816 sub ] exp/tri1/decode_native_subs/wer_13_1.0
%WER 1.40 [ 105 / 7498, 30 ins, 16 del, 59 sub ] exp/tri3b/decode_native_simple.si/wer_17_1.0
%WER 13.28 [ 996 / 7498, 119 ins, 123 del, 754 sub ] exp/tri2b/decode_native_subs/wer_15_0.5
%WER 1.21 [ 203 / 16713, 59 ins, 25 del, 119 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.14 [ 191 / 16713, 50 ins, 25 del, 116 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 1.04 [ 96 / 9215, 24 ins, 12 del, 60 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 10.26 [ 769 / 7498, 74 ins, 113 del, 582 sub ] exp/tri3b/decode_native_subs/wer_16_1.0
%WER 0.83 [ 138 / 16713, 33 ins, 23 del, 82 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 11 ins, 14 del, 44 sub ] exp/mono/decode_nonnative_simple/wer_17_0.0
%WER 0.64 [ 48 / 7498, 14 ins, 12 del, 22 sub ] exp/tri3b/decode_native_simple/wer_15_1.0
%WER 0.57 [ 96 / 16713, 15 ins, 27 del, 54 sub ] exp/mono/decode_test_simple/wer_17_0.0
%WER 0.56 [ 42 / 7498, 10 ins, 12 del, 20 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.55 [ 41 / 7498, 9 ins, 11 del, 21 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.37 [ 28 / 7498, 4 ins, 13 del, 11 sub ] exp/mono/decode_native_simple/wer_17_0.0
john@A-TEAM19054:~/work/kaldi/egs/heroico/s5$ 
- TODO Heroico: Contact Yenda about status of recipe.
- TODO Yaounde: What is wrong?
I decoded the training set:
%WER 21.59 [ 15107 / 69957, 1707 ins, 5179 del, 8221 sub ] exp/mono/decode_train/wer_12_1.0
This is still pretty bad.

- TODO Multilang: Minimal example.

** Goals for Friday:
- Objectives
- TODO Heroico: Run again with subs lm and without gplm.
- TODO Yaounde: Test on CA16.
- TODO African French: Get an lm working.
- TODO African French: Test on ca16.
* DAR <2017-10-04 Wed>
** Goals for Wednesday set Tuesday:
- TODO Objectives:

1. TECHNICAL COMPETENCE
a. Acoustic Models for Low Resource Languages
I. Problem
ASR components like acousti models are not available for key low resource languages and accented versions of major languages.
II. Research Question
Can small and large resources available from many languages be leveraged to build acoustic models for a language for which we have very few resources?
III. Proposed Method 
I will choose a target language say Korean for which we actually have some resources so that we can evaluate results. 
I will use the kaldi multilang recipe to build acoustic models for the target "low" resource language Korean given resources from many other source languages. 
I will obtain the source language resources from the GlobalPhone corpus and government owned corpora that are available to us (see below).
b. Corpus Curation
I. Problem:
In my previous job at West Point, I was part of a team that developed speech corpora for the following languages: 
A. Arabic (West Point LDC2002S02)
B. Arabic (Tunisia)
C. French (collected in Yaounde Cameroon)
D. Croatian (LDC2005S28)
E. German
F. Korean (LDC2006S36)
G. Portuguese (Brazilian LDC2008s04)
H. Russian (West Point LDC2003S05)
I. Russian (SOF Peter)
J. Spanish (Heroico LDC2006S37)

Of these 10 corpora, 6 were published in the Linguistic Data Consortium. 
The remaining 4 corpora for Arabic, French, German and Russian are available to our team and have yet to be published. 
Unless the corpora are published, results obtained from training ASR systems with them are not reproduceable.

ii. Proposed Method: 
I have 3 related goals this year concerning these 4 remaining corpora.
First, I want to prepare these corpora for use as source data in the multilang project mentioned above. 
Second, I want to publish these corpora in the openslrm.org repository.
Third, In addition to the multilang project, I want to write Kaldi recipes for each corpus. 

Publishing these corpora is an important goal. 
It is not hard to imagine these corpora disappearing after our generation retires. 

Preparing the data and writing the recipes will entail producing a lexicon that I also would like to publlish on openslr.org.

** Goals for Thursday:
- TODO Objectives:
- TODO SOFTunisia: Finish training with stage 16 speakers and get rough draft to ZAC.
- TODO African French: Build system without ARTI corpus.
- TODO Heroico: Incorporate subs trained lm into system.
- TODO Heroico: Contact Yenda about status of recipe.
- TODO Yaounde: What is wrong?
- TODO Multilang: Minimal example.

* DAR <2017-10-03 Tue>
** Goals for Tuesday set Monday:
- TODO Objectives.
- TODO Heroico: Get results for gplm 
The chain model WER results for the gplm decoding are not good.
I'm not sure what is wrong.
%WER 67.89 [ 6256 / 9215, 388 ins, 1391 del, 4477 sub ] exp/chain/tdnn1c_sp/decode_nonnative_gplm/wer_9_0.0
%WER 67.28 [ 6200 / 9215, 408 ins, 1353 del, 4439 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_gplm/wer_9_0.0
%WER 64.77 [ 5969 / 9215, 415 ins, 1251 del, 4303 sub ] exp/mono/decode_nonnative_gplm/wer_7_0.5
%WER 63.55 [ 10621 / 16713, 543 ins, 2647 del, 7431 sub ] exp/chain/tdnn1c_sp/decode_test_gplm/wer_9_0.5
%WER 62.47 [ 10441 / 16713, 688 ins, 2192 del, 7561 sub ] exp/chain/tdnn1c_sp_online/decode_test_gplm/wer_8_0.5
%WER 62.27 [ 10408 / 16713, 766 ins, 2062 del, 7580 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 59.18 [ 4437 / 7498, 338 ins, 808 del, 3291 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 58.08 [ 4355 / 7498, 316 ins, 863 del, 3176 sub ] exp/chain/tdnn1c_sp/decode_native_gplm/wer_7_1.0
%WER 58.01 [ 5346 / 9215, 647 ins, 754 del, 3945 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_13_1.0
%WER 56.82 [ 5236 / 9215, 385 ins, 1036 del, 3815 sub ] exp/tri1/decode_nonnative_gplm/wer_14_1.0
%WER 56.66 [ 4248 / 7498, 302 ins, 821 del, 3125 sub ] exp/chain/tdnn1c_sp_online/decode_native_gplm/wer_8_0.5
%WER 55.01 [ 5069 / 9215, 494 ins, 836 del, 3739 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.30 [ 8908 / 16713, 1171 ins, 1156 del, 6581 sub ] exp/tri3b/decode_test_gplm.si/wer_13_1.0
%WER 53.18 [ 8888 / 16713, 817 ins, 1494 del, 6577 sub ] exp/tri1/decode_test_gplm/wer_13_1.0
%WER 5.25 [ 484 / 9215, 32 ins, 164 del, 288 sub ] exp/chain/tdnn1c_sp/decode_nonnative_simple/wer_17_0.0
%WER 51.32 [ 4729 / 9215, 647 ins, 596 del, 3486 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_0.5
%WER 50.72 [ 8476 / 16713, 917 ins, 1277 del, 6282 sub ] exp/tri2b/decode_test_gplm/wer_14_1.0
%WER 5.00 [ 461 / 9215, 33 ins, 144 del, 284 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_simple/wer_16_0.0
%WER 48.25 [ 3618 / 7498, 513 ins, 403 del, 2702 sub ] exp/tri1/decode_native_gplm/wer_10_1.0
%WER 47.54 [ 7945 / 16713, 1171 ins, 916 del, 5858 sub ] exp/tri3b/decode_test_gplm/wer_16_0.5
%WER 47.43 [ 3556 / 7498, 424 ins, 482 del, 2650 sub ] exp/tri3b/decode_native_gplm.si/wer_16_1.0
%WER 4.54 [ 758 / 16713, 52 ins, 261 del, 445 sub ] exp/chain/tdnn1c_sp/decode_test_simple/wer_17_0.0
%WER 45.37 [ 3402 / 7498, 423 ins, 440 del, 2539 sub ] exp/tri2b/decode_native_gplm/wer_14_1.0
%WER 42.77 [ 3207 / 7498, 455 ins, 368 del, 2384 sub ] exp/tri3b/decode_native_gplm/wer_16_1.0
%WER 4.02 [ 672 / 16713, 49 ins, 227 del, 396 sub ] exp/chain/tdnn1c_sp_online/decode_test_simple/wer_17_0.0
%WER 3.96 [ 365 / 9215, 101 ins, 23 del, 241 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 3.56 [ 267 / 7498, 20 ins, 96 del, 151 sub ] exp/chain/tdnn1c_sp/decode_native_simple/wer_17_0.0
%WER 2.75 [ 206 / 7498, 13 ins, 74 del, 119 sub ] exp/chain/tdnn1c_sp_online/decode_native_simple/wer_17_0.0
%WER 2.70 [ 451 / 16713, 124 ins, 42 del, 285 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 1.66 [ 153 / 9215, 45 ins, 17 del, 91 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.0
%WER 1.25 [ 115 / 9215, 28 ins, 14 del, 73 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 1.20 [ 200 / 16713, 56 ins, 30 del, 114 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.19 [ 89 / 7498, 24 ins, 19 del, 46 sub ] exp/tri3b/decode_native_simple.si/wer_17_0.5
%WER 0.93 [ 156 / 16713, 32 ins, 27 del, 97 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 0.82 [ 76 / 9215, 17 ins, 7 del, 52 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 12 ins, 12 del, 45 sub ] exp/mono/decode_nonnative_simple/wer_15_0.0
%WER 0.69 [ 116 / 16713, 25 ins, 19 del, 72 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.62 [ 104 / 16713, 19 ins, 30 del, 55 sub ] exp/mono/decode_test_simple/wer_16_0.0
%WER 0.61 [ 46 / 7498, 10 ins, 13 del, 23 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.52 [ 39 / 7498, 8 ins, 12 del, 19 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.52 [ 39 / 7498, 4 ins, 13 del, 22 sub ] exp/tri3b/decode_native_simple/wer_16_0.0
%WER 0.39 [ 29 / 7498, 4 ins, 17 del, 8 sub ] exp/mono/decode_native_simple/wer_17_1.0

- TODO Heeroico: Build bigger lm and test.
I asked Justin to download the subs Spanishcorpus.
I'll try making an lm with subs.

- DONE SOFTunisia: Finish subs lm

- DONE SOFTunisia: Build cd gmm hmm system and chain models
I finished the cd gmm hmm and I sent Zac the rough draft for the simple decoding.
I did not do chain models.
- DONE: Get rough draft hypotheses for stage 16?
I sent Zac all the hypothesis transcripts for CTELLTWO.
I asked him to work on the first 4 speakers.
- TODO SOFTunisia: Compare WER scores for old and new lexicons(Zac will do this)

** Goals for Wednesday:
- TODO Objectives:
- TODO Heroico: Finish run on clsp cluster and contact Yenda
- TODO SOFTunisia: Get feed back from Zac and send him the hypotheses from the gplm decoding.
- TODO Yaounde: Why are WERs so bad?
- TODO Heroico: Build lm with subs? 
* DAR <2017-10-02 Mon>
** Goals for Next Week:
- TODO Objectives:
- TODO African French: build systems on progressively larger amounts of data.
- TODO Multilang: minimal example.
- TODO Yaounde: Writel recipe to kaldi standards (organize data).
- TODO Yaounde: Figure out why WER scores are so bad: test on training data
- TODO SOFTunisia: Rebuild system with Zac's new lexicon.
I focused on this today.
I am trying to make a clean fresh start.
I am building the new system in the softunisia/s5 directory.
I wrote new scripts to process the answers and recordings training data without copying files.
These scripts are very similar to the ones I wrote for heroico. 
Zac wants me to start from stage 15 and redo stage 16.
This is a good idea since he can compare the new lexicon with the old one. 
As I am getting ready to leave, I am building an LM with the subs corpus.

- Heroico: 
I added commands to my run.sh script to use the gp lm in testing.
I get the following WER results:
%WER 64.77 [ 5969 / 9215, 415 ins, 1251 del, 4303 sub ] exp/mono/decode_nonnative_gplm/wer_7_0.5
%WER 62.27 [ 10408 / 16713, 766 ins, 2062 del, 7580 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 59.18 [ 4437 / 7498, 338 ins, 808 del, 3291 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 58.01 [ 5346 / 9215, 647 ins, 754 del, 3945 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_13_1.0
%WER 56.82 [ 5236 / 9215, 385 ins, 1036 del, 3815 sub ] exp/tri1/decode_nonnative_gplm/wer_14_1.0
%WER 55.01 [ 5069 / 9215, 494 ins, 836 del, 3739 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.30 [ 8908 / 16713, 1171 ins, 1156 del, 6581 sub ] exp/tri3b/decode_test_gplm.si/wer_13_1.0
%WER 53.18 [ 8888 / 16713, 817 ins, 1494 del, 6577 sub ] exp/tri1/decode_test_gplm/wer_13_1.0
%WER 51.32 [ 4729 / 9215, 647 ins, 596 del, 3486 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_0.5
%WER 50.72 [ 8476 / 16713, 917 ins, 1277 del, 6282 sub ] exp/tri2b/decode_test_gplm/wer_14_1.0
%WER 48.25 [ 3618 / 7498, 513 ins, 403 del, 2702 sub ] exp/tri1/decode_native_gplm/wer_10_1.0
%WER 47.54 [ 7945 / 16713, 1171 ins, 916 del, 5858 sub ] exp/tri3b/decode_test_gplm/wer_16_0.5
%WER 47.43 [ 3556 / 7498, 424 ins, 482 del, 2650 sub ] exp/tri3b/decode_native_gplm.si/wer_16_1.0
%WER 45.37 [ 3402 / 7498, 423 ins, 440 del, 2539 sub ] exp/tri2b/decode_native_gplm/wer_14_1.0
%WER 42.77 [ 3207 / 7498, 455 ins, 368 del, 2384 sub ] exp/tri3b/decode_native_gplm/wer_16_1.0
%WER 3.96 [ 365 / 9215, 101 ins, 23 del, 241 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 2.70 [ 451 / 16713, 124 ins, 42 del, 285 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 1.66 [ 153 / 9215, 45 ins, 17 del, 91 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.0
%WER 1.25 [ 115 / 9215, 28 ins, 14 del, 73 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 1.20 [ 200 / 16713, 56 ins, 30 del, 114 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.19 [ 89 / 7498, 24 ins, 19 del, 46 sub ] exp/tri3b/decode_native_simple.si/wer_17_0.5
%WER 0.93 [ 156 / 16713, 32 ins, 27 del, 97 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 0.82 [ 76 / 9215, 17 ins, 7 del, 52 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 12 ins, 12 del, 45 sub ] exp/mono/decode_nonnative_simple/wer_15_0.0
%WER 0.69 [ 116 / 16713, 25 ins, 19 del, 72 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.62 [ 104 / 16713, 19 ins, 30 del, 55 sub ] exp/mono/decode_test_simple/wer_16_0.0
%WER 0.61 [ 46 / 7498, 10 ins, 13 del, 23 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.52 [ 39 / 7498, 8 ins, 12 del, 19 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.52 [ 39 / 7498, 4 ins, 13 del, 22 sub ] exp/tri3b/decode_native_simple/wer_16_0.0
%WER 0.39 [ 29 / 7498, 4 ins, 17 del, 8 sub ] exp/mono/decode_native_simple/wer_17_1.0

The chain models are training as I am getting ready to leave.

** Goals for Tuesday:
- TODO Objectives.
- TODO Heroico: Get results for gplm 
- TODO Heeroico: Build bigger lm and test.
- TODO SOFTunisia: Finish subs lm
- TODO SOFTunisia: Build cd gmm hmm system and chain models
- TODO: Get rough draft hypotheses for stage 16?
- TODO SOFTunisia: Compare WER scores for old and new lexicons(Zac will do this)
