* DAR <2018-01-02 Tue>
** Goals for Wednesday set Tuesday:
- TODO Work with Michelle's draft of the MTL paper.
- TODO Get results from multilang trilingual run.
%WER 18.19 [ 3324 / 18274, 367 ins, 695 del, 2262 sub ] exp/nnet3/multi_bnf/gp_mandarin/decode_dev/wer_13_0.0
%WER 17.94 [ 3278 / 18274, 396 ins, 667 del, 2215 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_13_0.0

So the WER went up aft adding the Bulgarian.
Did I do anything else?
18.19 is still better than the tri3b 19.07.

Here are the gp_mandarin WERs:

%WER 46.38 [ 9973 / 21502, 623 ins, 2029 del, 7321 sub ] exp/mono/decode_eval/wer_13_0.0
%WER 36.63 [ 6694 / 18274, 351 ins, 1489 del, 4854 sub ] exp/mono/decode_dev/wer_15_0.0
%WER 32.78 [ 7049 / 21502, 676 ins, 1358 del, 5015 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 32.02 [ 6886 / 21502, 747 ins, 1223 del, 4916 sub ] exp/tri3b/decode_eval.si/wer_16_0.0
%WER 31.17 [ 6703 / 21502, 725 ins, 1244 del, 4734 sub ] exp/tri2b/decode_eval/wer_17_0.0
%WER 27.48 [ 5909 / 21502, 631 ins, 1170 del, 4108 sub ] exp/tri3b/decode_eval/wer_17_0.5
%WER 23.89 [ 4365 / 18274, 407 ins, 850 del, 3108 sub ] exp/tri3b/decode_dev.si/wer_17_0.5
%WER 23.51 [ 4296 / 18274, 400 ins, 825 del, 3071 sub ] exp/tri1/decode_dev/wer_17_0.5
%WER 22.54 [ 4119 / 18274, 475 ins, 724 del, 2920 sub ] exp/tri2b/decode_dev/wer_17_0.0
%WER 21.52 [ 4628 / 21502, 587 ins, 893 del, 3148 sub ] exp/chain/tdnn1a_sp/decode_eval/wer_11_0.0
%WER 19.07 [ 3484 / 18274, 435 ins, 644 del, 2405 sub ] exp/tri3b/decode_dev/wer_17_0.5
%WER 15.52 [ 2836 / 18274, 359 ins, 585 del, 1892 sub ] exp/chain/tdnn1a_sp/decode_dev/wer_11_0.5

| model | dev WER |
| mono | 36.63 |
| tri1 | 23.51 |
| tri2b | 22.54 |
| tri3b | 19.07 |
| chain | 15.52 |

- TODO Investigate Paper publication venues.

** Goals for Thursday:
- TODO Multilang: Get results for run with i-vectors and 7 languages.
- TODO MTL Paper: Write background.

* DAR <2018-01-02 Tue>
** Goals for January:
- TODO Setup work environment on b-team GPU workstation.
- TODO Multilang: Flesh out paper idea. Search for publication venue.
- TODO Multilang: Extend build script to use i-vectors and bottleneck features.
- TODO GP: Take a pass through languages that are not performing well yet.
- TODO Multilang: Add well behaving GP languages  to build.
- TODO S2S: minimal example.
- TODO AMTA: Get data from endangered languages.
- TODO Softunisia: Write an end 2 end recipe suitable for submission to the kaldi repository.
- TODO African French ditto for African French (Yaounde)
- TODO Heroico: Write tn.
- TODO Heroico: Contact Dan and Yenda about publishing the recipe.
- TODO African French: Write outline of paper.

- Current WER scores for GP:
| language | tri3b| chain |
| Arabic dev | 55.98 | 51.17 |
| Bulgarian dev | 24.78      | 19.47 |
| Croatian dev | 28.53 | |
| Czech dev | 43.72 | |
| French dev | 93.41 | |
| German dev | 38.04 | |
| Hausa dev | 24.64 | |
| Japanese dev | 6.15 | |
| Korean dev | 25.64 | |
| Mandarin dev | 19.07 | |
| Polish dev | 48.23 | |
| Portuguese dev | 24.11 | |
| Russian dev | 55.81 | 49.23 |
| Spanish dev | 42.97 | |
| Swedish dev | 62.07 | |
| tamil dev | | |
| Thai dev | | |
| Turkish dev | 75.25 | |
| Vietnamese dev | 37.49 | |

** Goals for Wednesday
- TODO Work with Michelle's draft of the MTL paper.
- TODO Get results from multilang trilingual run.
- TODO Investigate Paper publication venues.

* DAR <2017-12-13 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Multilang: Write a script that runs end to end and uses the above setup commands:
The commands were:
mkdir -p data/gp_japanese data/gp_mandarin
#  link source data/ train directories into building directory:
ln -s ~/gp_japanese/s5/data/train data/gp_japanese
ln -s ~/gp_mandarin/s5/data/train data/gp_mandarin
# make experiment directories under multilang building directory:
mkdir -p exp/gp_japanese exp/gp_mandarin
# link source tri3b alignment directories into multilang building directory:
ln -s ~/gp_japanese/s5/exp/tri3b_ali exp/gp_japanese
ln -s ~/gp_mandarin/s5/exp/tri3b_ali exp/gp_mandarin

Here are the WER scores I got after decoding:
%WER 17.94 [ 3278 / 18274, 396 ins, 667 del, 2215 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_13_0.0

I am going to work on building chain models for gp_mandarin.
then I can compare the multilang results with a neural network model.

- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- DONE African French: Write outline of paper.
- TODO Softunisia: Chain models.
I worked on the lexicon.
I restricted the words to only those appearing in the training set.
I think I need to include the test data in the list of words.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

** Goals for Thursday:
- TODO Softunisia: Incorporate test set words into lexicon.
- TODO Softunisia:  Convert phones in  lexicon to IPA utf8.
- TODO Softunisia: Test new lexicon with gmm hmms and chain models.
- TODO Multilang: Get gp_mandarin chain model results.
- TODO Multilang: Incorporate i-vectors and bottlenect layer into neural network model.
- TODO Multilang: Refine paper outline ( focus on incorporating diverse genres: Broadcast News and Read Speech).
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

* DAR <2017-12-12 Tue>
** Goals for Tuesday set Monday:
- TODO Multilang: Minimal example. Get scripts to do the right thing with the decoding graph.
I'm going to try to list the steps for a minimal exapmle:
- Languages: 
gp_japanese and gp_mandarin

- Locations:
~/gp_japanese/s5 and ~/gp_mandarin/s5

These two directories are the locations where I built the tri3b models.

- multilang Building directory:
~/multilang/s5
This is where we will build new multilang models.

- move to the multilang building directory
cd ~/multilang/s5

- make data directories in building directory:
mkdir -p data/gp_japanese data/gp_mandarin

- link source data/ train directories into building directory:
ln -s ~/gp_japanese/s5/data/train data/gp_japanese
ln -s ~/gp_mandarin/s5/data/train data/gp_mandarin

- make experiment directories under multilang building directory:
mkdir -p exp/gp_japanese exp/gp_mandarin

- link source tri3b alignment directories into multilang building directory:
ln -s ~/gp_japanese/s5/exp/tri3b_ali exp/gp_japanese
ln -s ~/gp_mandarin/s5/exp/tri3b_ali exp/gp_mandarin

- feature extraction:
I wanted to avoid plp features and speed perturbation, but it is happening automatically somehow.


- link dev data:
ln -s ~/gp_mandarin/s5/data/dev data/gp_mandarin/
This and the following commands are  for linking the directory that is needed for decoding

- link lang directory
ln -s ~/gp_mandarin/s5/data/lang_test data/gp_mandarin/

- link tree directory
ln -s ~/gp_mandarin/s5/exp/tri3b/tree  exp/nnet3/multi/gp_mandarin/

- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

** Goals for Wednesday:
- TODO Multilang: Write a script that runs end to end and uses the above setup commands
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?


* DAR <2017-12-11 Mon>
**  Goals for Monday set Friday:
- DONE Multilang: Decode the target Russian and Spanish with the new hybrid multilang system. 
I am marking this goal as DONE even though it was modified.
I had a misunderstanding about how multilang works and I might still have a misunderstandig. 
I thought the source and target languages were supposed to be different.
My understanding now is that the input and output languages are the same.
The process is meant to improve all(?) the input languages. 
In particular it improves the low resource language.
But the low resource language is one of the input languages.
So I modified my minimal example (and now it is really minimal) to only work with japanese and mandarin.
The idea is that the parameters from these two languages are shared. 
One model set is output (is this right?).
That model can be used to decode both Japanese and mandarin (really?).
Anyway, I am not sure about all the steps in this multilang process. 
I ran the multilang training scripts without any bells and whistles. 
No i-vectors, no pitch and no bottleneck features; I don't even thik I did speed perturbation. 
A neural network was trained. 
Then, I think what happens is that the final layer is adjusted for each language.
So  2 sllightly different models are written, one for Japanese and one for Mandarin?
Then I decoded the Mandaring dev data with the new mandarin model.
I had to do some of this by hand, the scripts did not make sense to me.
Specifically, the scripts wanted to make the decoding graph in the old tri3b directory.
This can't be right.
Anyway, I copied the tree and the lang_test directories from the tri3b model set to the new nnet3/multi model directories.
I ran mkgraph in the new mandarin directory.
Then I decoded with this graph.
The results:
%WER 18.17 [ 3320 / 18274, 373 ins, 697 del, 2250 sub ] exp/nnet3/multi/gp_mandarin/decode_dev/wer_14_0.0

I don't know yet if this is good or not, but at least it is better than the tri3b WER wich was 19.07.
I would have to train the neural network models without the multilang than compare them.
But this seems pretty good since I am not using pitch which was used in the tri3b results.
Anyway, I am happy since I got a minimal example to run.
I'll have to work on the script to make it run without any intervention. 

- TODO Softunisia: Write an end 2 end recipe suitable for submission to the kaldi repository.
I made some progress on this today.
I got the sarraj data from Zac.
I am working on incorporating this into the test set.
I worked a lot on getting the westpoint data incorporated as test data.
I'd rather use data that we can put on openslr.org as test data.

- TODO African French ditto for African French (Yaounde)
- TODO Heroico: Write tn.
- TODO African French: Write outline of paper.
- DONE Softunisia Recipe: Test set. (Zac's transcription of Libian data, Westpoint?)
I got a lot done on this today.
In fact I am probably almost finished with this, so I'm going to mark it DONE.

- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.


** Goals for Tuesday:
- TODO Multilang: Minimal example. Get scripts to do the right thing with the decoding graph.
- TODO African French: Write  a recipe suitable for kaldi submission (Yaounde).
- TODO Heroico: Write tn.
- TODO Heroico: Get response from Dan and Yenda.
- TODO African French: Write outline of paper.
- TODO Softunisia: Chain models.
- TODO Softunisia: Testing with chain models.
- TODO S2S: minimal example.
- TODO AMTA: Data from endangered languages.
- TODO AMTA: Writing?

* DAR <2017-12-08 Fri>
**  Goals for Friday set Thursday:
- TODO Multilang: Simplify multilang recipe. Hard code sp, hires, pitch, bnf.
I did not do this.
Instead I reverted back to the original script.
I set bnf, speed perturb, ivector and ivector to false.
I am trying to get a minimal system to run.
I also restricted the source languages to only the  two languages Japanese and Mandarin.
I made a lot of progress today with this strategy.
In fact, I got passed the training stage and adjustment of priors.
I am trying to get the decoding to run.
I am decoding on Russian for now.

- TODO Softunisia: Write official recipe with chain models (test set?).
I also made a lot of progress on this goal today.
I am working on a test set.
For now I am working with the West Point test data.
I trained up to tri1.
- TODO GP: take a pass on all the languages to try to improve WERs.
- TODO S2S: minimal example.
- TODO Write tn for Heroico.

* DAR <2017-12-07 Thu>
** Goals for Thursday set Wednesday:
- DONE Multilang: Train the i-vector extractor.
I am going back to the beginning.
I need to get the variables set correctly.
Variables include the suffixes for directories.
I am including pitch, speed perturbation and bottlenec features all of which makes suffixes get appended to directory names.
The suffixes and affixes are a mess.
I got pretty far with this today, but the directory names is too messed up.
I am only working with 5 languages:
gp_hausa gp_japanese gp_korean gp_mandarin  gp_portuguese            
- TODO Softunisia: Train chain models.
- TODO SofTunisia: Make an official kaldi recipe.
I got started on this.
- TODO Heroico: Contact Dan and Yenda.
I sent a message.
Have not heard back yet.

- TODO S2S: Minimal example.
- TODO GP: Take another pass on each language to try to improve WER scores.

** Goals for Friday:
- TODO Multilang: Simplify multilang recipe. Hard code sp, hires, pitch, bnf.
- TODO Softunisia: Write official recipe with chain models (test set?).
- TODO GP: take a pass on all the languages to try to improve WERs.
- TODO S2S: minimal example.
- TODO Write tn for Heroico.
* DAR <2017-12-06 Wed>
** Goals for Wednesday: set Tuesday
- DONE Brief Reggie on Multilang project.
- DONE GP Russian: Train through tri3b_ali.

%WER 73.73 [ 13468 / 18266, 933 ins, 2027 del, 10508 sub ] exp/mono/decode_eval/wer_8_1.0
%WER 69.25 [ 13131 / 18962, 644 ins, 2626 del, 9861 sub ] exp/mono/decode_dev/wer_9_1.0
%WER 60.97 [ 11137 / 18266, 1537 ins, 1140 del, 8460 sub ] exp/tri3b/decode_eval.si/wer_16_1.0
%WER 59.76 [ 10916 / 18266, 1053 ins, 1508 del, 8355 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 59.58 [ 10883 / 18266, 1277 ins, 1282 del, 8324 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 58.33 [ 10655 / 18266, 1538 ins, 1059 del, 8058 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 57.71 [ 10943 / 18962, 1422 ins, 1346 del, 8175 sub ] exp/tri3b/decode_dev.si/wer_16_1.0
%WER 56.92 [ 10793 / 18962, 1085 ins, 1678 del, 8030 sub ] exp/tri1/decode_dev/wer_16_1.0
%WER 56.58 [ 10729 / 18962, 1217 ins, 1557 del, 7955 sub ] exp/tri2b/decode_dev/wer_16_1.0
%WER 55.81 [ 10582 / 18962, 1479 ins, 1274 del, 7829 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 49.23 [ 8993 / 18266, 642 ins, 1329 del, 7022 sub ] exp/chain/tdnn1c_sp/decode_eval/wer_11_1.0
%WER 49.21 [ 8988 / 18266, 787 ins, 1160 del, 7041 sub ] exp/chain/tdnn1c_sp_online/decode_eval/wer_10_1.0
%WER 46.87 [ 8888 / 18962, 883 ins, 1214 del, 6791 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_9_1.0
%WER 46.78 [ 8870 / 18962, 855 ins, 1233 del, 6782 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_9_1.0


- Current WER scores for GP:
| language | tri3b| chain |
| Arabic dev | 70.73 | 64.57 |
| Bulgarian dev | 24.78      | 19.47 |
| Croatian dev | 28.53 | |
| Czech dev | 43.72 | |
| French dev | 93.41 | |
| German dev | 38.04 | |
| Hausa dev | 24.64 | |
| Japanese dev | 6.15 | |
| Korean dev | 25.64 | |
| Mandarin dev | 19.07 | |
| Polish dev | 48.23 | |
| Portuguese dev | 24.11 | |
| Russian dev | 55.81 | 49.23 |
| Spanish dev | 42.97 | |
| Swedish dev | 62.07 | |
| tamil dev | | |
| Thai dev | | |
| Turkish dev | 75.25 | |
| Vietnamese dev | 37.49 | |

- TODO Multilang: Fix problem with alignment file location.
I had to soft link the Russian directories.
Here is the command I am running:

steps/train_lda_mllt.sh --cmd run.pl --num-iters 3 --splice-opts "--left-context=3 --right-context=3" --boost-silence 1.5 6000 75000 data/gp_russian/train_sp_hires data/gp_russian/lang exp/gp_russian/tri3b_ali exp/gp_russian/nnet3_pitch/tri3b 

That command ran, but when I go to run the master script it requires other arguments:
This command seems to run:
steps/train_lda_mllt.sh --cmd run.pl --num-iters 13 --splice-opts --left-context=3 --right-context=3 --boost-silence 1.5 6000 75000 data/gp_russian/train_sp_hires_pitch data/gp_russian/lang exp/gp_russian/tri3b_ali_sp exp/gp_russian/nnet3_pitch/tri3b
Notice the _pitch suffix.
I've got to fix these problems in the script.

- TODO Multilang: Minimal example.
- TODO Multilang: Train a global i-vecgor extractor on pooled data.
I think the lda_mllt training above is a step towards training the i-vector extractor.

- TODO S2S Demo: Write files to correct locations for decoding.
- TODO Heroico: Contact Dan and Yenda ( I think I'm done).
- DONE Softunisia: Get stage 19 hypotheses to Zac. (6 speakers)

** Goals for Thursday:
- TODO Multilang: Train the i-vector extractor.
- TODO Softunisia: Train chain models.
- TODO SofTunisia: Make an official kaldi recipe.
- TODO Heroico: Contact Dan and Yenda.
- TODO S2S: Minimal example.
- TODO GP: Take another pass on each language to try to improve WER scores.
  
* DAR <2017-12-05 Tuef>
**  Goals for Tuesday set Monday:
- DONE GP Russian: Convert Romanized Russian to UTF8 with charmap given in documentation.
Here are the WERs I have so far:
%WER 73.73 [ 13468 / 18266, 933 ins, 2027 del, 10508 sub ] exp/mono/decode_eval/wer_8_1.0
%WER 69.25 [ 13131 / 18962, 644 ins, 2626 del, 9861 sub ] exp/mono/decode_dev/wer_9_1.0
%WER 59.76 [ 10916 / 18266, 1053 ins, 1508 del, 8355 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 56.92 [ 10793 / 18962, 1085 ins, 1678 del, 8030 sub ] exp/tri1/decode_dev/wer_16_1.0
%WER 56.58 [ 10729 / 18962, 1217 ins, 1557 del, 7955 sub ] exp/tri2b/decode_dev/wer_16_1.0

- Current WER scores for GP:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | 26.24 | 24.11 | | |
| Russian dev | 69.25 | 56.92 | 56.58 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

- TODO S2S Demo: Decode input with tri2b English models.
Worked on this, but still have problems writing files to the correct place. 
- TODO S2S Demo: Repeat with tri2b Spanish models what was done for English.
- TODO Heroico: Contact Dan and Yenda about recipe.
- TODO Multilang: Minimal Example with 14 GP languages.
The scripts expect an alignment file under tri3b_ali_sp.
The alignments are actually under tri3b_ali_train_sp.


** Goals for Wednesday:
- TODO Brief Reggie on Multilang project.
- TODO GP Russian: Train through tri3b_ali.
- TODO Multilang: Fix problem with alignment file location.
- TODO Multilang: Minimal example.
- TODO Multilang: Train a global i-vecgor extractor on pooled data.
- TODO S2S Demo: Write files to correct locations for decoding.
- TODO Heroico: Contact Dan and Yenda ( I think I'm done).
- TODO Softunisia: Get stage 19 hypotheses to Zac. (6 speakers)

* DAR <2017-12-04 Mon>
** Goals for Next Week:
- TODO Multilang: Train SAT models for all gp languages.
GP Russian is still a mystery.
The dictionary and the transcriptions seem to come from different places.
I think the problem is with the transcripts.
Heer is what the GP documentation says:
I. Dictionary Generation and Format
The phone-based pronunciation dictionary for Russian contains the pronunciations of all word forms found in the transcription data of the GlobalPhone audio recordings of this language. 
The word forms are given in original Russian Cyrillic script in UTF-8 encoding like it appears in the transcription of the GlobalPhone speech and  text corpus in the directory /trl. 

No, they are not.

The dictionary can also be provided in Romanized script using ASCII encoding as appearing in the directory “/rmn” in the speech & text corpus. 
The conversion between the Roman and the Cyrillic script is given in the section “Romanization” below.

Maybe I can convert from the romanization to utf8?
Yes, I think this is what I need to do.
II. Romanization
The following list describes the original Cyrillic characters used in Russian script, the corresponding UTF-8 code point, and the Romanized form as used in the Romanized transcription files. 
The Romanization and Back-transformation can be achieved by simple one-to-one reversible substitution rules based on regular expressions, e.g. in case of tcl use the following line to convert
“w” into “в”: regsub -all {w} $temp "\u0432" temp
| Original Russian character | Romanized Character | Unicode code point | Description |
| а | a | U+0430 | CYRILLIC SMALL LETTER A |
|б \ b | U+0431 | CYRILLIC SMALL LETTER BE |
в w U+0432 CYRILLIC SMALL LETTER VE
г g U+0433 CYRILLIC SMALL LETTER GHE
д d U+0434 CYRILLIC SMALL LETTER DE
е ye U+0435 CYRILLIC SMALL LETTER IE
ж jscH U+0436 CYRILLIC SMALL LETTER ZHE
з z U+0437 CYRILLIC SMALL LETTER ZE
и i U+0438 CYRILLIC SMALL LETTER I
й j U+0439 CYRILLIC SMALL LETTER SHORT I
к k U+043A CYRILLIC SMALL LETTER KA
л l U+043B CYRILLIC SMALL LETTER EL
м m U+043C CYRILLIC SMALL LETTER EM
н n U+043D CYRILLIC SMALL LETTER EN
о o U+043E CYRILLIC SMALL LETTER O
п p U+043F CYRILLIC SMALL LETTER PE
р r U+0440 CYRILLIC SMALL LETTER ER
с s U+0441 CYRILLIC SMALL LETTER ES
т t U+0442 CYRILLIC SMALL LETTER TE
у u U+0443 CYRILLIC SMALL LETTER U
ф f U+0444 CYRILLIC SMALL LETTER EF
х h U+0445 CYRILLIC SMALL LETTER HA
ц tS U+0446 CYRILLIC SMALL LETTER TSE
ч tscH U+0447 CYRILLIC SMALL LETTER CHE
ш sch U+0448 CYRILLIC SMALL LETTER SHA
щ schTsch U+0449 CYRILLIC SMALL LETTER SHCHA
ъ Q U+044A CYRILLIC SMALL LETTER HARD SIGN
ы i2 U+044B CYRILLIC SMALL LETTER YERU
ь ~ U+044C CYRILLIC SMALL LETTER SOFT SIGN
э e U+044D CYRILLIC SMALL LETTER E
ю yu U+044E CYRILLIC SMALL LETTER YU
я ya U+044F CYRILLIC SMALL LETTER YA
А A U+0410 CYRILLIC CAPITAL LETTER A
Б B U+0411 CYRILLIC CAPITAL LETTER BE
В W U+0412 CYRILLIC CAPITAL LETTER VE
Г G U+0413 CYRILLIC CAPITAL LETTER GHE
Д D U+0414 CYRILLIC CAPITAL LETTER DE
Е YE U+0415 CYRILLIC CAPITAL LETTER IE
Ж JscH U+0416 CYRILLIC CAPITAL LETTER ZHE
З Z U+0417 CYRILLIC CAPITAL LETTER ZE
И I U+0418 CYRILLIC CAPITAL LETTER I
Й J U+0419 CYRILLIC CAPITAL LETTER SHORT I
К K U+041A CYRILLIC CAPITAL LETTER KA
Л L U+041B CYRILLIC CAPITAL LETTER EL
М M U+041C CYRILLIC CAPITAL LETTER EM
Н N U+041D CYRILLIC CAPITAL LETTER EN
О O U+041E CYRILLIC CAPITAL LETTER O
П P U+041F CYRILLIC CAPITAL LETTER PE
Р R U+0420 CYRILLIC CAPITAL LETTER ER
С S U+0421 CYRILLIC CAPITAL LETTER ES
Т T U+0422 CYRILLIC CAPITAL LETTER TE
У U U+0423 CYRILLIC CAPITAL LETTER U
Ф F U+0424 CYRILLIC CAPITAL LETTER EF
Х H U+0425 CYRILLIC CAPITAL LETTER HA
Ц TS U+0426 CYRILLIC CAPITAL LETTER TSE
Ч TscH U+0427 CYRILLIC CAPITAL LETTER CHE
Ш Sch U+0428 CYRILLIC CAPITAL LETTER SHA
Щ SchTsch U+0429 CYRILLIC CAPITAL LETTER SHCHA
Ъ Q U+042A CYRILLIC CAPITAL LETTER HARD SIGN
Ы I2 U+042B CYRILLIC CAPITAL LETTER YERU
Ь ~ U+042C CYRILLIC CAPITAL LETTER SOFT SIGN
Э E U+042D CYRILLIC CAPITAL LETTER E
Ю Yu U+042E CYRILLIC CAPITAL LETTER YU
Я Ya U+042F CYRILLIC CAPITAL LETTER YA

- TODO Multilang: USE alignments from SAT models to start multilang building process.
I am starting with 14 gp languages.
- TODO Heroico: Contact Dan Povey and Yenda about next step (am I finished? Is the recipe ready?)
- TODO Write TN.
- TODO S2S: Minimal example using English mini_librispeech and Heroico Spanish.
I worked a little on this today.
I have a script that invokes a recording program called rec.
I can extract mfcc features and cmvn them.

- TODO Softunisia: Retrain and get transcripts to Zac.
Zac called me.
He had problems with the transcripts  I sent him.
He was able to retrieve them intact from my github webpage.


** Goals for Tuesday:
- TODO GP Russian: Convert Romanized Russian to UTF8 with charmap given in documentation.
- TODO S2S Demo: Decode input with tri2b English models.
- TODO S2S Demo: Repeat with tri2b Spanish models what was doen for English.
- TODO Heroico: Contact Dan and Yenda about recipe.
- TODO Multilang: Minimal Example with 14 GP languages.

* DAR <2017-12-01 Fri>
** oals for Friday set Thursday:
- DONE Heroico: 1e experiment with 7 epochs instead of 10 to avoid overfitting.
./local/chain/compare_wer.sh exp/chain/tdnn1d_sp exp/chain/tdnn1e_sp
System                tdnn1d_sp tdnn1e_sp
WER devtest       52.78     52.21
WER native       55.32     53.43
nonnative     64.35     61.03
WER test       60.28     57.70
 Final train prob        -0.0229   -0.0250
 Final valid prob        -0.0683   -0.0678
 Final train prob (xent)   -0.7525   -0.7887
 Final valid prob (xent)   -1.0296   -1.0419

-  info
exp/chain/tdnn1e_sp:
 num-iters=105
 nj=1..1
 num-params=6.6M
 dim=40+100->1392
 combine=-0.036->-0.033
 xent:train/valid[69,104,final]=(-1.20,-0.917,-0.789/-1.35,-1.16,-1.04)
 logprob:train/valid[69,104,final]=(-0.049,-0.030,-0.025/-0.082,-0.075,-0.068)

- Word Error Rates on folds
%WER 61.03 [ 5624 / 9215, 630 ins, 727 del, 4267 sub ] exp/chain/tdnn1e_sp/decode_nonnative/wer_8_1.0
%WER 57.70 [ 9644 / 16713, 1249 ins, 1040 del, 7355 sub ] exp/chain/tdnn1e_sp/decode_test/wer_7_1.0
%WER 53.43 [ 4006 / 7498, 558 ins, 408 del, 3040 sub ] exp/chain/tdnn1e_sp/decode_native/wer_7_1.0
%WER 52.21 [ 3994 / 7650, 585 ins, 456 del, 2953 sub ] exp/chain/tdnn1e_sp/decode_devtest/wer_9_1.0

| fold | 1a | 1b | 1c | 1d | 1e |
| devtest | 54.46 | 54.20 | 54.16 | 52.78 | 52.21 |
| native |  62.14 | 62.32 | 61.70 | 55.32 | 53.43 |
| nonnative | 70.58 | 71.20 | 71.68 | 64.35 | 61.03 |
| test | 66.85 | 67.21 | 67.25 | 60.28 | 57.70 |

- TODO Heroico: Write tn .
- TODO AMTA2018: Work on Minimal Example (English mini_librispeechSpanish GP/Heroico?)
- TODO Multilang: Train up to Russian tri3b_ali.

* DAR <2017-11-30 Thu>
** Goals for Thursday set Wednesday:
- DONE SofTunisia: Retrain with Zac's new dictionary.
The tri3b decoding finished.
I have to get it to Zac.
I tried to send him the file in an email.

- DONE Heroico: 1d remove proportional shrinking.
I  removed the proportional shrinking option but I also added the l2 regularization on the 8 layer setup.

%WER 64.35 [ 5930 / 9215, 726 ins, 734 del, 4470 sub ] exp/chain/tdnn1d_sp/decode_nonnative/wer_7_1.0
%WER 60.28 [ 10074 / 16713, 1324 ins, 1175 del, 7575 sub ] exp/chain/tdnn1d_sp/decode_test/wer_7_1.0
%WER 55.32 [ 4148 / 7498, 600 ins, 435 del, 3113 sub ] exp/chain/tdnn1d_sp/decode_native/wer_7_1.0
%WER 52.78 [ 4038 / 7650, 708 ins, 401 del, 2929 sub ] exp/chain/tdnn1d_sp/decode_devtest/wer_8_1.0

# | fold | 1a | 1b | 1c | 1d | 1e |
#| devtest | 54.46 | 54.20 | 54.16 | 52.78 |
#| native |  62.14 | 62.32 | 61.70 | 55.32 |
#| nonnative | 70.58 | 71.20 | 71.68 | 64.35 |
#| test | 66.85 | 67.21 | 67.25 | 60.28 |

The change made a big difference.
There are now 8 layers that use l2 regularization.

- DONE Multilang: Train tri3b models for Portuguese
%WER 43.56 [ 2752 / 6318, 185 ins, 672 del, 1895 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 28.00 [ 1769 / 6318, 265 ins, 279 del, 1225 sub ] exp/tri3b/decode_dev.si/wer_17_0.0
%WER 27.45 [ 1734 / 6318, 274 ins, 240 del, 1220 sub ] exp/tri1/decode_dev/wer_16_0.0
%WER 26.24 [ 1658 / 6318, 244 ins, 252 del, 1162 sub ] exp/tri2b/decode_dev/wer_17_0.0
%WER 24.11 [ 1523 / 6318, 249 ins, 225 del, 1049 sub ] exp/tri3b/decode_dev/wer_17_0.0


- TODO Write paper.
I wrote a little bit more on the tn and a very rough draft of the AMTA abstract.

- Current WER scores for GP:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | 26.24 | 24.11 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |


** oals for Friday:
- TODO Heroico: 1e experiment with 7 epochs instead of 10 to avoid overfitting.
- TODO Heroico: Write tn .
- TODO AMTA2018: Work on Minimal Example (English mini_librispeechSpanish GP/Heroico?)
- TODO Multilang: Train up to Russian tri3b_ali.

* DAR  <2017-11-29 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Write paper.
I wrote some more on the Heroico project.
- DONE Heroico: Next experiment (1c)
1b lowered the number of leaves from 3500 to 200 and 3 out of 4 WERS went up.
1c will set number of leaves to 2500.
# | fold | 1a | 1b | 1c |
| devtest | 54.46 | 54.20 | 54.16 |
| native |  62.14 | 62.32 | 61.70 |
| nonnative | 70.58 | 71.20 | 71.68 |
| test | 66.85 | 67.21 | 67.25 |

Inconclusive.
I am resetting the number of leaves to 3500 and removing the proportional shrink in experiment 1d.
- TODO GP: Train tri3b models for all languages.
I started Korean today.
- TODO Multilang: Get minimal example running.
I am going to wait until I have the tri3b models for most of the languages.
I am missing Korean and Portuguese and probably several more.
Korean should be done soon.
I'll finish Portuguese tomorrow.

** Goals for Thursday:
- TODO SofTunisia: Retrain with Zac's new dictionary.
- TODO Heroico: 1d remove proportional shrinking.
- TODO Multilang: Train tri3b models for Portuguese
- TODO Write paper.

* DAR <2017-11-28 Tue>
** Goals for Tuesday set Monday:
- TODO Write something!
I am writing about the Heroico corpus.
I will probably make this into another TN like the one we wrote for the Yaounde Speech Corpus. 

- TODO multilang: Get minimal example running.
- TODO Multilang: Train all languages up through tri3b_ali. 
- TODO Heroico: Tuning experiments.
I finished the 1a run:
1b is running. It uses a smaller (2000 instead of 3500) number of leaves.

| fold | 1a |
| devtest | 54.46 |
| native |  62.14 |
| nonnative | 70.58 |
| test | 66.85 |

SofTunisia: Zac called me. He reminded me that I owed him for stage 17 done with the new dictionary.
I had run it a couple of weeks ago.
I found some problems with the run. 
I fixed the problems and I reran the old stage with the new dictionary.
Hazrat emailed the results to Zac.

** Goals for Wednesday:
- TODO Write paper.
- TODO Heroico: Next experiment (1c)
- TODO GP: Train tri3b models for all languages.
- TODO Multilang: Get minimal example running.

* <2017-11-27 Mon>
** Goals for After leave:
- TODO GlobalPhone: Make a pass through each language to check if text encoding (including casing) matches between text to train acoustic models, text to train lm and text in dictionary.

Bulgarian: I deleted control m characters. 
Croatian: remove cr. LM is not clear . Possible OOV problem. Dictionary has words with upper case first letter.
Czech: remove cr and down case
Korean: The LM is for hangul characters in UTF8.
Polish: Delete control MM and down case
Russian: Remove cr
Swedish: The 3 components are in different encodings!
The GP dictionary is originally in ASCII.
Thai: Weird line termiators.



| language | am train text  | dict | lm |
| Arabic | ascii | ascii | ascii |
| Bulgarian | UTF8 | UTF8 | UTF8 |
| Croatian | UTF8 | UTF8 | ? |
| Czech | UTF8 | UTF8 | UTF8 |
| French | UTF8 | UTF8 | UTF8 |
| German | UTF8 | UTF8 | UTF8 |
| Hausa | ASCII | ASCII | ASCII |
| Japanese | UTF8 | UTF8 | UTF8 |
| Korean | ASCII | ASCII | ASCII |
| Mandarin | ASCII | ASCII | ASCII |
| Polish | UTF8 | UTF8 | UTF8 |
| Portuguese | UTF8 | UTF8 | UTF8 | 
| Russian | UTF8 | UTF8 | UTF8 |
| Spanish | UTF8 | UTF8 | UTF8 |
| Swedish | ASCII | ASCII | UTF8 (ASCII with the exception of  —) |
| Tamil | UTF8 | UTF8 | UTF8 |
| Thai | UTF8 lf nel terminators | UTF8 | UTF8 |
| Turkish | UTF8 | UTF8 |
| Vietnamese | UTF8 | UTF8 (fortran) | UTF8 |

- TODO Heroico: Address Dan's comments ( try to get l2 regularization working)
- TODO Heroico: Modify layers (from 6 to 8)
I am starting from the very beginning.
- TODO Write paper.
- TODO African French: Work with Steve, Luis and Mike Li to get minimal Ultra example working.


** Goals for Tuesday:
- TODO Write something!
- TODO multilang: Get minimal example running.
- TODO Multilang: Train all languages up through tri3b_ali. 
- TODO Heroico: Tuning experiments.

* DAR <2017-11-16 Thu>
**  Goals for Thursday set Wednesday:
- TODO African French: Miniturize. Build up the lm from a minimal working example.
I included the transcripts  for the ca16 test  set  in the lm training set and here are the WER scores:
%WER 31.59 [ 1008 / 3191, 60 ins, 342 del, 606 sub ] exp/mono/decode_ca16/wer_11_0.0

%WER 20.78 [ 663 / 3191, 79 ins, 163 del, 421 sub ] exp/tri3b/decode_ca16.si/wer_17_0.0

%WER 19.24 [ 614 / 3191, 91 ins, 134 del, 389 sub ] exp/tri1/decode_ca16/wer_14_0.0

%WER 17.80 [ 568 / 3191, 76 ins, 130 del, 362 sub ] exp/tri2b/decode_ca16/wer_17_0.0

%WER 16.14 [ 515 / 3191, 85 ins, 99 del, 331 sub ] exp/tri3b/decode_ca16/wer_17_0.0

| model | lm train | lm train + test  |
| mono | 80.38 | 31.59 |
| tri1 | 62.93 | 19.24 |
| tri2b | 60.61 | 17.80 |
| tri3b | 58.26 | 16.14 |

I tried decoding without retraining.

%WER 99.66 [ 3180 / 3191, 13 ins, 1621 del, 1546 sub ] exp/mono/decode_ca16/wer_16_0.5

%WER 20.78 [ 663 / 3191, 79 ins, 163 del, 421 sub ] exp/tri3b/decode_ca16.si/wer_17_0.0

%WER 16.14 [ 515 / 3191, 85 ins, 99 del, 331 sub ] exp/tri3b/decode_ca16/wer_17_0.0

%WER 101.44 [ 3237 / 3191, 81 ins, 1166 del, 1990 sub ] exp/tri2b/decode_ca16/wer_17_1.0

%WER 100.19 [ 3197 / 3191, 43 ins, 1506 del, 1648 sub ] exp/tri1/decode_ca16/wer_17_1.0

This dos not look good so I'm retraining from the beginning.
- TODO Heroico: Address Dan's comments.  remove proportional shrinking
- TODO Heroico: L2 regularization? Does it work with my current version of kaldi?
- TODO Heroico: Modify layers (from 6 to 8)
- TODO Multilang: Take another pass on each language. Try to get comparable WERs to published scores.

Arabic: 
I found some encoding issues in the text files and dictionary.
I corrected them and I am going to run the system build again.
I am including the dev and eval text in the lm training text.
Just in case here are the WER scores before I start this:
%WER 77.57 [ 7015 / 9043, 349 ins, 1201 del, 5465 sub ] exp/mono/decode_dev/wer_16_0.0

%WER 71.79 [ 6492 / 9043, 685 ins, 580 del, 5227 sub ] exp/tri3b/decode_dev.si/wer_17_1.0

%WER 71.49 [ 6465 / 9043, 530 ins, 773 del, 5162 sub ] exp/tri1/decode_dev/wer_17_1.0

%WER 70.80 [ 6402 / 9043, 586 ins, 727 del, 5089 sub ] exp/tri2b/decode_dev/wer_17_1.0

%WER 70.73 [ 6396 / 9043, 679 ins, 571 del, 5146 sub ] exp/tri3b/decode_dev/wer_17_1.0

%WER 64.95 [ 5873 / 9043, 501 ins, 1325 del, 4047 sub ] exp/chain/tdnn1c_sp_online/decode_dev/wer_9_0.0

%WER 64.57 [ 5839 / 9043, 501 ins, 1325 del, 4013 sub ] exp/chain/tdnn1c_sp/decode_dev/wer_9_0.0

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 | 19.46 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.25      | 47.12 | 44.62 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
| Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

- TODO Writing.

- French: I just found out that the French lexicon is all lower case.
My training text is uppercase.

- Heroico 1b results:
%WER 76.91 [ 7087 / 9215, 680 ins, 1165 del, 5242 sub ] exp/chain/tdnn1b_sp/decode_nonnative/wer_8_1.0

%WER 76.22 [ 7024 / 9215, 811 ins, 1007 del, 5206 sub ] exp/chain/tdnn1b_sp_online/decode_nonnative/wer_7_1.0

%WER 75.78 [ 6983 / 9215, 1377 ins, 507 del, 5099 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0

%WER 74.25 [ 5680 / 7650, 1187 ins, 431 del, 4062 sub ] exp/tri3b/decode_devtest.si/wer_16_1.0

%WER 73.76 [ 12328 / 16713, 2541 ins, 804 del, 8983 sub ] exp/tri3b/decode_test.si/wer_17_1.0

%WER 71.87 [ 12012 / 16713, 1342 ins, 1746 del, 8924 sub ] exp/chain/tdnn1b_sp/decode_test/wer_7_1.0

%WER 71.86 [ 5497 / 7650, 530 ins, 959 del, 4008 sub ] exp/mono/decode_devtest/wer_7_1.0

%WER 71.64 [ 6602 / 9215, 646 ins, 939 del, 5017 sub ] exp/mono/decode_nonnative/wer_7_1.0

%WER 71.26 [ 5343 / 7498, 1159 ins, 293 del, 3891 sub ] exp/tri3b/decode_native.si/wer_17_1.0

%WER 71.09 [ 11882 / 16713, 1162 ins, 1999 del, 8721 sub ] exp/chain/tdnn1b_sp_online/decode_test/wer_8_1.0

%WER 69.59 [ 11630 / 16713, 1153 ins, 1643 del, 8834 sub ] exp/mono/decode_test/wer_7_1.0

%WER 67.09 [ 6182 / 9215, 907 ins, 626 del, 4649 sub ] exp/tri1/decode_nonnative/wer_14_1.0

%WER 66.98 [ 5022 / 7498, 503 ins, 700 del, 3819 sub ] exp/mono/decode_native/wer_7_1.0

%WER 66.78 [ 6154 / 9215, 1048 ins, 537 del, 4569 sub ] exp/tri2b/decode_nonnative/wer_15_1.0

%WER 66.64 [ 6141 / 9215, 1226 ins, 425 del, 4490 sub ] exp/tri3b/decode_nonnative/wer_16_1.0

%WER 66.33 [ 5074 / 7650, 921 ins, 481 del, 3672 sub ] exp/tri1/decode_devtest/wer_11_1.0

%WER 66.30 [ 5072 / 7650, 1198 ins, 328 del, 3546 sub ] exp/tri3b/decode_devtest/wer_11_1.0

%WER 65.88 [ 5040 / 7650, 985 ins, 450 del, 3605 sub ] exp/tri2b/decode_devtest/wer_13_1.0

%WER 65.63 [ 4921 / 7498, 551 ins, 741 del, 3629 sub ] exp/chain/tdnn1b_sp/decode_native/wer_7_1.0

%WER 65.05 [ 10872 / 16713, 1725 ins, 959 del, 8188 sub ] exp/tri1/decode_test/wer_13_1.0

%WER 64.90 [ 4866 / 7498, 543 ins, 739 del, 3584 sub ] exp/chain/tdnn1b_sp_online/decode_native/wer_7_1.0

%WER 64.45 [ 10772 / 16713, 2261 ins, 698 del, 7813 sub ] exp/tri3b/decode_test/wer_16_1.0

%WER 64.33 [ 10751 / 16713, 1955 ins, 845 del, 7951 sub ] exp/tri2b/decode_test/wer_14_1.0

%WER 62.54 [ 4689 / 7498, 781 ins, 379 del, 3529 sub ] exp/tri1/decode_native/wer_13_1.0

%WER 61.66 [ 4623 / 7498, 1038 ins, 267 del, 3318 sub ] exp/tri3b/decode_native/wer_15_1.0

%WER 61.28 [ 4595 / 7498, 899 ins, 309 del, 3387 sub ] exp/tri2b/decode_native/wer_13_1.0

%WER 54.00 [ 4131 / 7650, 731 ins, 389 del, 3011 sub ] exp/chain/tdnn1b_sp_online/decode_devtest/wer_7_1.0

%WER 53.62 [ 4102 / 7650, 720 ins, 383 del, 2999 sub ] exp/chain/tdnn1b_sp/decode_devtest/wer_7_1.0

1c:
%WER 71.45 [ 6584 / 9215, 645 ins, 1079 del, 4860 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_7_1.0

%WER 62.26 [ 4668 / 7498, 515 ins, 667 del, 3486 sub ] exp/chain/tdnn1c_sp/decode_native/wer_7_1.0

%WER 54.03 [ 4133 / 7650, 756 ins, 386 del, 2991 sub ] exp/chain/tdnn1c_sp/decode_devtest/wer_7_1.0

| fold | 1a | 1b | 1c |
| devtest | | 53.62 | 54.03 | 
| native | 64.76 | 65.63 | 62.26 | 
| nonnative | 73.85 | 76.91 | 71.45 |
| test | 69.84 | 71.87 | 67.32 |

The change made in experiment 1b definitly made the chain models worse.

1c improved WER scores with the  exception of devtest.

** Goals for After leave:
- TODO GlobalPhone: Make a pass through each language to check if text encoding (including casing) matches between text to train acoustic models, text to train lm and text in dictionary.
- TODO Heroico: Address Dan's comments ( try to get l2 regularization working)
- TODO Heroico: Modify layers (from 6 to 8)

- TODO Write paper.
- TODO African French: Work with Steve, Luis and Mike Li to get minimal Ultra example working.

* DAR <2017-11-15 Wed>
** Goals for Wednesday set Tuesday:
- DONE Heroico: Run ende to end and adress Dan's comments
I ran end2end once with the new data folds.
There is now a devtext fold.
I separated out the data in Heroico that was read from the same promps that were read at USMA.
- DONE Heroico: Write README
The README might need more work.
- DONE Heroico: Get chain model info 
I did this for the 1a run.
- DONE Heroico: Get WERs
Here are the 1a scores:
%WER 75.78 [ 6983 / 9215, 1377 ins, 507 del, 5099 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 74.25 [ 5680 / 7650, 1187 ins, 431 del, 4062 sub ] exp/tri3b/decode_devtest.si/wer_16_1.0
%WER 73.76 [ 12328 / 16713, 2541 ins, 804 del, 8983 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 71.86 [ 5497 / 7650, 530 ins, 959 del, 4008 sub ] exp/mono/decode_devtest/wer_7_1.0
%WER 71.64 [ 6602 / 9215, 646 ins, 939 del, 5017 sub ] exp/mono/decode_nonnative/wer_7_1.0
%WER 71.26 [ 5343 / 7498, 1159 ins, 293 del, 3891 sub ] exp/tri3b/decode_native.si/wer_17_1.0
%WER 69.59 [ 11630 / 16713, 1153 ins, 1643 del, 8834 sub ] exp/mono/decode_test/wer_7_1.0
%WER 67.09 [ 6182 / 9215, 907 ins, 626 del, 4649 sub ] exp/tri1/decode_nonnative/wer_14_1.0
%WER 66.98 [ 5022 / 7498, 503 ins, 700 del, 3819 sub ] exp/mono/decode_native/wer_7_1.0
%WER 66.78 [ 6154 / 9215, 1048 ins, 537 del, 4569 sub ] exp/tri2b/decode_nonnative/wer_15_1.0
%WER 66.64 [ 6141 / 9215, 1226 ins, 425 del, 4490 sub ] exp/tri3b/decode_nonnative/wer_16_1.0
%WER 66.33 [ 5074 / 7650, 921 ins, 481 del, 3672 sub ] exp/tri1/decode_devtest/wer_11_1.0
%WER 66.30 [ 5072 / 7650, 1198 ins, 328 del, 3546 sub ] exp/tri3b/decode_devtest/wer_11_1.0
%WER 65.88 [ 5040 / 7650, 985 ins, 450 del, 3605 sub ] exp/tri2b/decode_devtest/wer_13_1.0
%WER 65.05 [ 10872 / 16713, 1725 ins, 959 del, 8188 sub ] exp/tri1/decode_test/wer_13_1.0
%WER 64.45 [ 10772 / 16713, 2261 ins, 698 del, 7813 sub ] exp/tri3b/decode_test/wer_16_1.0
%WER 64.33 [ 10751 / 16713, 1955 ins, 845 del, 7951 sub ] exp/tri2b/decode_test/wer_14_1.0
%WER 62.54 [ 4689 / 7498, 781 ins, 379 del, 3529 sub ] exp/tri1/decode_native/wer_13_1.0
%WER 61.66 [ 4623 / 7498, 1038 ins, 267 del, 3318 sub ] exp/tri3b/decode_native/wer_15_1.0
%WER 61.28 [ 4595 / 7498, 899 ins, 309 del, 3387 sub ] exp/tri2b/decode_native/wer_13_1.0

- DONE Heroico: symbolic link to chain model script under tuning with 1a affix.
- DONE African French: Miniturize. Start with small LM.

I trained the lm only on the transcripts used for training the acoustic models plus the bic corpus. 
Here are the cd gmm hmm results:
%WER 80.38 [ 2565 / 3191, 108 ins, 722 del, 1735 sub ] exp/mono/decode_ca16/wer_8_0.0
%WER 62.93 [ 2008 / 3191, 117 ins, 556 del, 1335 sub ] exp/tri1/decode_ca16/wer_13_0.5
%WER 62.36 [ 1990 / 3191, 188 ins, 413 del, 1389 sub ] exp/tri3b/decode_ca16.si/wer_10_0.5
%WER 60.61 [ 1934 / 3191, 160 ins, 449 del, 1325 sub ] exp/tri2b/decode_ca16/wer_12_0.5
%WER 58.26 [ 1859 / 3191, 138 ins, 435 del, 1286 sub ] exp/tri3b/decode_ca16/wer_13_1.0

ls -sh exp/tri2b/graph/HCLG.fst 
11M exp/tri2b/graph/HCLG.fst

I am going to include the text from the test set in the training data for the lm.
My hypothesis is that this will make the WER scores go down.
I propose we make a "canned" demo that only works well on phrases from the CA16 test set.

- TODO SOFTunisia: Contact Zac. Where are we?
- TODO Multilang: Get global phone chain models running (latest version?)
- TODO Writing

** Goals for Thursday:
- TODO African French: Miniturize. Build up the lm from a minimal working example.
- TODO Heroico: Address Dan's comments.  remove proportional shrinking
- TODO Heroico: L2 regularization? Does it work with my current version of kaldi?
- TODO Heroico: Modify layers (from 6 to 8)
- TODO Multilang: Take another pass on each language. Try to get comparable WERs to published scores.
- TODO Writing.

* DAR <2017-11-14 Tue>
- Current WER scores:
| language | mono | tri1 | tri2b | tri3b| chain | chain online |
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 | 64.57 | 64.95 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      | 19.47 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.52 | 47.35 | 45.08 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese dev | 43.56 | 27.45 | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

** Goals for Wednesday:
- TODO Heroico: Run ende to end and adress Dan's comments
- TODO Heroico: Write README
- TODO Heroico: Get chain model info 
- TODO Heroico: Get WERs
- TODO Heroico: symbolic link to chain model script under tuning with 1a affix.
- TODO African French: Miniturize. Start with small LM.
- TODO SOFTunisia: Contact Zac. Where are we?
- TODO Multilang: Get global phone chain models running (latest version?)
- TODO Writing

* DAR <2017-11-13 Mon>
**  Goals set Last Week:
- TODO Multilang: build cd gmm hmm systems for all the GP languages (with reference lm).
- TODO Multilang: Build  chain models for each GP language (baselines?)
- TODO Multilang: Do multilang training?
- TODO Incorporate Government-owned corpora into multilang setup. ( WestPoint, ARL Urdu Pashto, Transtac Babel)
- TODO Babel: Search for data sampled at >= 16khz.

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 42.62      | 28.13      | 26.57      | 24.78      |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
Croatian dev | 36.53 | 30.60 | 29.19 | 28.53 |
| Czech dev | 57.44      | 53.88      | 50.83      | 43.72 |
| French dev | 95.06 | 93.35 | 93.51 | 93.41 |
| German dev | 49.52 | 47.35 | 45.08 | 38.04 |
| Hausa dev | 36.48 | 36.84 | 32.30 | 24.64 |
| Japanese eval | 15.18 | 9.01 | 8.73 | 7.77 |
Japanese dev | 10.40 | 6.54 | 6.25 | 6.15 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 36.63 | 23.51 | 22.54 | 19.07 |
| Polish dev | 65.87 | 57.63 | 53.05 | 48.23 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 60.12 | 49.38 | 46.04 | 42.97 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 50.71 | 40.63 | 38.94 | 37.49 |

* DAR <2017-11-09 Thu>
**  Goals for Thursday set Wednesday:
- TODO Multilang: Why do some languages not have dev sets?
- TODO Multilang Portuguese: What is wrong with GP Portuguese?
I think there are a lot of bad recordings.
I run the utils/fix_data_dir.sh script after doing plp_pitch feature extraction.
This finds around 3k bad files and it makes lists with only the good files.
It also exits with an error status.
I ignore this error status. 
- TODO Multilang: Russian: What is wrong with Russian?
- TODO Multilang: Tamil: What is wrong with GP Tamil?
- TODO Multilang: Thai: What is wrong with GP Thai?
- TODO SOFTunisia: Run the previous stage with the new lexicon.
- TODO Multilang: Fix feature extraction for nnet3 alignment

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 49.37 | 28.13 | 32.38 | 30.98 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Czech dev | 69.95 | 66.71 | 65.96 | 65.60 |
| French | | | | |
| German dev | 68.19 | 57.64 | 56.59 | 54.63 |
| Hausa dev | 36.48 | | |
| Hausa eval | 49.81 | 44.17 | 40.87      | 29.02 |
| Japanese eval | 41.73 | 26.38 | 25.17 | 23.01 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 48.84 | 33.99 | 32.55 | 28.35 |
| Polish dev | 73.33 | 66.21 | 62.52 | 58.56 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 69.30 | 57.16 | 55.50 | 53.56 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 95.62 | 84.70 | 83.90 | 80.29 |

* DAR <2017-11-08 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Mandatory Training (NDA)
- TODO Read more of Thang disertation.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).
Feature vectors are extracted from the acoustic data in several ways:
1. plp and pitch features to train and test the cd gmm hmm models.
2.  low-resolution plp pitch features to get alignments to do chain models
3. high resolution mfcc features to train and test the ivector extractor.
4. ? for training testing the chain models.

The feature vectors extracted in 2. are done in 3 ways by speed perturbing the data.

- DONE Multilang: Incorporate reference LMs.
LMs for Arabic and Turkish are missing.

- Current WER scores:
| language | mono | tri1 | tri2b | tri3b|
| Arabic dev | 77.57 | 71.49       | 70.80 | 70.73 |
| Bulgarian dev | 49.37 | 33.71 | 32.38 | 30.98 |
| Croatian eval | 66.30 | 56.89 | 56.55 | 53.65 |
| Czech dev | 69.95 | 66.71 | 65.96 | 65.60 |
| French | | | | |
| German dev | 68.19 | 57.64 | 56.59 | 54.63 |
| Hausa eval | 49.81 | 44.17 | 41.45 | 33.03 |
| Japanese eval | 41.73 | 26.38 | 25.17 | 23.01 |
| Korean dev | 51.61 | 30.79 | 29.71 | 25.64 |
| Mandarin dev | 48.84 | 33.99 | 32.55 | 28.35 |
| Polish dev | 73.33 | 66.21 | 62.52 | 58.56 |
| Portuguese | | | | |
| Russian dev | 97.56 | | | |
| Spanish dev | 69.30 | 57.16 | 55.50 | 53.56 |
| Swedish dev | 80.77 | 66.17 | 64.39 | 62.07 |
| Tamil eval | 100.00 | | | |
| Thai dev | 101.40 | | | 
| Turkish dev | 79.76 | 75.65 | 74.97 | 75.25 |
| Vietnamese dev | 95.62 | 84.70 | 83.90 | 80.29 |

- SOFTunisia: Zac gave me the new lexicon.
He wants me to rerun the last stage with the new lexicon so we can compare results.
I'm not sure where the latest batch starts.

** Goals for Thursday:
- TODO Multilang: Why do some languages not have dev sets?
- TODO Multilang Portuguese: What is wrong with GP Portuguese?
- TODO Multilang: Russian: What is wrong with Russian?
- TODO Multilang: Tamil: What is wrong with GP Tamil?
- TODO Multilang: Thai: What is wrong with GP Thai?
- TODO SOFTunisia: Run the previous stage with the new lexicon.
- TODO Multilang: Fix feature extraction for nnet3 alignment

* DAR <2017-11-07 Tue>
**  Goals for Tuesday set Monday:
- TODO Mandatory Training (NDA)
- DONE Read chapter 4 of Thang disertation.
-TODO Multilang: Expand tabs to white space in all dictionaries (start from tamil). 
problems with thai.
There is a bad character somewhere.

- TODO Multilang: convert tab to space in <UNK> entry.
- DONE Multilang: make sure all files are in UTF8 (start from tamil).
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.
Korean:
%WER 51.61 [ 18946 / 36707, 488 ins, 2286 del, 16172 sub ] exp/mono/decode_dev/wer_8_0.0
%WER 47.52 [ 18495 / 38920, 374 ins, 3631 del, 14490 sub ] exp/mono/decode_eval/wer_9_0.0
%WER 30.79 [ 11303 / 36707, 543 ins, 1005 del, 9755 sub ] exp/tri1/decode_dev/wer_13_0.5
%WER 29.71 [ 10907 / 36707, 511 ins, 1029 del, 9367 sub ] exp/tri2b/decode_dev/wer_13_0.5
%WER 29.62 [ 10874 / 36707, 522 ins, 781 del, 9571 sub ] exp/tri3b/decode_dev.si/wer_10_1.0
%WER 25.64 [ 9412 / 36707, 474 ins, 670 del, 8268 sub ] exp/tri3b/decode_dev/wer_13_0.5
%WER 14.28 [ 5559 / 38920, 437 ins, 709 del, 4413 sub ] exp/tri3b/decode_eval.si/wer_10_0.0
%WER 11.84 [ 4610 / 38920, 356 ins, 644 del, 3610 sub ] exp/tri2b/decode_eval/wer_12_0.0
%WER 11.83 [ 4604 / 38920, 398 ins, 623 del, 3583 sub ] exp/tri1/decode_eval/wer_11_0.0
%WER 10.96 [ 4265 / 38920, 391 ins, 569 del, 3305 sub ] exp/tri3b/decode_eval/wer_12_0.0

Mandarin:
%WER 52.13 [ 11209 / 21502, 686 ins, 2250 del, 8273 sub ] exp/mono/decode_eval/wer_12_1.0
%WER 48.84 [ 8925 / 18274, 593 ins, 1547 del, 6785 sub ] exp/mono/decode_dev/wer_11_1.0
%WER 38.76 [ 8334 / 21502, 894 ins, 1434 del, 6006 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 37.84 [ 8137 / 21502, 963 ins, 1262 del, 5912 sub ] exp/tri3b/decode_eval.si/wer_14_1.0
%WER 36.84 [ 7921 / 21502, 826 ins, 1374 del, 5721 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 33.99 [ 6211 / 18274, 692 ins, 1053 del, 4466 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 33.44 [ 6110 / 18274, 769 ins, 893 del, 4448 sub ] exp/tri3b/decode_dev.si/wer_14_1.0
%WER 33.24 [ 7147 / 21502, 881 ins, 1197 del, 5069 sub ] exp/tri3b/decode_eval/wer_16_1.0
%WER 32.55 [ 5949 / 18274, 707 ins, 945 del, 4297 sub ] exp/tri2b/decode_dev/wer_15_1.0
%WER 28.35 [ 5180 / 18274, 821 ins, 723 del, 3636 sub ] exp/tri3b/decode_dev/wer_14_1.0

Polish:
%WER 73.89 [ 11214 / 15176, 706 ins, 2922 del, 7586 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 73.33 [ 13183 / 17977, 628 ins, 4142 del, 8413 sub ] exp/mono/decode_dev/wer_12_0.0
%WER 66.83 [ 10142 / 15176, 713 ins, 3754 del, 5675 sub ] exp/tri1/decode_eval/wer_13_0.0
%WER 66.21 [ 11903 / 17977, 981 ins, 3977 del, 6945 sub ] exp/tri1/decode_dev/wer_12_0.0
%WER 63.62 [ 9655 / 15176, 833 ins, 2940 del, 5882 sub ] exp/tri2b/decode_eval/wer_13_0.5
%WER 62.52 [ 11240 / 17977, 1032 ins, 3366 del, 6842 sub ] exp/tri2b/decode_dev/wer_13_0.0
%WER 62.09 [ 9423 / 15176, 745 ins, 2771 del, 5907 sub ] exp/tri3b/decode_eval.si/wer_15_0.0
%WER 61.56 [ 9342 / 15176, 878 ins, 2603 del, 5861 sub ] exp/tri3b/decode_eval/wer_16_0.5
%WER 60.30 [ 10840 / 17977, 809 ins, 3170 del, 6861 sub ] exp/tri3b/decode_dev.si/wer_14_0.0
%WER 58.56 [ 10527 / 17977, 940 ins, 2922 del, 6665 sub ] exp/tri3b/decode_dev/wer_17_0.5

Spanish:
%WER 69.30 [ 13237 / 19101, 753 ins, 2829 del, 9655 sub ] exp/mono/decode_dev/wer_10_0.0
%WER 59.76 [ 7417 / 12411, 585 ins, 1444 del, 5388 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 57.16 [ 10918 / 19101, 949 ins, 2412 del, 7557 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 56.72 [ 10835 / 19101, 1168 ins, 1812 del, 7855 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 55.50 [ 10601 / 19101, 1084 ins, 1998 del, 7519 sub ] exp/tri2b/decode_dev/wer_16_1.0
%WER 53.56 [ 10231 / 19101, 1299 ins, 1500 del, 7432 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 49.49 [ 6142 / 12411, 1042 ins, 711 del, 4389 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 48.80 [ 6056 / 12411, 802 ins, 981 del, 4273 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 48.09 [ 5969 / 12411, 943 ins, 763 del, 4263 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 48.09 [ 5969 / 12411, 1148 ins, 584 del, 4237 sub ] exp/tri3b/decode_eval/wer_17_1.0

- TODO Multilang: Figure out why chain model training fails (speed perturbed data).

** Goals for Wednesday:
- TODO Mandatory Training (NDA)
- TODO Read more of Thang disertation.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).
- TODO Multilang: Incorporate reference LMs.

* DAR <2017-11-06 Mon>
**  Goals for Next Week:
-TODO Multilang: Expand tabs to white space in all dictionaries.
I worked a lot on this today.
I am working in alphabetical order.
I finished up through gp_spanish.

I should go back and fix the <UNK> entry, it still has a tab.
- TODO Multilang: make sure all files are in UTF8 (or ascii).
Same as above, I finished up through gp_spanish.
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.

Here is an update on WER scores:
Arabic:
%WER 77.57 [ 7015 / 9043, 349 ins, 1201 del, 5465 sub ] exp/mono/decode_dev/wer_16_0.0
%WER 73.09 [ 12048 / 16484, 598 ins, 1511 del, 9939 sub ] exp/mono/decode_eval/wer_13_0.5
%WER 72.07 [ 6517 / 9043, 731 ins, 608 del, 5178 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 71.68 [ 6482 / 9043, 572 ins, 802 del, 5108 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 71.03 [ 6423 / 9043, 746 ins, 574 del, 5103 sub ] exp/tri3b/decode_dev/wer_17_1.0
%WER 70.64 [ 6388 / 9043, 590 ins, 780 del, 5018 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 66.68 [ 10991 / 16484, 1281 ins, 720 del, 8990 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 66.11 [ 10898 / 16484, 1073 ins, 955 del, 8870 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 65.63 [ 10819 / 16484, 1059 ins, 996 del, 8764 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 65.47 [ 10792 / 16484, 1370 ins, 547 del, 8875 sub ] exp/tri3b/decode_eval/wer_17_1.0

Bulgarian:
%WER 52.68 [ 7312 / 13881, 541 ins, 1575 del, 5196 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 49.37 [ 7464 / 15118, 680 ins, 1425 del, 5359 sub ] exp/mono/decode_dev/wer_11_0.0
%WER 37.98 [ 5272 / 13881, 913 ins, 761 del, 3598 sub ] exp/tri3b/decode_eval.si/wer_16_1.0
%WER 37.82 [ 5250 / 13881, 917 ins, 707 del, 3626 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 36.36 [ 5047 / 13881, 898 ins, 695 del, 3454 sub ] exp/tri2b/decode_eval/wer_17_0.5
%WER 34.37 [ 4771 / 13881, 969 ins, 608 del, 3194 sub ] exp/tri3b/decode_eval/wer_16_1.0
%WER 33.71 [ 5097 / 15118, 887 ins, 693 del, 3517 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 33.31 [ 5036 / 15118, 1005 ins, 625 del, 3406 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 32.38 [ 4895 / 15118, 903 ins, 682 del, 3310 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 30.98 [ 4683 / 15118, 1034 ins, 536 del, 3113 sub ] exp/tri3b/decode_dev/wer_17_1.0

Croatian:
%WER 66.30 [ 5657 / 8533, 380 ins, 1006 del, 4271 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 57.38 [ 4896 / 8533, 484 ins, 999 del, 3413 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 56.89 [ 4854 / 8533, 366 ins, 1294 del, 3194 sub ] exp/tri1/decode_eval/wer_17_0.5
%WER 56.55 [ 4825 / 8533, 382 ins, 1306 del, 3137 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 53.65 [ 4578 / 8533, 558 ins, 725 del, 3295 sub ] exp/tri3b/decode_eval/wer_17_1.0

Czech:
%WER 72.32 [ 8565 / 11844, 404 ins, 1743 del, 6418 sub ] exp/mono/decode_eval/wer_11_1.0
%WER 69.95 [ 6312 / 9024, 276 ins, 1516 del, 4520 sub ] exp/mono/decode_dev/wer_14_0.5
%WER 69.30 [ 8208 / 11844, 1641 ins, 520 del, 6047 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 68.92 [ 6219 / 9024, 1344 ins, 541 del, 4334 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 68.10 [ 8066 / 11844, 761 ins, 2364 del, 4941 sub ] exp/tri1/decode_eval/wer_17_1.0
%WER 67.11 [ 7949 / 11844, 1658 ins, 767 del, 5524 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 66.77 [ 7908 / 11844, 956 ins, 1951 del, 5001 sub ] exp/tri2b/decode_eval/wer_17_1.0
%WER 66.71 [ 6020 / 9024, 596 ins, 1774 del, 3650 sub ] exp/tri1/decode_dev/wer_17_1.0
%WER 65.96 [ 5952 / 9024, 793 ins, 1593 del, 3566 sub ] exp/tri2b/decode_dev/wer_17_1.0
%WER 65.60 [ 5920 / 9024, 1381 ins, 642 del, 3897 sub ] exp/tri3b/decode_dev/wer_17_1.0

German:
%WER 77.09 [ 9219 / 11959, 519 ins, 1990 del, 6710 sub ] exp/mono/decode_eval/wer_13_1.0
%WER 68.19 [ 10492 / 15387, 799 ins, 2876 del, 6817 sub ] exp/mono/decode_dev/wer_14_0.5
%WER 63.17 [ 7554 / 11959, 1142 ins, 1587 del, 4825 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 61.66 [ 7374 / 11959, 525 ins, 2845 del, 4004 sub ] exp/tri1/decode_eval/wer_16_1.0
%WER 61.16 [ 7314 / 11959, 544 ins, 2704 del, 4066 sub ] exp/tri2b/decode_eval/wer_16_1.0
%WER 60.28 [ 7209 / 11959, 1309 ins, 1127 del, 4773 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 57.64 [ 8869 / 15387, 693 ins, 4022 del, 4154 sub ] exp/tri1/decode_dev/wer_15_0.0
%WER 56.59 [ 8707 / 15387, 824 ins, 3682 del, 4201 sub ] exp/tri2b/decode_dev/wer_15_0.0
%WER 56.07 [ 8628 / 15387, 1150 ins, 2603 del, 4875 sub ] exp/tri3b/decode_dev.si/wer_17_1.0
%WER 54.63 [ 8406 / 15387, 1852 ins, 1230 del, 5324 sub ] exp/tri3b/decode_dev/wer_17_1.0

Hausa:
%WER 49.81 [ 769 / 1544, 37 ins, 164 del, 568 sub ] exp/mono/decode_eval/wer_11_0.5
%WER 44.17 [ 682 / 1544, 52 ins, 242 del, 388 sub ] exp/tri1/decode_eval/wer_13_0.0
%WER 41.45 [ 640 / 1544, 55 ins, 225 del, 360 sub ] exp/tri2b/decode_eval/wer_15_0.0
%WER 35.56 [ 549 / 1544, 85 ins, 77 del, 387 sub ] exp/tri3b/decode_eval.si/wer_17_0.5
%WER 33.03 [ 510 / 1544, 53 ins, 130 del, 327 sub ] exp/tri3b/decode_eval/wer_17_1.0

Japanese:
%WER 41.73 [ 7476 / 17915, 961 ins, 1342 del, 5173 sub ] exp/mono/decode_eval/wer_10_0.0
%WER 27.67 [ 4957 / 17915, 880 ins, 688 del, 3389 sub ] exp/tri3b/decode_eval.si/wer_17_0.5
%WER 26.38 [ 4726 / 17915, 857 ins, 683 del, 3186 sub ] exp/tri1/decode_eval/wer_16_0.5
%WER 25.17 [ 4509 / 17915, 864 ins, 612 del, 3033 sub ] exp/tri2b/decode_eval/wer_15_0.5
%WER 23.01 [ 4123 / 17915, 829 ins, 588 del, 2706 sub ] exp/tri3b/decode_eval/wer_14_1.0

- TODO Multilang: Run chain model training for all languages (this will help down the line).
I am working on French.
It looks like the problem is with the speed perturbed data.
I think it requires matrices with more rows or columns that for some reason do not exist.
Either I do not use sp data or I figure out how to get the larger matrices.
I can get things to run without the sp data, but I do not think this is what I want.

** Goals for Tuesday:
- TODO Mandatory Training (NDA)
- TODO Read chapter 4 of Thang disertation.
-TODO Multilang: Expand tabs to white space in all dictionaries (start from tamil). 
- TODO Multilang: convert tab to space in <UNK> entry.
- TODO Multilang: make sure all files are in UTF8 (start from tamil).
- TODO Multilang: Incorporate reference LMs.
- TODO Multilang: Train CD GMM HMM systems for all languages.
- TODO Multilang: Figure out why chain model training fails (speed perturbed data).

* DAR <2017-11-03 Fri>
**  Goals for Thursday set Wednesday:
- DONE Workshop. (should take up the whole day)
I thinkthe the workshop was very successful.

* DAR <2017-11-01 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Setup the gp_french and incorporate it into multilang.
I am starting the multilang over again.
I got to the point where the multilang recipe was going to train the ivector extractor.
All the data was pooled.
At this point the utt2spk file failed.
Most of the GP corpora use speaker names like 001.
I have to make them distinct across languages.
So, for example, I'll label them as AR001 and SP001 for Arabic and Spanish respectively.
I also plan on starting small, maybe with 3 languages.
I am starting with Arabic, Bulgarian , Croatian and French. Maybe Turkish too.
Later I'll start over again using the GP LMs, for now I am building the LMS on the training text.

- TODO Study the multilang recipe.
I am reading the Disertation by Ngoc Thang Vu.
It looks like my project this year will consist of replicating some of the work in this disertation and then improving on it with chain models.
The disertation uses DNNS.
My experience with DNNs was a little disappointing.
I think Chain models should do better than the  DNNs. 
- TODO Writing.
The Ngoc Thang Vu disertation is good background for anything we will write about.

** Goals for Thursday:
- TODO Workshop. (should take up the whole day)

* DAR <2017-10-31 Tue>
** Goals for Tuesday set Monday:
- TODO Multilang: Investigate recipe.
My current understanding is that the multilang recipe is going to make a nnet3 model on all the data I feed it, which right now comes from 8 languages.
Then the target language data is used to train/retrain the last layer of the neural net.

- TODO Writing.
Steve and I discussed a mind map for the paper.
I think we can write about the chain model versus tri3b results:
Look at the first 3 columns of the table below.
tri3b on gp: 35.85
Chain model on GP: 51.27
Add 3 hours of Gabon Read: 
tri3b with 3h of babon read: 19.84
chain with 3h of gabon read: 14.78

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 44.59 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 45.28 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 35.85 | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | 52.40 | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | 51.27 | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |


** Goals for Wednesday:
- TODO Setup the gp_french and incorporate it into multilang.
- TODO Study the multilang recipe.
- TODO Writing.

* DAR <2017-10-30 Mon>
** Goals for Monday:
- DONE Check Mandarin dictionary normalization.
The list of phones looks good.
** Goals for Next Week:
- DONE Multilang: Finish dictionary work for all languages.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok
hausa: ok
Japanese: ok
Korean: ok
Mandarin: ok
Polish: ok
Portuguese: ok
Russian: ok
Spanish: ok
Swedish: ok
Tamil: There are backslashes
Thai: ok
Turkish: ok
Vietnamese: ok

- TODO Multilang: Train cd gmm hmm systems for each language.
Arabic: Started
Bulgarian: Started
Croatian: Started
Czech: DONE
German: Done
Hausa: Done
Japanese: Started
Korean: Started
Mandarin: Started
Polish: Started 
Portuguese: Problems with the folds. There are files that don't really have usable data  in them . They are very small.
Russian: Started
Spanish: Done
Swedish: Done
Tamil: Done
Thai: Started
Turkish: Done
Vietnamese: Started

- Polish mfcc:
%WER 71.43 [ 10840 / 15176, 838 ins, 2521 del, 7481 sub ] exp/mono/decode_eval/wer_11_0.0
%WER 64.05 [ 9720 / 15176, 905 ins, 3022 del, 5793 sub ] exp/tri1/decode_eval/wer_13_0.0

- TODO Workshop: (Thursday).
- TODO Writing.



- Multilang recipe:
I started running the multilang recipe.
I am only using the languages that I have built cd gmm hmm systems for so far.
Arabic
Czech
German
Hausa
Spanish
Swedish
Turkish


I had to link the data/train and exp/tri3b_ali directories to the local working directory.
Now I am extracting high resolution mfcc features (and pitch?).

** Goals for Tuesday:
- TODO Multilang: Investigate recipe.
- TODO Writing.

* DAR <2017-10-27 Fri>
** Goals for Friday set Thursday:
- TODO Multilang: Continue checking dictionaries.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok
hausa: ok
Japanese ok
Korean: ok

- TODO Multilang: Get monophone results for each language.

| language | hours | monoWER |
| Arabic | 15.3 | 99.91 |
| Bulgarian | 17.1 | 100.00 |
| Croatian | 7.7 | 77.30 |
| Czech | 16.0 | 88.96 |
| German | 14.8 | 81.46 |
| Hausa| 4.8 | 48.38 |
| Japanese | 28.8 | 100.00 |
| Korean | 18.9 | 100.00 |
| Mandarin | 26.6 | 103.04 |
| Polish | 18.2 | 71.43 |
| Portuguese | 16.0 | 100.0 |
| Russian | 20.9 | 99.89 |
| Spanish | 17.5 | 60.20 |
| Swedish | 17.4 | 81.69 |
| Turkish | 13.2 | 82.91 |
| Vietnamese | 13.6 | 97.80 |

- Swedish MFCC: 
%WER 81.69 [ 14830 / 18154, 826 ins, 3532 del, 10472 sub ] exp/mono/decode_eval/wer_9_1.0
%WER 71.25 [ 12935 / 18154, 1753 ins, 2091 del, 9091 sub ] exp/tri3b/decode_eval.si/wer_17_1.0
%WER 69.66 [ 12646 / 18154, 1892 ins, 1931 del, 8823 sub ] exp/tri3b/decode_eval/wer_17_1.0

-Turkish:
%WER 82.91 [ 10400 / 12543, 215 ins, 2685 del, 7500 sub ] exp/mono/decode_eval/wer_10_1.0


- TODO Writing

** Goals for Monday:
- TODO Check Mandarin dictionary normalization.
* DAR <2017-10-26 Thu>
** Goals for Thursday set Wednesday:
- TODO Multilang: Setup all languages to train on plp pitch (start tomorrow with Japanese).
- TODO Multilang: Go through each language and check the state of the dictionary and try to correct problems.
Arabic: ok
Bulgarian: ok
Croatian: ok
Czech: ok
German: ok

- TODO Writing.
- TODO Multilang: What is the next step?


- Bulgarian:
17.1 hours of speech.
I fixed some issues with the dictionary normalization script.
This should work better now.
- Croatian:
7.7 hours of data
%WER 77.30 [ 6596 / 8533, 264 ins, 1480 del, 4852 sub ] exp/mono/decode_eval/wer_13_0.5
This maybe as good as this will get for such a small corpus.
-Czech:
16.0 hours of data
- German:
14.8 hours of data
%WER 81.46 [ 3664 / 4498, 249 ins, 669 del, 2746 sub ] [PARTIAL] exp/mono/decode_eval/wer_13_0.5
I think this should be doing better than this.
- Hausa:
4.8 hours of data

** Goals for Friday:
- TODO Multilang: Continue checking dictionaries.
- TODO Multilang: Get monophone results for each language.
- TODO Writing

* DAR <2017-10-25 Wed>
** Goals for Wednesday set Tuesday:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Thai
Tamil 
tamil does not have a globalphone dictionary.
Babel has a tamil dictionary.
Turkish
Vietnamese
wuu
Wuu has no dictionary

- Swedish: 
17.4 hours of data

I started looking at the multilang recipe.
It suggests using plp + pitch features for each language.
I started doing this for each file.
I copied the plp pitch configuration files to each directory.
I also fixed some problems with dictionaries. 
I am currently working on Japanese.

- TODO African French: Build with some more combinations of data.
- TODO African French: Get hours of speech by running monophone training, alignment and testing for data sets that are missing hours.
- TODO Start writing paper.

** Goals for Thursday:
- TODO Multilang: Setup all languages to train on plp pitch (start tomorrow with Japanese).
- TODO Multilang: Go through each language and check the state of the dictionary and try to correct problems.
- TODO Writing.
- TODO Multilang: What is the next step?
 
* DAR <2017-10-24 Tue>
** Goals set Last Week:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Swedish (This needs to be worked on first. I think it needs a dictionary normalization script.)
Thai
Tamil
Turkish
Vietnamese
wu

I got a lot done on Swedish today.

- DONE African French: Get results for GP aloen and other training sequences.

%WER 53.90 [ 1720 / 3191, 125 ins, 386 del, 1209 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 52.40 [ 1672 / 3191, 159 ins, 443 del, 1070 sub ] exp/chain/tdnn_sp/decode_ca16/wer_17_0.5
%WER 51.27 [ 1636 / 3191, 165 ins, 396 del, 1075 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_17_0.5
%WER 48.82 [ 1558 / 3191, 189 ins, 401 del, 968 sub ] exp/tri3b/decode_ca16.si/wer_17_0.5
%WER 45.28 [ 1445 / 3191, 176 ins, 390 del, 879 sub ] exp/tri2b/decode_ca16/wer_17_0.0
%WER 44.59 [ 1423 / 3191, 220 ins, 262 del, 941 sub ] exp/tri1/decode_ca16/wer_17_0.0
%WER 35.85 [ 1144 / 3191, 162 ins, 254 del, 728 sub ] exp/tri3b/decode_ca16/wer_17_1.0

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 44.59 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 45.28 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 35.85 | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | 52.40 | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | 51.27 | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

** Goals for Wednesday:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Thai
Tamil
Turkish
Vietnamese
wu
- TODO African French: Build with some more combinations of data.
- TODO African French: Get hours of speech by running monophone training, alignment and testing for data sets that are missing hours.
- TODO Start wrigin paper.
* DAR <2017-10-19 Thu>
** Goals for Thursday set Wednesday:
- DONE Multilang: Arabic training and evaluation.
15.35 hours of training data.

I got a lot done on this goal today.
I setup the basic recipe for 13 of the 18 languages:
Arabic
Bulgarian
Croatian
Czech
German
Hausa
Japanese
Korean
Mandarin
Polish
Portuguese
Russian
Spanish
- DONE Multilang: Bulgarian?
- TODO African French: What do we get when we only run on gp?

I am taking Friday and Monday off.

** Goals for Next Week:
- TODO Setup kaldi recipes for the remaining gp languages:
French ?
Swedish (This needs to be worked on first. I think it needs a dictionary normalization script.)
Thai
Tamil
Turkish
Vietnamese
u

- TODO African French: Get results for GP aloen and other training sequences.

* DAR <2017-10-18 Wed>
** Goals for Wednesday set Tuesday:
- DONE Multilang: Make my own data prep scripts for Arabic.
I made a lot of progress on this goal today.
I have scripts that prepare the data and write the lists for acoustic model training and testing.
I train an LM on the training text.
I am using the dictionary supplied by the Globalphone corpus.
Monophone training is running, but I don not know yet if decoding will work.
- DONE African French: Get current results.
%WER 45.28 [ 1445 / 3191, 107 ins, 337 del, 1001 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 27.08 [ 864 / 3191, 144 ins, 162 del, 558 sub ] exp/tri3b/decode_ca16.si/wer_16_1.0
%WER 25.51 [ 814 / 3191, 157 ins, 129 del, 528 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 21.87 [ 698 / 3191, 142 ins, 93 del, 463 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 19.84 [ 633 / 3191, 133 ins, 91 del, 409 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 14.95 [ 477 / 3191, 75 ins, 89 del, 313 sub ] exp/chain/tdnn_sp/decode_ca16/wer_12_0.0
%WER 14.79 [ 472 / 3191, 72 ins, 94 del, 306 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_13_0.0

| model | WER gp 22.7hours | gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 53.90 | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 47.23 | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 14.95 | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 14.79 | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

- TODO African French: Get results when only Training on gp (I probably have these results already, but get them anyway just for completeness).

** Goals for Thursday:
- TODO Multilang: Arabic training and evaluation.
- TODO Multilang: Bulgarian?
- TODO African French: What do we get when we only run on gp?

* DAR <2017-10-17 Tue>
** Goals for Tuesday set Monday:
- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
- TODO African French: Get chain model results and move on to next step by removing more data.
%WER 43.94 [ 1402 / 3191, 117 ins, 288 del, 997 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 23.72 [ 757 / 3191, 154 ins, 100 del, 503 sub ] exp/tri1/decode_ca16/wer_15_0.0
%WER 22.56 [ 720 / 3191, 104 ins, 165 del, 451 sub ] exp/tri2b/decode_ca16/wer_17_1.0
%WER 19.08 [ 609 / 3191, 121 ins, 80 del, 408 sub ] exp/tri3b/decode_ca16/wer_17_0.5
%WER 14.10 [ 450 / 3191, 78 ins, 66 del, 306 sub ] exp/chain/tdnn_sp/decode_ca16/wer_11_0.0
%WER 13.88 [ 443 / 3191, 69 ins, 73 del, 301 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_11_0.5

| model | WER gabonread gp 25.6 hours | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 45.28 | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 25.51 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 21.87 | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 19.84 | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 14.10 | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 13.88 | 12.28 | 12.85 | 11.69 | 12.60 |

%WER 45.28 [ 1445 / 3191, 107 ins, 337 del, 1001 sub ] exp/mono/decode_ca16/wer_14_0.0
%WER 27.08 [ 864 / 3191, 144 ins, 162 del, 558 sub ] exp/tri3b/decode_ca16.si/wer_16_1.0
%WER 25.51 [ 814 / 3191, 157 ins, 129 del, 528 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 21.87 [ 698 / 3191, 142 ins, 93 del, 463 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 19.84 [ 633 / 3191, 133 ins, 91 del, 409 sub ] exp/tri3b/decode_ca16/wer_16_0.5

The chain model results are not ready yet.
- TODO Heroico: Maybe start from beginning since scripts are not moving forward and they die on pca transform estimation.
%WER 23.33 [ 2150 / 9215, 162 ins, 373 del, 1615 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 20.46 [ 3420 / 16713, 288 ins, 533 del, 2599 sub ] exp/mono/decode_test/wer_8_0.0
%WER 18.42 [ 1697 / 9215, 233 ins, 193 del, 1271 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 16.67 [ 1250 / 7498, 123 ins, 181 del, 946 sub ] exp/mono/decode_native/wer_7_0.0
%WER 15.72 [ 1449 / 9215, 147 ins, 224 del, 1078 sub ] exp/tri1/decode_nonnative/wer_17_0.5
%WER 14.60 [ 1345 / 9215, 222 ins, 135 del, 988 sub ] exp/tri2b/decode_nonnative/wer_17_0.0
%WER 14.36 [ 2400 / 16713, 318 ins, 306 del, 1776 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 13.01 [ 2175 / 16713, 230 ins, 320 del, 1625 sub ] exp/tri1/decode_test/wer_16_0.5
%WER 11.70 [ 1078 / 9215, 128 ins, 137 del, 813 sub ] exp/tri3b/decode_nonnative/wer_17_1.0
%WER 11.63 [ 1944 / 16713, 292 ins, 211 del, 1441 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 9.59 [ 719 / 7498, 78 ins, 100 del, 541 sub ] exp/tri1/decode_native/wer_15_0.5
%WER 9.20 [ 690 / 7498, 123 ins, 76 del, 491 sub ] exp/tri3b/decode_native.si/wer_14_0.0
%WER 9.02 [ 1507 / 16713, 192 ins, 166 del, 1149 sub ] exp/tri3b/decode_test/wer_16_0.5
%WER 7.79 [ 584 / 7498, 80 ins, 63 del, 441 sub ] exp/tri2b/decode_native/wer_14_0.0
%WER 5.49 [ 412 / 7498, 34 ins, 52 del, 326 sub ] exp/tri3b/decode_native/wer_14_0.5

The run failed again on the ubm training step.
I enables this line in my path.sh file:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64

Maybe this is what was missing all this time?
No. It failed again :(

** Goals for Wednesday:
- TODO Multilang: Make my own data prep scripts for Arabic.
- TODO African French: Get current results.
- TODO African French: Get results when only Training on gp (I probably have these results already, but get them anyway just for completeness).

* DAR <2017-10-16 Mon>
** Goals set Last Week:
- DONE Objectives (Monday) 
(1) TECHNICAL Objectives (Weight 30)
A. Acoustic Models for Low Resource Languages
Adapt the Kaldi multilang recipe to build acoustic models for a target low resource language given resources from many other source languages. 

Specific Rating tasks:
Modify the Kaldi multilang recipe from its original keyword spotting task to the Speech to Speech (S2S) task.
Build 18 ASR systems from source language resources in the GlobalPhone corpus and government owned corpora (see corpus curation task below).
Setup experiment to evaluate effectiveness of the multilang approach. 

B. Corpus Curation
Curate four government owned speech corpora.

Specific Rating Tasks:
Prepare Arabic, French, German and Russian speech data for use in the multilang project listed above.
Write Kaldi recipes for each language corpus.
Submit recipes for publication in Kaldi repository.
Publish data, lexicons and recipes in ARL NSRL repository.

C. Speech to Speech Technology
Investigate S2S hardware restrictions and software solutions with the goal of contributing optimized components. 

Specific Rating Tasks:
Study methods for performing online or real time ASR processing and produce ASR components that are optimized to work with these methods. 
Study methods for integrating ASR and MT components in S2S applications and tailor our products to conform to these methods.
Study methods for making S2S ASR highly responsive and accurate and use results of investigations to guide our choices of models and algorithms. 



(2) COOPERATION (Weight 10)

A. Cooperate with colleagues.

Specific Rating Tasks:
Collaborate with Steve LaRocca in first quarter to write papers that report on advances made in our projects. 
Collaborate with the Basic Research team and CERDEC by contributing speech recognition components to Human Robot communication efforts. 

(3) COMMUNICATIONS (Weight 30)

A. Publish papers and reports

Specific Rating Tasks:
Write a TR with Steve LaRocca in the first quarter documenting projects. 
Write journal paper with Steve LaRocca that reports on multilang project results.

B. Activity Reports
Write weekly reports to help guide research and to recored progress .

C. Establish Professional Communication Channels with Scientists contributing to Kaldi project.

Specific Rating Tasks:
Contribute algorithm to Kaldi

(4) MGMT. OF TIME & RESOURCES (Weight 15)

A. Curate and archive our own valuable speech and text corpora on our branch storage disks. 


Specific Rating Tasks:
Format the data so that the corpora that can be made publically available are ready to be transfered. 
Organize the data so that it is easy to access from recipes running on connected branch machines.
Stay abreast of possible areas where hardware upgrades could improve work efficiency. 

(5) CUSTOMER RELATIONS (Weight 15)

Establish relationships with MFLTS and CERDEC to remain aware of Army requirements.
Establish contacts with researchers in the ASR and NLP fields. 
Establish contacts with s2s application developers.

(6) TECH TRANSITION (Weight 10)

Contribute recipes for building ASR systems with our corpora to the MFLTS. 
Transition ASR components and our other products to USA Army Africa and MFLTS. 

(7) DIVERSITY: 
Support ARL's diversity initiatives by participating in locally-sponsored diversity training, broad outreach, and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees. 

(8) SHARP: 
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all. 
Demonstrate support for the SHARP program by actively participating in required training and other educational programs. 
Intervene and appropriately respond to any instances of sexual harassment or sexual assault and encourage others to do the same.

- TODO Heroico: Tune Chain Models?
I found more references to the mini_librispeech recipe in the scripts I am using to do the i-vector extraction and chain model training.
I removed the references to the data splitting in the scripts when they are run on the clsp cluster.

- TODO African French: Get WER scores for models trained on progressivley smaller training sets. (try removing yaounde)
%WER 41.99 [ 1340 / 3191, 114 ins, 314 del, 912 sub ] exp/mono/decode_ca16/wer_11_0.0
%WER 24.01 [ 766 / 3191, 154 ins, 105 del, 507 sub ] exp/tri3b/decode_ca16.si/wer_13_0.0
%WER 23.22 [ 741 / 3191, 85 ins, 185 del, 471 sub ] exp/tri1/decode_ca16/wer_14_1.0
%WER 20.78 [ 663 / 3191, 112 ins, 126 del, 425 sub ] exp/tri2b/decode_ca16/wer_15_0.0
%WER 17.05 [ 544 / 3191, 100 ins, 90 del, 354 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 12.28 [ 392 / 3191, 65 ins, 66 del, 261 sub ] exp/chain/tdnn_sp/decode_ca16/wer_13_0.0
%WER 12.28 [ 392 / 3191, 54 ins, 74 del, 264 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_12_0.5

| model | WER gabonread gp gabonconv 26.3 hours | WER gabonread gp yaounde gabonconv 36.6 hours | WER gabonread gp niger yaounde gabonconv 37.3 hours| gabonread gp niger yaounde gabonconv srica | gabonread gp niger yaounde gabonconv srica |arti
mono | 43.94 | 41.99 | 41.43 | 42.09 | 41.37 |
| tri1 | 23.72 |23.22 | 22.78 | 23.03 | 22.63 |
| tri2b | 22.56 | 20.78 | 20.34 | 20.90 | 20.09 |
| tri3b | 19.08 | 17.05 | 16.64 | 16.61 | 15.98 |
| chain | | 12.28 | 12.75 | 11.69 |12.63 |
|chaine online | | 12.28 | 12.85 | 11.69 | 12.60 |


%WER 43.94 [ 1402 / 3191, 117 ins, 288 del, 997 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 23.72 [ 757 / 3191, 154 ins, 100 del, 503 sub ] exp/tri1/decode_ca16/wer_15_0.0
%WER 22.56 [ 720 / 3191, 104 ins, 165 del, 451 sub ] exp/tri2b/decode_ca16/wer_17_1.0
%WER 19.08 [ 609 / 3191, 121 ins, 80 del, 408 sub ] exp/tri3b/decode_ca16/wer_17_0.5

- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
I worked a little on this today.
None of the languages work out of the box.
I think I'm going to write my own scripts.
I want to use utf8 and I don't want to mess with converting waveform data.
I will put the waveform data that is ready for processing under /mnt/disk01/globalphone.

** Goals for Tuesday:
- TODO MultiLang: Start processing GlobalPhone corpora. Start with corpora that overlap with our own corpora, i.e. Arabic, Croatian, French, German, Korean, Portuguese, Russian, Spanish.
- TODO African French: Get chain model results and move on to next step by removing more data.
- TODO Heroico: Maybe start from beginning since scripts are not moving forward and they die on pca transform estimation.

* DAR <2017-10-12 Thu>
** Goals for Thursday:
- TODO Objectives.
- TODO African French: Get tri3b results.
%WER 22.56 [ 720 / 3191, 124 ins, 116 del, 480 sub ] exp/tri3b/decode_ca16.si/wer_14_0.0
%WER 16.61 [ 530 / 3191, 97 ins, 86 del, 347 sub ] exp/tri3b/decode_ca16/wer_16_0.5
%WER 42.09 [ 1343 / 3191, 132 ins, 270 del, 941 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 141 ins, 120 del, 474 sub ] exp/tri1/decode_ca16/wer_13_0.0
%WER 20.90 [ 667 / 3191, 100 ins, 133 del, 434 sub ] exp/tri2b/decode_ca16/wer_15_0.5
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0

| model | WER |
mono | 42.09 |
| tri1 | 23.03 |
tri2b | 20.90 \
| tri3b | 16.61 |
| chain | 11.69 \ |
|chaine online | 11.69 |

- TODO Heroico: Tune chain models.
Here are the WER scores I get on the clsp cluster:
%WER 44.07 [ 4061 / 9215, 121 ins, 1871 del, 2069 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_12_0.0
%WER 41.95 [ 3866 / 9215, 149 ins, 1600 del, 2117 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative/wer_11_0.0
%WER 36.95 [ 6176 / 16713, 269 ins, 2525 del, 3382 sub ] exp/chain/tdnn1c_sp/decode_test/wer_9_0.0
%WER 35.25 [ 5891 / 16713, 251 ins, 2406 del, 3234 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_10_0.0
%WER 28.03 [ 2102 / 7498, 86 ins, 951 del, 1065 sub ] exp/chain/tdnn1c_sp/decode_native/wer_9_0.0
%WER 26.81 [ 2010 / 7498, 83 ins, 873 del, 1054 sub ] exp/chain/tdnn1c_sp_online/decode_native/wer_9_0.0
%WER 23.28 [ 2145 / 9215, 169 ins, 364 del, 1612 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 20.36 [ 3402 / 16713, 266 ins, 590 del, 2546 sub ] exp/mono/decode_test/wer_9_0.0
%WER 19.63 [ 1809 / 9215, 241 ins, 219 del, 1349 sub ] exp/tri3b/decode_nonnative.si/wer_17_1.0
%WER 16.70 [ 1252 / 7498, 97 ins, 221 del, 934 sub ] exp/mono/decode_native/wer_9_0.0
%WER 15.68 [ 1445 / 9215, 162 ins, 202 del, 1081 sub ] exp/tri1/decode_nonnative/wer_17_0.5
%WER 15.23 [ 2545 / 16713, 333 ins, 319 del, 1893 sub ] exp/tri3b/decode_test.si/wer_16_1.0
%WER 15.07 [ 1389 / 9215, 182 ins, 186 del, 1021 sub ] exp/tri2b/decode_nonnative/wer_17_0.5
%WER 12.91 [ 2158 / 16713, 225 ins, 299 del, 1634 sub ] exp/tri1/decode_test/wer_16_0.5
%WER 12.49 [ 1151 / 9215, 133 ins, 153 del, 865 sub ] exp/tri3b/decode_nonnative/wer_16_1.0
%WER 11.92 [ 1992 / 16713, 237 ins, 278 del, 1477 sub ] exp/tri2b/decode_test/wer_17_0.5
%WER 9.47 [ 1583 / 16713, 169 ins, 225 del, 1189 sub ] exp/tri3b/decode_test/wer_16_1.0
%WER 9.43 [ 707 / 7498, 93 ins, 89 del, 525 sub ] exp/tri3b/decode_native.si/wer_17_0.5
%WER 9.32 [ 699 / 7498, 65 ins, 93 del, 541 sub ] exp/tri1/decode_native/wer_14_0.5
%WER 7.94 [ 595 / 7498, 63 ins, 82 del, 450 sub ] exp/tri2b/decode_native/wer_14_0.5
%WER 5.61 [ 421 / 7498, 40 ins, 59 del, 322 sub ] exp/tri3b/decode_native/wer_16_0.5

| model | native | both | nonnative |
| mono | 16.70 | 20.36 | 23.28 |
| tri1 | 9.32 | 12.91 | 15.68 |
| tri2b | 7.94 | 11.92 | 15.07 |
| tri3b | 5.61 | 9.47 | 12.49 |
| chain | 28.03 | 36.95 | 44.07 |
| chain online | 26.81 | 35.25 | 41.95 |

- TODO African French: Run with another training chunk removed.
I am now running with Niger removed. 
- TODO Yaounde: More work to figure out why results are so bad.
I am going to test on the CA16 corpus.

- Hispanic Heritage Month Activity: I attended the presentation by Raquel Tamez.

** Goals for Friday:
- TODO Objectives
- TODO Yaounde: What WER scores do we get for ca16?
%WER 96.96 [ 3094 / 3191, 47 ins, 1382 del, 1665 sub ] exp/mono/decode_ca16/wer_17_0.0
%WER 90.99 [ 2050 / 2253, 39 ins, 971 del, 1040 sub ] exp/mono/decode_test/wer_14_1.0

So the problem is definitely not with the ARTI242 test set. 

- TODO African French: WER scores when srica is removed.
%WER 41.43 [ 1322 / 3191, 117 ins, 272 del, 933 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 133 ins, 124 del, 478 sub ] exp/tri3b/decode_ca16.si/wer_14_0.0
%WER 22.78 [ 727 / 3191, 109 ins, 144 del, 474 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 20.34 [ 649 / 3191, 114 ins, 128 del, 407 sub ] exp/tri2b/decode_ca16/wer_17_0.0
%WER 16.64 [ 531 / 3191, 106 ins, 75 del, 350 sub ] exp/tri3b/decode_ca16/wer_17_0.0
%WER 12.85 [ 410 / 3191, 65 ins, 73 del, 272 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_12_0.5
%WER 12.75 [ 407 / 3191, 77 ins, 56 del, 274 sub ] exp/chain/tdnn_sp/decode_ca16/wer_12_0.0

| model | WER |
mono | 41.43 |
| tri1 | 22.78 |
tri2b | 20.34 \
| tri3b | 16.64 |
| chain | 12.75 \ |
|chaine online | 12.85 |

* DAR <2017-10-11 Wed>
** Goals for Wednesday set Tuesday:
- TODO Objectives
I got the form from Shanel.
- TODO Yaounde: What happens with subs trained lm?
- TODO African French: Complete set of results.
Here is what I have now:
%WER 42.09 [ 1343 / 3191, 132 ins, 270 del, 941 sub ] exp/mono/decode_ca16/wer_10_0.0
%WER 23.03 [ 735 / 3191, 141 ins, 120 del, 474 sub ] exp/tri1/decode_ca16/wer_13_0.0
%WER 20.90 [ 667 / 3191, 100 ins, 133 del, 434 sub ] exp/tri2b/decode_ca16/wer_15_0.5
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0


| model | WER |
mono | 42.09 |
| tri1 | 23.03 |
tri2b | 20.90 \
| tri3b | |
| chain | 11.69 \ |
|chaine online | 11.69 |

- TODO Heroico: Results including chain model results and contact Yenda.
I contacted Yenda.
He was not much help.
I fixed a reference to the clsp cluster in the ivector prep script.
It was hard coded to use the mini_librispeech corpus.

%WER 9.47 [ 710 / 7498, 96 ins, 94 del, 520 sub ] exp/tri3b/decode_native.si/wer_17_0.5
%WER 9.44 [ 708 / 7498, 79 ins, 103 del, 526 sub ] exp/tri1/decode_native/wer_14_0.5
%WER 9.24 [ 1544 / 16713, 164 ins, 214 del, 1166 sub ] exp/tri3b/decode_test/wer_17_1.0
%WER 8.27 [ 620 / 7498, 76 ins, 90 del, 454 sub ] exp/tri2b/decode_native/wer_15_0.5
%WER 5.57 [ 418 / 7498, 43 ins, 53 del, 322 sub ] exp/tri3b/decode_native/wer_16_0.5
%WER 27.34 [ 2519 / 9215, 191 ins, 558 del, 1770 sub ] exp/chain/tdnn1c_sp/decode_nonnative/wer_11_0.0
%WER 26.16 [ 2411 / 9215, 184 ins, 537 del, 1690 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative/wer_11_0.0
%WER 23.13 [ 2131 / 9215, 173 ins, 376 del, 1582 sub ] exp/mono/decode_nonnative/wer_9_0.0
%WER 22.44 [ 3750 / 16713, 278 ins, 848 del, 2624 sub ] exp/chain/tdnn1c_sp/decode_test/wer_11_0.0
%WER 21.58 [ 3607 / 16713, 273 ins, 819 del, 2515 sub ] exp/chain/tdnn1c_sp_online/decode_test/wer_11_0.0
%WER 20.40 [ 3410 / 16713, 273 ins, 610 del, 2527 sub ] exp/mono/decode_test/wer_9_0.0
%WER 19.32 [ 1780 / 9215, 246 ins, 206 del, 1328 sub ] exp/tri3b/decode_nonnative.si/wer_16_1.0
%WER 17.07 [ 1280 / 7498, 98 ins, 231 del, 951 sub ] exp/mono/decode_native/wer_9_0.0
%WER 16.16 [ 1212 / 7498, 77 ins, 306 del, 829 sub ] exp/chain/tdnn1c_sp/decode_native/wer_12_0.0
%WER 15.91 [ 1193 / 7498, 73 ins, 303 del, 817 sub ] exp/chain/tdnn1c_sp_online/decode_native/wer_12_0.0
%WER 15.74 [ 1450 / 9215, 159 ins, 211 del, 1080 sub ] exp/tri1/decode_nonnative/wer_16_0.5
%WER 15.37 [ 1416 / 9215, 218 ins, 155 del, 1043 sub ] exp/tri2b/decode_nonnative/wer_17_0.0
%WER 14.98 [ 2504 / 16713, 382 ins, 278 del, 1844 sub ] exp/tri3b/decode_test.si/wer_17_0.5
%WER 12.91 [ 2158 / 16713, 240 ins, 303 del, 1615 sub ] exp/tri1/decode_test/wer_15_0.5
%WER 12.14 [ 1119 / 9215, 127 ins, 153 del, 839 sub ] exp/tri3b/decode_nonnative/wer_17_1.0
%WER 12.12 [ 2026 / 16713, 303 ins, 232 del, 1491 sub ] exp/tri2b/decode_test/wer_17_0.0

| model | native | both | nonnative |
| mono | 17.07 | 20.40 | 23.13 |
| tri1 | 9.44 | 12.91 | 15.74 |
| tri2b | 8.27 | 12.12 | 15.37 |
| tri3b | 5.57 | 9.24 | 12.14 |
| chain | 16.16 | 22.44 | 27.34 |
| chain online | 15.91 | 21.58 | 26.16 |

Why are the chain models not better than the cd gmm hmm ?

** Goals for Thursday:
- TODO Objectives
- TODO African French: Get tri3b results.
- TODO Heroico: Tune chain models. 
- TODO African French: Run with another training chunk removed 
- TODO Yaounde: More work to figure out why results are so bad.

* DAR <2017-10-10 Tue>
** Goals for Next Week:
- TODO Objectives
- TODO Heroico: Chain model results?
- DONE Heroico: Decide about lm (include simple lm?)
I am going with only subs.
- TODO Yaounde: Chain model results?

- TODO African French: Build system on progressivly smaller training sets.

I removed the ARTI data set of 242 utterances.
So far I only have chain model results.
%WER 11.69 [ 373 / 3191, 56 ins, 65 del, 252 sub ] exp/chain/tdnn_sp/decode_ca16/wer_10_1.0
%WER 11.69 [ 373 / 3191, 54 ins, 65 del, 254 sub ] exp/chain/tdnn_sp_online/decode_ca16/wer_10_1.0

This is better than the previous result which was 12.63.
Is there something wrong with the ARTI242 data? (Transcripts, recording parameters, ...)

- TODO Multilang: Minimal example
** Goals for Wednesday:
- TODO Objectives
- TODO Yaounde: What happens with subs trained lm?
- TODO African French: Complete set of results.
- TODO Heroico: Results including chain model results and contact Yenda.

* DAR <2017-10-05 Thu>
** Goals for Thursday set Wednesday:
- TODO Objectives:
- DONE SOFTunisia: Finish training with stage 16 speakers and get rough draft to ZAC.
- TODO African French: Build system without ARTI corpus.
- DONE Heroico: Incorporate subs trained lm into system.
I am going to remove the gp lm from the recipe.
I want to use UTF8 as the text encoding.
I am pretty sure the gp lm is not in utf8.
Here are the WER scores for today.
I don't have the chain model results for subs yet.
%WER 67.34 [ 6205 / 9215, 398 ins, 1455 del, 4352 sub ] exp/chain/tdnn1c_sp/decode_nonnative_gplm/wer_8_0.5
%WER 66.61 [ 6138 / 9215, 388 ins, 1417 del, 4333 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_gplm/wer_8_0.5
%WER 65.28 [ 6016 / 9215, 453 ins, 1214 del, 4349 sub ] exp/mono/decode_nonnative_gplm/wer_8_0.0
%WER 62.40 [ 10429 / 16713, 733 ins, 2089 del, 7607 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 6.22 [ 573 / 9215, 28 ins, 182 del, 363 sub ] exp/chain/tdnn1c_sp/decode_nonnative_simple/wer_15_0.0
%WER 60.64 [ 10135 / 16713, 686 ins, 2199 del, 7250 sub ] exp/chain/tdnn1c_sp/decode_test_gplm/wer_8_0.5
%WER 59.98 [ 10024 / 16713, 665 ins, 2155 del, 7204 sub ] exp/chain/tdnn1c_sp_online/decode_test_gplm/wer_8_0.5
%WER 58.98 [ 4422 / 7498, 339 ins, 820 del, 3263 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 57.58 [ 5306 / 9215, 571 ins, 787 del, 3948 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_14_1.0
%WER 57.04 [ 5256 / 9215, 526 ins, 861 del, 3869 sub ] exp/tri1/decode_nonnative_gplm/wer_12_1.0
%WER 55.14 [ 5081 / 9215, 518 ins, 838 del, 3725 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.06 [ 8868 / 16713, 950 ins, 1356 del, 6562 sub ] exp/tri1/decode_test_gplm/wer_12_1.0
%WER 52.74 [ 8815 / 16713, 1047 ins, 1206 del, 6562 sub ] exp/tri3b/decode_test_gplm.si/wer_14_1.0
%WER 52.41 [ 3930 / 7498, 288 ins, 734 del, 2908 sub ] exp/chain/tdnn1c_sp/decode_native_gplm/wer_7_1.0
%WER 51.73 [ 3879 / 7498, 227 ins, 829 del, 2823 sub ] exp/chain/tdnn1c_sp_online/decode_native_gplm/wer_8_1.0
%WER 50.87 [ 4688 / 9215, 588 ins, 673 del, 3427 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_1.0
%WER 50.66 [ 8466 / 16713, 1049 ins, 1184 del, 6233 sub ] exp/tri2b/decode_test_gplm/wer_13_1.0
%WER 48.07 [ 3604 / 7498, 422 ins, 497 del, 2685 sub ] exp/tri1/decode_native_gplm/wer_12_1.0
%WER 47.30 [ 7906 / 16713, 1125 ins, 942 del, 5839 sub ] exp/tri3b/decode_test_gplm/wer_15_1.0
%WER 4.72 [ 435 / 9215, 18 ins, 152 del, 265 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_simple/wer_17_0.0
%WER 46.87 [ 3514 / 7498, 512 ins, 379 del, 2623 sub ] exp/tri3b/decode_native_gplm.si/wer_13_1.0
%WER 45.25 [ 3393 / 7498, 467 ins, 413 del, 2513 sub ] exp/tri2b/decode_native_gplm/wer_13_1.0
%WER 42.92 [ 3218 / 7498, 566 ins, 287 del, 2365 sub ] exp/tri3b/decode_native_gplm/wer_13_1.0
%WER 4.19 [ 700 / 16713, 43 ins, 223 del, 434 sub ] exp/chain/tdnn1c_sp/decode_test_simple/wer_15_0.0
%WER 3.80 [ 350 / 9215, 93 ins, 22 del, 235 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 3.31 [ 553 / 16713, 33 ins, 187 del, 333 sub ] exp/chain/tdnn1c_sp_online/decode_test_simple/wer_17_0.0
%WER 31.57 [ 2909 / 9215, 193 ins, 610 del, 2106 sub ] exp/mono/decode_nonnative_subs/wer_9_0.0
%WER 28.51 [ 4765 / 16713, 401 ins, 880 del, 3484 sub ] exp/mono/decode_test_subs/wer_8_0.0
%WER 2.71 [ 453 / 16713, 121 ins, 37 del, 295 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 25.64 [ 2363 / 9215, 351 ins, 290 del, 1722 sub ] exp/tri3b/decode_nonnative_subs.si/wer_16_0.5
%WER 24.69 [ 1851 / 7498, 178 ins, 310 del, 1363 sub ] exp/mono/decode_native_subs/wer_8_0.0
%WER 22.91 [ 2111 / 9215, 245 ins, 311 del, 1555 sub ] exp/tri1/decode_nonnative_subs/wer_17_0.0
%WER 21.33 [ 1966 / 9215, 164 ins, 361 del, 1441 sub ] exp/tri2b/decode_nonnative_subs/wer_17_1.0
%WER 21.00 [ 3509 / 16713, 427 ins, 510 del, 2572 sub ] exp/tri3b/decode_test_subs.si/wer_17_1.0
%WER 19.26 [ 3219 / 16713, 314 ins, 522 del, 2383 sub ] exp/tri1/decode_test_subs/wer_16_0.5
%WER 18.13 [ 1671 / 9215, 208 ins, 247 del, 1216 sub ] exp/tri3b/decode_nonnative_subs/wer_17_1.0
%WER 17.88 [ 2989 / 16713, 275 ins, 511 del, 2203 sub ] exp/tri2b/decode_test_subs/wer_16_1.0
%WER 1.71 [ 158 / 9215, 44 ins, 15 del, 99 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.5
%WER 1.65 [ 124 / 7498, 12 ins, 42 del, 70 sub ] exp/chain/tdnn1c_sp/decode_native_simple/wer_17_0.0
%WER 1.64 [ 123 / 7498, 15 ins, 38 del, 70 sub ] exp/chain/tdnn1c_sp_online/decode_native_simple/wer_17_0.0
%WER 1.54 [ 142 / 9215, 36 ins, 14 del, 92 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 15.30 [ 1147 / 7498, 149 ins, 154 del, 844 sub ] exp/tri3b/decode_native_subs.si/wer_17_1.0
%WER 14.62 [ 2444 / 16713, 282 ins, 359 del, 1803 sub ] exp/tri3b/decode_test_subs/wer_17_1.0
%WER 14.55 [ 1091 / 7498, 122 ins, 153 del, 816 sub ] exp/tri1/decode_native_subs/wer_13_1.0
%WER 1.40 [ 105 / 7498, 30 ins, 16 del, 59 sub ] exp/tri3b/decode_native_simple.si/wer_17_1.0
%WER 13.28 [ 996 / 7498, 119 ins, 123 del, 754 sub ] exp/tri2b/decode_native_subs/wer_15_0.5
%WER 1.21 [ 203 / 16713, 59 ins, 25 del, 119 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.14 [ 191 / 16713, 50 ins, 25 del, 116 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 1.04 [ 96 / 9215, 24 ins, 12 del, 60 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 10.26 [ 769 / 7498, 74 ins, 113 del, 582 sub ] exp/tri3b/decode_native_subs/wer_16_1.0
%WER 0.83 [ 138 / 16713, 33 ins, 23 del, 82 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 11 ins, 14 del, 44 sub ] exp/mono/decode_nonnative_simple/wer_17_0.0
%WER 0.64 [ 48 / 7498, 14 ins, 12 del, 22 sub ] exp/tri3b/decode_native_simple/wer_15_1.0
%WER 0.57 [ 96 / 16713, 15 ins, 27 del, 54 sub ] exp/mono/decode_test_simple/wer_17_0.0
%WER 0.56 [ 42 / 7498, 10 ins, 12 del, 20 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.55 [ 41 / 7498, 9 ins, 11 del, 21 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.37 [ 28 / 7498, 4 ins, 13 del, 11 sub ] exp/mono/decode_native_simple/wer_17_0.0
john@A-TEAM19054:~/work/kaldi/egs/heroico/s5$ 
- TODO Heroico: Contact Yenda about status of recipe.
- TODO Yaounde: What is wrong?
I decoded the training set:
%WER 21.59 [ 15107 / 69957, 1707 ins, 5179 del, 8221 sub ] exp/mono/decode_train/wer_12_1.0
This is still pretty bad.

- TODO Multilang: Minimal example.

** Goals for Friday:
- Objectives
- TODO Heroico: Run again with subs lm and without gplm.
- TODO Yaounde: Test on CA16.
- TODO African French: Get an lm working.
- TODO African French: Test on ca16.
* DAR <2017-10-04 Wed>
** Goals for Wednesday set Tuesday:
- TODO Objectives:

1. TECHNICAL COMPETENCE
a. Acoustic Models for Low Resource Languages
I. Problem
ASR components like acousti models are not available for key low resource languages and accented versions of major languages.
II. Research Question
Can small and large resources available from many languages be leveraged to build acoustic models for a language for which we have very few resources?
III. Proposed Method 
I will choose a target language say Korean for which we actually have some resources so that we can evaluate results. 
I will use the kaldi multilang recipe to build acoustic models for the target "low" resource language Korean given resources from many other source languages. 
I will obtain the source language resources from the GlobalPhone corpus and government owned corpora that are available to us (see below).
b. Corpus Curation
I. Problem:
In my previous job at West Point, I was part of a team that developed speech corpora for the following languages: 
A. Arabic (West Point LDC2002S02)
B. Arabic (Tunisia)
C. French (collected in Yaounde Cameroon)
D. Croatian (LDC2005S28)
E. German
F. Korean (LDC2006S36)
G. Portuguese (Brazilian LDC2008s04)
H. Russian (West Point LDC2003S05)
I. Russian (SOF Peter)
J. Spanish (Heroico LDC2006S37)

Of these 10 corpora, 6 were published in the Linguistic Data Consortium. 
The remaining 4 corpora for Arabic, French, German and Russian are available to our team and have yet to be published. 
Unless the corpora are published, results obtained from training ASR systems with them are not reproduceable.

ii. Proposed Method: 
I have 3 related goals this year concerning these 4 remaining corpora.
First, I want to prepare these corpora for use as source data in the multilang project mentioned above. 
Second, I want to publish these corpora in the openslrm.org repository.
Third, In addition to the multilang project, I want to write Kaldi recipes for each corpus. 

Publishing these corpora is an important goal. 
It is not hard to imagine these corpora disappearing after our generation retires. 

Preparing the data and writing the recipes will entail producing a lexicon that I also would like to publlish on openslr.org.

** Goals for Thursday:
- TODO Objectives:
- TODO SOFTunisia: Finish training with stage 16 speakers and get rough draft to ZAC.
- TODO African French: Build system without ARTI corpus.
- TODO Heroico: Incorporate subs trained lm into system.
- TODO Heroico: Contact Yenda about status of recipe.
- TODO Yaounde: What is wrong?
- TODO Multilang: Minimal example.

* DAR <2017-10-03 Tue>
** Goals for Tuesday set Monday:
- TODO Objectives.
- TODO Heroico: Get results for gplm 
The chain model WER results for the gplm decoding are not good.
I'm not sure what is wrong.
%WER 67.89 [ 6256 / 9215, 388 ins, 1391 del, 4477 sub ] exp/chain/tdnn1c_sp/decode_nonnative_gplm/wer_9_0.0
%WER 67.28 [ 6200 / 9215, 408 ins, 1353 del, 4439 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_gplm/wer_9_0.0
%WER 64.77 [ 5969 / 9215, 415 ins, 1251 del, 4303 sub ] exp/mono/decode_nonnative_gplm/wer_7_0.5
%WER 63.55 [ 10621 / 16713, 543 ins, 2647 del, 7431 sub ] exp/chain/tdnn1c_sp/decode_test_gplm/wer_9_0.5
%WER 62.47 [ 10441 / 16713, 688 ins, 2192 del, 7561 sub ] exp/chain/tdnn1c_sp_online/decode_test_gplm/wer_8_0.5
%WER 62.27 [ 10408 / 16713, 766 ins, 2062 del, 7580 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 59.18 [ 4437 / 7498, 338 ins, 808 del, 3291 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 58.08 [ 4355 / 7498, 316 ins, 863 del, 3176 sub ] exp/chain/tdnn1c_sp/decode_native_gplm/wer_7_1.0
%WER 58.01 [ 5346 / 9215, 647 ins, 754 del, 3945 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_13_1.0
%WER 56.82 [ 5236 / 9215, 385 ins, 1036 del, 3815 sub ] exp/tri1/decode_nonnative_gplm/wer_14_1.0
%WER 56.66 [ 4248 / 7498, 302 ins, 821 del, 3125 sub ] exp/chain/tdnn1c_sp_online/decode_native_gplm/wer_8_0.5
%WER 55.01 [ 5069 / 9215, 494 ins, 836 del, 3739 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.30 [ 8908 / 16713, 1171 ins, 1156 del, 6581 sub ] exp/tri3b/decode_test_gplm.si/wer_13_1.0
%WER 53.18 [ 8888 / 16713, 817 ins, 1494 del, 6577 sub ] exp/tri1/decode_test_gplm/wer_13_1.0
%WER 5.25 [ 484 / 9215, 32 ins, 164 del, 288 sub ] exp/chain/tdnn1c_sp/decode_nonnative_simple/wer_17_0.0
%WER 51.32 [ 4729 / 9215, 647 ins, 596 del, 3486 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_0.5
%WER 50.72 [ 8476 / 16713, 917 ins, 1277 del, 6282 sub ] exp/tri2b/decode_test_gplm/wer_14_1.0
%WER 5.00 [ 461 / 9215, 33 ins, 144 del, 284 sub ] exp/chain/tdnn1c_sp_online/decode_nonnative_simple/wer_16_0.0
%WER 48.25 [ 3618 / 7498, 513 ins, 403 del, 2702 sub ] exp/tri1/decode_native_gplm/wer_10_1.0
%WER 47.54 [ 7945 / 16713, 1171 ins, 916 del, 5858 sub ] exp/tri3b/decode_test_gplm/wer_16_0.5
%WER 47.43 [ 3556 / 7498, 424 ins, 482 del, 2650 sub ] exp/tri3b/decode_native_gplm.si/wer_16_1.0
%WER 4.54 [ 758 / 16713, 52 ins, 261 del, 445 sub ] exp/chain/tdnn1c_sp/decode_test_simple/wer_17_0.0
%WER 45.37 [ 3402 / 7498, 423 ins, 440 del, 2539 sub ] exp/tri2b/decode_native_gplm/wer_14_1.0
%WER 42.77 [ 3207 / 7498, 455 ins, 368 del, 2384 sub ] exp/tri3b/decode_native_gplm/wer_16_1.0
%WER 4.02 [ 672 / 16713, 49 ins, 227 del, 396 sub ] exp/chain/tdnn1c_sp_online/decode_test_simple/wer_17_0.0
%WER 3.96 [ 365 / 9215, 101 ins, 23 del, 241 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 3.56 [ 267 / 7498, 20 ins, 96 del, 151 sub ] exp/chain/tdnn1c_sp/decode_native_simple/wer_17_0.0
%WER 2.75 [ 206 / 7498, 13 ins, 74 del, 119 sub ] exp/chain/tdnn1c_sp_online/decode_native_simple/wer_17_0.0
%WER 2.70 [ 451 / 16713, 124 ins, 42 del, 285 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 1.66 [ 153 / 9215, 45 ins, 17 del, 91 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.0
%WER 1.25 [ 115 / 9215, 28 ins, 14 del, 73 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 1.20 [ 200 / 16713, 56 ins, 30 del, 114 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.19 [ 89 / 7498, 24 ins, 19 del, 46 sub ] exp/tri3b/decode_native_simple.si/wer_17_0.5
%WER 0.93 [ 156 / 16713, 32 ins, 27 del, 97 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 0.82 [ 76 / 9215, 17 ins, 7 del, 52 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 12 ins, 12 del, 45 sub ] exp/mono/decode_nonnative_simple/wer_15_0.0
%WER 0.69 [ 116 / 16713, 25 ins, 19 del, 72 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.62 [ 104 / 16713, 19 ins, 30 del, 55 sub ] exp/mono/decode_test_simple/wer_16_0.0
%WER 0.61 [ 46 / 7498, 10 ins, 13 del, 23 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.52 [ 39 / 7498, 8 ins, 12 del, 19 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.52 [ 39 / 7498, 4 ins, 13 del, 22 sub ] exp/tri3b/decode_native_simple/wer_16_0.0
%WER 0.39 [ 29 / 7498, 4 ins, 17 del, 8 sub ] exp/mono/decode_native_simple/wer_17_1.0

- TODO Heeroico: Build bigger lm and test.
I asked Justin to download the subs Spanishcorpus.
I'll try making an lm with subs.

- DONE SOFTunisia: Finish subs lm

- DONE SOFTunisia: Build cd gmm hmm system and chain models
I finished the cd gmm hmm and I sent Zac the rough draft for the simple decoding.
I did not do chain models.
- DONE: Get rough draft hypotheses for stage 16?
I sent Zac all the hypothesis transcripts for CTELLTWO.
I asked him to work on the first 4 speakers.
- TODO SOFTunisia: Compare WER scores for old and new lexicons(Zac will do this)

** Goals for Wednesday:
- TODO Objectives:
- TODO Heroico: Finish run on clsp cluster and contact Yenda
- TODO SOFTunisia: Get feed back from Zac and send him the hypotheses from the gplm decoding.
- TODO Yaounde: Why are WERs so bad?
- TODO Heroico: Build lm with subs? 
* DAR <2017-10-02 Mon>
** Goals for Next Week:
- TODO Objectives:
- TODO African French: build systems on progressively larger amounts of data.
- TODO Multilang: minimal example.
- TODO Yaounde: Writel recipe to kaldi standards (organize data).
- TODO Yaounde: Figure out why WER scores are so bad: test on training data
- TODO SOFTunisia: Rebuild system with Zac's new lexicon.
I focused on this today.
I am trying to make a clean fresh start.
I am building the new system in the softunisia/s5 directory.
I wrote new scripts to process the answers and recordings training data without copying files.
These scripts are very similar to the ones I wrote for heroico. 
Zac wants me to start from stage 15 and redo stage 16.
This is a good idea since he can compare the new lexicon with the old one. 
As I am getting ready to leave, I am building an LM with the subs corpus.

- Heroico: 
I added commands to my run.sh script to use the gp lm in testing.
I get the following WER results:
%WER 64.77 [ 5969 / 9215, 415 ins, 1251 del, 4303 sub ] exp/mono/decode_nonnative_gplm/wer_7_0.5
%WER 62.27 [ 10408 / 16713, 766 ins, 2062 del, 7580 sub ] exp/mono/decode_test_gplm/wer_7_0.5
%WER 59.18 [ 4437 / 7498, 338 ins, 808 del, 3291 sub ] exp/mono/decode_native_gplm/wer_7_0.5
%WER 58.01 [ 5346 / 9215, 647 ins, 754 del, 3945 sub ] exp/tri3b/decode_nonnative_gplm.si/wer_13_1.0
%WER 56.82 [ 5236 / 9215, 385 ins, 1036 del, 3815 sub ] exp/tri1/decode_nonnative_gplm/wer_14_1.0
%WER 55.01 [ 5069 / 9215, 494 ins, 836 del, 3739 sub ] exp/tri2b/decode_nonnative_gplm/wer_14_1.0
%WER 53.30 [ 8908 / 16713, 1171 ins, 1156 del, 6581 sub ] exp/tri3b/decode_test_gplm.si/wer_13_1.0
%WER 53.18 [ 8888 / 16713, 817 ins, 1494 del, 6577 sub ] exp/tri1/decode_test_gplm/wer_13_1.0
%WER 51.32 [ 4729 / 9215, 647 ins, 596 del, 3486 sub ] exp/tri3b/decode_nonnative_gplm/wer_16_0.5
%WER 50.72 [ 8476 / 16713, 917 ins, 1277 del, 6282 sub ] exp/tri2b/decode_test_gplm/wer_14_1.0
%WER 48.25 [ 3618 / 7498, 513 ins, 403 del, 2702 sub ] exp/tri1/decode_native_gplm/wer_10_1.0
%WER 47.54 [ 7945 / 16713, 1171 ins, 916 del, 5858 sub ] exp/tri3b/decode_test_gplm/wer_16_0.5
%WER 47.43 [ 3556 / 7498, 424 ins, 482 del, 2650 sub ] exp/tri3b/decode_native_gplm.si/wer_16_1.0
%WER 45.37 [ 3402 / 7498, 423 ins, 440 del, 2539 sub ] exp/tri2b/decode_native_gplm/wer_14_1.0
%WER 42.77 [ 3207 / 7498, 455 ins, 368 del, 2384 sub ] exp/tri3b/decode_native_gplm/wer_16_1.0
%WER 3.96 [ 365 / 9215, 101 ins, 23 del, 241 sub ] exp/tri3b/decode_nonnative_simple.si/wer_17_0.0
%WER 2.70 [ 451 / 16713, 124 ins, 42 del, 285 sub ] exp/tri3b/decode_test_simple.si/wer_17_0.0
%WER 1.66 [ 153 / 9215, 45 ins, 17 del, 91 sub ] exp/tri2b/decode_nonnative_simple/wer_17_0.0
%WER 1.25 [ 115 / 9215, 28 ins, 14 del, 73 sub ] exp/tri3b/decode_nonnative_simple/wer_17_0.0
%WER 1.20 [ 200 / 16713, 56 ins, 30 del, 114 sub ] exp/tri2b/decode_test_simple/wer_17_0.0
%WER 1.19 [ 89 / 7498, 24 ins, 19 del, 46 sub ] exp/tri3b/decode_native_simple.si/wer_17_0.5
%WER 0.93 [ 156 / 16713, 32 ins, 27 del, 97 sub ] exp/tri3b/decode_test_simple/wer_17_0.0
%WER 0.82 [ 76 / 9215, 17 ins, 7 del, 52 sub ] exp/tri1/decode_nonnative_simple/wer_17_0.0
%WER 0.75 [ 69 / 9215, 12 ins, 12 del, 45 sub ] exp/mono/decode_nonnative_simple/wer_15_0.0
%WER 0.69 [ 116 / 16713, 25 ins, 19 del, 72 sub ] exp/tri1/decode_test_simple/wer_17_0.0
%WER 0.62 [ 104 / 16713, 19 ins, 30 del, 55 sub ] exp/mono/decode_test_simple/wer_16_0.0
%WER 0.61 [ 46 / 7498, 10 ins, 13 del, 23 sub ] exp/tri2b/decode_native_simple/wer_17_0.5
%WER 0.52 [ 39 / 7498, 8 ins, 12 del, 19 sub ] exp/tri1/decode_native_simple/wer_16_0.0
%WER 0.52 [ 39 / 7498, 4 ins, 13 del, 22 sub ] exp/tri3b/decode_native_simple/wer_16_0.0
%WER 0.39 [ 29 / 7498, 4 ins, 17 del, 8 sub ] exp/mono/decode_native_simple/wer_17_1.0

The chain models are training as I am getting ready to leave.

** Goals for Tuesday:
- TODO Objectives.
- TODO Heroico: Get results for gplm 
- TODO Heeroico: Build bigger lm and test.
- TODO SOFTunisia: Finish subs lm
- TODO SOFTunisia: Build cd gmm hmm system and chain models
- TODO: Get rough draft hypotheses for stage 16?
- TODO SOFTunisia: Compare WER scores for old and new lexicons(Zac will do this)
