#+TITLE: Tech Note: A Minimal Example of Multi-Task Learning Using Librispeech 960 and Tunisian MSA Corpora

John J. Morgan

* ABSTRACT
Multi-Task Learning was applied to a Large corpus of English and a small corpus of Modern Standard Arabic  read speech. 
An improvement in Word Error Rate over the best Single-Task Learning method was observed. 
* INTRODUCTION
We are using neural networks as a framework for acoustic modeling in Automatic Speech Recognition. 
large ASR systems are trained on tens of thousands of hours of speech data (Heigold, et al., 2013). 
NN models perform well when these amounts of data are available. 
We assume we are under conditions of severe training data sparcity in the target language.
We also assume that we have access to a large corpus of speech in another language. 
We will refer to this other language as the background language.
In these situations where training data is scarce for the target language, more sophisticated methods need to be employed in order to see benefits from NN models. 
Multi-task Learning (MTL) is a framework that enables the advantages of Deep Learning to be applied in the situation where we have access to large amounts of data in background tasks and scarce   resources in the target task.  

Here our tasks are languages.
In MTL We build an NN that has two kinds of layers.
1. Shared Layers.
2. Language Specific Layers.

The shared layers are trained on all the training data from all the languages.
We assume that the shared layers extract the important features that are common across languages. 

The language specific layers are trained only on data from the target language.

For this note we present an experiment to test the MTL method on two specific publically available corpora: the large Librispeech English corpus and a small corpus of Tunisian Accented Modern Standard Arabic. 

We formulate our research question as follows.

Can NN acoustic models trained with MTL on data  from  two different languages improve performance of an  ASR system for a Low Resource target language?

*  DATA
** Librispeech
LibriSpeech is a corpus of read speech, based on LibriVox's public domain audio books. 
We used the cleaned training fold of 960 hours of speech. 

** Tunisian MSA:
This is a corpus of ten hours of Modern Standard Arabic as spoken by 120 male and female Tunisians in 2003. 
The informants provided recitations and answers to questions. 



Each language has 8 layers?

1. One input layer,
2. 6 hidden layers,
3. One Bottleneck layer,
4. One affine layer, and
5. One soft max layer.

The dimension of the hidden layers is 1024.
The dimension of the Bottleneck layer is 512.

The soft max layer outputs a probability distribution over the clustered triphones.

Context:
16 frames to the left, 12 frames to the right.
 
*  EXPERIMENT
We used the kaldi toolkit to build our ASR systems. 
We derived our setup from the kaldi babel_multilang recipe. 
We tried to implement our setup to contain only methods that are required to run the MTL method. 
Thus, We did not use i-vectors which are standard in many kaldi recipes. 
We did include however a bottleneck layer. 

We built baseline SAT GMM HMM acoustic models for both the Librispeech and Tunisian MSA corpora.
For the Tunisian MSA baseline system we derived our pronouncing dictionary from the 2 million entries from the QCRI vowelized dictionary available at:
http://alt.qcri.org/resources/speech/dictionary/ar-ar_lexicon_2014-03-17.txt.bz2
We added the out of vocabulary words from the Tunisian MSA training set and the test set. 
We trained our 3-gram language model with srilm on the transcripts from the training and test data. 
Our best WER results for Tunisian MSA were obtained with online chain models.

We followed the kaldi standard recipe for the librispeech cleaned 960 hours of speech task with one exception. 
We extracted and trained with plp_pitch features instead of mfcc features. 
We followed this path since we derived our scripts from the kaldi babel_multilang recipe. 

Alignments generated by the  SAT GMM HMMs were used to train a 7-layer bilingual raw deep neural network  on the combined set of training examples from the English and Tunisian MSA Librispeech  corpora.

Note that instead of considering this as a bilingual model it can be viewed as a Tunisian MSA model whose parameters are shared with a English model. 

The data from the Tunisian MSA corpus was used to readjust the parameters in the last two layers of the bilingual DNN model to produce a new monolingual Tunisian MSA  acoustic model. 

Similarly, a new monolingual English model was produced. These two models share the parameters in their first five layers, only their final 2 layers are different.

The monolingual Tunisian MSA acoustic model was used to decode a test set of speech from four speakers, 3 Libyan males and one Tunisian female.

* RESULTS

Tunisian_MSA Baseline:
%WER 11.03 [ 726 / 6584, 61 ins, 237 del, 428 sub ] exp/chain/tdnn1a_sp_online/decode_test/wer_13_0.0

WER after MTL:
%WER 7.12 [ 469 / 6584, 100 ins, 124 del, 245 sub ] exp/multi/Tunisian_MSA/decode_test/wer_13_0.0


* 7 CONCLUSIONS AND FUTURE WORK
* 8 REFERENCES
Heigold, G., V. Vanhoucke, A. Senior, P. Nguyen, M. Ranzato, M. Devin, J. Dean. 2013. Multilingual acoustic models using distributed deep neural networks. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), IEEE, Vancouver, CA.
* ABBREVIATIONS
ASR	Automatic Speech Recognition
DL	Deep Learning
ML	Machine Learning
MT	Machine Translation
MTL	Multi-Task Learning
NN	Neural Network
