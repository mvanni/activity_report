* DAR <2017-09-12 Tue>* DAR <2017-09-11 Mon>
** Goals for Tuesday set Monday:
- TODO Write Accomplishments.
I wrote a bit more today.
- DONE Mandatory Tart Training tomorrow at 10. ( I   am pretty sure I have already done this, but it is still in my ted) . I've learned that is is always always best to leave  mandatory training to  as late as possible otherwise you end up doing them twice.
- TODO African French ASR: Prepare data from all our GP and African French Speech holdings for training 1 system.
I added niger and the ca16 test set.
I also made the speaker directories distinct from each other across corpora.
**  Goals for Next Week:
- TODO Write accomplishments.
I wrote a little today.
- TODO Mandatory Training.
- TODO Multilang: Minimal example.
- DONE French: prepare data from SRI Canadian African Accented speech for use in training and  testing ASR systems.



- GP French:
I finally have the WER scores for chain models:
Here are all the WER scores I computed:
%WER 44.74 [ 9981 / 22307, 672 ins, 1926 del, 7383 sub ] exp/mono/decode_dev_tgpr/wer_9_0.0
%WER 41.48 [ 9002 / 21700, 637 ins, 1627 del, 6738 sub ] exp/mono/decode_eval_tgpr/wer_9_0.0
%WER 29.02 [ 6473 / 22307, 780 ins, 762 del, 4931 sub ] exp/tri1/decode_dev_tgpr/wer_15_0.0
%WER 28.98 [ 6464 / 22307, 816 ins, 728 del, 4920 sub ] exp/tri2a/decode_dev_tgpr/wer_14_0.0
%WER 28.52 [ 6361 / 22307, 887 ins, 669 del, 4805 sub ] exp/tri3b/decode_dev_tgpr.si/wer_15_0.0
%WER 27.44 [ 6121 / 22307, 810 ins, 665 del, 4646 sub ] exp/tri2b/decode_dev_tgpr/wer_16_0.0
%WER 26.61 [ 5774 / 21700, 757 ins, 524 del, 4493 sub ] exp/tri2a/decode_eval_tgpr/wer_14_0.0
%WER 26.58 [ 5767 / 21700, 674 ins, 593 del, 4500 sub ] exp/tri1/decode_eval_tgpr/wer_13_0.5
%WER 26.35 [ 5719 / 21700, 776 ins, 539 del, 4404 sub ] exp/tri3b/decode_eval_tgpr.si/wer_13_0.5
%WER 26.06 [ 5814 / 22307, 768 ins, 635 del, 4411 sub ] exp/sgmm2b/decode_dev_tgpr/wer_11_0.0
%WER 25.89 [ 5776 / 22307, 799 ins, 600 del, 4377 sub ] exp/tri3b/decode_dev_tgpr/wer_17_0.0
%WER 25.65 [ 5567 / 21700, 761 ins, 494 del, 4312 sub ] exp/tri2b/decode_eval_tgpr/wer_15_0.0
%WER 25.14 [ 5608 / 22307, 785 ins, 543 del, 4280 sub ] exp/sgmm2_4a/decode_dev_tgpr/wer_11_0.0
%WER 24.48 [ 5461 / 22307, 782 ins, 490 del, 4189 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it1/wer_11_0.0
%WER 24.34 [ 5429 / 22307, 756 ins, 491 del, 4182 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it2/wer_12_0.0
%WER 24.33 [ 5427 / 22307, 788 ins, 453 del, 4186 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it4/wer_12_0.0
%WER 24.32 [ 5426 / 22307, 766 ins, 471 del, 4189 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it3/wer_12_0.0
%WER 24.02 [ 5212 / 21700, 738 ins, 443 del, 4031 sub ] exp/tri3b/decode_eval_tgpr/wer_17_0.0
%WER 23.51 [ 5101 / 21700, 608 ins, 516 del, 3977 sub ] exp/sgmm2_4a/decode_eval_tgpr/wer_12_0.5
%WER 23.32 [ 5201 / 22307, 633 ins, 457 del, 4111 sub ] exp/chain/tdnn_sp_online/decode_dev/wer_11_0.0
%WER 23.24 [ 5184 / 22307, 585 ins, 518 del, 4081 sub ] exp/chain/tdnn_sp/decode_dev/wer_12_0.0
%WER 23.05 [ 5001 / 21700, 693 ins, 389 del, 3919 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it1/wer_10_0.5
%WER 22.97 [ 4985 / 21700, 767 ins, 323 del, 3895 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it4/wer_11_0.0
%WER 22.91 [ 4972 / 21700, 735 ins, 353 del, 3884 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it3/wer_12_0.0
%WER 22.86 [ 4961 / 21700, 666 ins, 400 del, 3895 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it2/wer_11_0.5
%WER 21.24 [ 4610 / 21700, 515 ins, 337 del, 3758 sub ] exp/chain/tdnn_sp_online/decode_eval/wer_12_0.0
%WER 21.21 [ 4602 / 21700, 513 ins, 331 del, 3758 sub ] exp/chain/tdnn_sp/decode_eval/wer_12_0.0


- African French:
I am writing scripts to train on gp, sri ca, gabon read, ARTI Cameroon, Niger and Yaounde.
I'm almost done preparing the data.
I might include  more data if we have it.
Maybe the conversational data?
I'm still missing yaounde.
** Goals for Tuesday:
- TODO Write Accomplishments.
- TODO Mandatory Tart Training tomorrow at 10. ( I   am pretty sure I have already done this, but it is still in my ted) . I've learned that is is always always best to leave  mandatory training to  as late as possible otherwise you end up doing them twice.
- TODO African French ASR: Prepare data from all our GP and African French Speech holdings for training 1 system.

* DAR <2017-09-08 Fri>
** Goals for Friday set Thursday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Run minimal example with french german and spanish.
- TODO Heroico: tune triphone parameters and address warnings in log files concerning acoustic models not getting training data.
- TODO GP French: Run cd gmm hmm chain model end to end with results.
- TODO GP Yaounde + Canadian Accented  African French
- TODO ARTI_Cameroon_242_fr: Prepare data for processing with kaldi
- TODO SOFTunisia: Dictionary work.
- GP French:
I am running the GP French system on the CA16 test corpus:
I am seeing some strange.
I get a better WER score on my laptop for the mono system.
%WER 75.99 [ 2425 / 3191, 79 ins, 741 del, 1605 sub ] exp/mono/decode_test/wer_11_0.5

%WER 76.15 [ 2430 / 3191, 158 ins, 538 del, 1734 sub ] exp/mono/decode_test/wer_9_0.0
%WER 70.89 [ 2262 / 3191, 187 ins, 601 del, 1474 sub ] exp/tri1/decode_test/wer_14_0.5
%WER 70.26 [ 2242 / 3191, 185 ins, 618 del, 1439 sub ] exp/tri2a/decode_test/wer_17_0.0
%WER 69.48 [ 2217 / 3191, 135 ins, 643 del, 1439 sub ] exp/chain/tdnn_sp_online/decode_test/wer_17_0.0
%WER 44.74 [ 9981 / 22307, 672 ins, 1926 del, 7383 sub ] exp/mono/decode_dev_tgpr/wer_9_0.0
%WER 41.48 [ 9002 / 21700, 637 ins, 1627 del, 6738 sub ] exp/mono/decode_eval_tgpr/wer_9_0.0
%WER 29.02 [ 6473 / 22307, 780 ins, 762 del, 4931 sub ] exp/tri1/decode_dev_tgpr/wer_15_0.0
%WER 28.98 [ 6464 / 22307, 816 ins, 728 del, 4920 sub ] exp/tri2a/decode_dev_tgpr/wer_14_0.0
%WER 28.52 [ 6361 / 22307, 887 ins, 669 del, 4805 sub ] exp/tri3b/decode_dev_tgpr.si/wer_15_0.0
%WER 27.44 [ 6121 / 22307, 810 ins, 665 del, 4646 sub ] exp/tri2b/decode_dev_tgpr/wer_16_0.0
%WER 26.61 [ 5774 / 21700, 757 ins, 524 del, 4493 sub ] exp/tri2a/decode_eval_tgpr/wer_14_0.0
%WER 26.58 [ 5767 / 21700, 674 ins, 593 del, 4500 sub ] exp/tri1/decode_eval_tgpr/wer_13_0.5
%WER 26.35 [ 5719 / 21700, 776 ins, 539 del, 4404 sub ] exp/tri3b/decode_eval_tgpr.si/wer_13_0.5
%WER 26.06 [ 5814 / 22307, 768 ins, 635 del, 4411 sub ] exp/sgmm2b/decode_dev_tgpr/wer_11_0.0
%WER 25.89 [ 5776 / 22307, 799 ins, 600 del, 4377 sub ] exp/tri3b/decode_dev_tgpr/wer_17_0.0
%WER 25.65 [ 5567 / 21700, 761 ins, 494 del, 4312 sub ] exp/tri2b/decode_eval_tgpr/wer_15_0.0
%WER 25.14 [ 5608 / 22307, 785 ins, 543 del, 4280 sub ] exp/sgmm2_4a/decode_dev_tgpr/wer_11_0.0
%WER 24.48 [ 5461 / 22307, 782 ins, 490 del, 4189 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it1/wer_11_0.0
%WER 24.34 [ 5429 / 22307, 756 ins, 491 del, 4182 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it2/wer_12_0.0
%WER 24.33 [ 5427 / 22307, 788 ins, 453 del, 4186 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it4/wer_12_0.0
%WER 24.32 [ 5426 / 22307, 766 ins, 471 del, 4189 sub ] exp/sgmm2_4a_mmi_b0.1/decode_dev_tgpr_it3/wer_12_0.0
%WER 24.02 [ 5212 / 21700, 738 ins, 443 del, 4031 sub ] exp/tri3b/decode_eval_tgpr/wer_17_0.0
%WER 23.51 [ 5101 / 21700, 608 ins, 516 del, 3977 sub ] exp/sgmm2_4a/decode_eval_tgpr/wer_12_0.5
%WER 23.32 [ 5201 / 22307, 633 ins, 457 del, 4111 sub ] exp/chain/tdnn_sp_online/decode_dev/wer_11_0.0
%WER 23.24 [ 5184 / 22307, 585 ins, 518 del, 4081 sub ] exp/chain/tdnn_sp/decode_dev/wer_12_0.0
%WER 23.05 [ 5001 / 21700, 693 ins, 389 del, 3919 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it1/wer_10_0.5
%WER 22.97 [ 4985 / 21700, 767 ins, 323 del, 3895 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it4/wer_11_0.0
%WER 22.91 [ 4972 / 21700, 735 ins, 353 del, 3884 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it3/wer_12_0.0
%WER 22.86 [ 4961 / 21700, 666 ins, 400 del, 3895 sub ] exp/sgmm2_4a_mmi_b0.1/decode_eval_tgpr_it2/wer_11_0.5
%WER 21.24 [ 4610 / 21700, 515 ins, 337 del, 3758 sub ] exp/chain/tdnn_sp_online/decode_eval/wer_12_0.0
%WER 21.21 [ 4602 / 21700, 513 ins, 331 del, 3758 sub ] exp/chain/tdnn_sp/decode_eval/wer_12_0.0



** Goals for Wednesday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO African French: Incorporate Yaounde into system scripts.

* DAR <2017-09-07 Thu>
** Goals for Thursday set Wednesday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Investigate possibly running on babel corpora to understand method.
With help from Yenda, I'm getting a much better idea  of how this works.
1. build monolingual systems up to lda mllt sat adapted context dependent gmm hmm  models. 
babel calls these models tri5 they used to be called tri3b.
2. 
Run Yenda's script that sets up directories for multilang training.
3. Do multilang training.
I'm not sure about this since Yenda seems to have skipped this in his explanation, but I think you have to run:
local/nnet3/run_tdnn_multilang.sh
4. Extract BN features.
I think step 3 produces a bnf extractor.
If not, that has to be done somehow.
5. Train systems with new feature vectors.
Concatenate BN features to mfcc features.

- TODO Heroico: Tune triphone parameters.
Yenda says he will incorporate the heroico recipe in the kaldi repo.
He says they would like to put the lexicon on openslr.org because it takes up too much space on the git repo.
I told him to go ahead with that plan.
I wonder if the LDC heroico speech data could be mirrored there as well?

- TODO SOFTunisia: Dictionary work.
- TODO: GP French: Get Chain model results.
I decided to rewrite the scripts so that they do not put everything under an extra FR directory layer.
This was causing problems at chain model decoding  time.
I am currently rerunning the training scripts.

** Goals for Friday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Run minimal example with french german and spanish.
- TODO Heroico: tune triphone parameters and address warnings in log files concerning acoustic models not getting training data.
- TODO GP French: Run cd gmm hmm chain model end to end with results.
- TODO GP Yaounde + Canadian Accented  African French
- TODO ARTI_Cameroon_242_fr: Prepare data for processing with kaldi
- TODO SOFTunisia: Dictionary work.

* DAR <2017-09-06 Wed>
** Goals for Wednesday set Tuesday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Run a very simple example with our corpora.
- TODO Heroico: Tune triphone parameters.
- TODO SOFTunisia: Dictionary work.
- TODO: GP French: Get Chain model results.
I think training is done.
I have to get chain model decoding working for GP FR.

** Goals for Thursday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Investigate possibly running on babel corpora to understand method.
- TODO Heroico: Tune triphone parameters.
- TODO SOFTunisia: Dictionary work.
- TODO: GP French: Get Chain model results.

* DAR <2017-09-05 Tue>
**  Goals for Next Week:
- TODO Write Accomplishments.
- TODO Mandatory Training.
I am trying to access the CTIP training.

- TODO Heroico: Tune triphone parameters.
- TODO SOFTunisia: Dictionary work.
- TODO: GP French: Get Chain model results.
- DONE Multilang: Investigate first steps.
Yenda sent me his setup script.
I am modifying it to work now with: french (gp), heroico, german, croatian and russian.
I might as well run it on all the corpora I have now.
I've got it partially running.

** Goals for Wednesday:
- TODO Write Accomplishments.
- TODO Mandatory Training.
- TODO Multilang: Run a very simple example with our corpora.
- TODO Heroico: Tune triphone parameters.
- TODO SOFTunisia: Dictionary work.
- TODO: GP French: Get Chain model results.

* DAR <2017-08-31 Thu>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments.
I actually started working on this finally!
- TODO SOFTunisia: Get Encode::Arabic::Buckwalter to print with no vocalization
I downloaded http://alt.qcri.org/resources/msa-dictionary/releases/current/ar-ar_lexicon_2014-03-17.txt.bz2
It looks like the same dictionary available from the LDC.
No utf8.
The transcripts of the GALE data are at:
/mnt/corpora/LDC2013T17

We could use these data as input to the method described in the paper:
A COMPLETE KALDI RECIPE FOR BUILDING ARABIC SPEECH RECOGNITION SYSTEMS


- Heroico:
I contacted Yenda to see if I can contribute the recipe to the Kaldi repo.
I separated the native and nonnative data from the usma corpus.
I am training on heroico and testing on usma.
I am hoping that the fact that we have a corpus of nonnative speech will make Yenda and Dan accept the recipe for contribution to kaldi.

As I am getting ready to leave, I am running the heroico recipe again.
It is almost at the point where it training the chain models.
I don't do any testing until I've trained all the models.

** Goals for Friday:
- TODO Accomplishments
- TODO Mandatory training.
- TODO GP French: Get recipe running.
- TODO SOFTunisia: Search for dictionary in original text encoding.

* DAR <2017-08-30 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Accomplishments.
- DONE Mandatory training.
I did the no fear training.

- Heroico:
Here are the WER results I have so far:
%WER 30.34 [ 5071 / 16713, 646 ins, 1174 del, 3251 sub ] exp/tri3b/decode_test.si/wer_16_1.0
%WER 28.09 [ 4695 / 16713, 400 ins, 1637 del, 2658 sub ] exp/tri2b/decode_test/wer_14_0.5
%WER 27.24 [ 4552 / 16713, 355 ins, 852 del, 3345 sub ] exp/mono/decode_test/wer_9_1.0
%WER 26.61 [ 4447 / 16713, 407 ins, 1187 del, 2853 sub ] exp/tri1/decode_test/wer_15_1.0
%WER 25.69 [ 4293 / 16713, 616 ins, 604 del, 3073 sub ] exp/tri3b/decode_test/wer_17_1.0

I do not understand why the tri2b is worse than mono.

The chain models are training now.

** Goals for Thursday:
- TODO Accomplishments.
- TODO SOFTunisia: Get Encode::Arabic::Buckwalter to print with no vocalization

* DAR <2017-08-29 Tue>
**  Goals for Tuesday set Monday:
- TODO Accomplishments
- TODO  Mandatory Training.
- TODO GP French: Finish the data prep and start training.
I wrote scripts that rename the data files.
I did this so that they are oredered correctly.
I think I succeeded.

- TODO Spanish Heroico: Do extra careful recipe writing.
I also have sorting problems here.
I checked in the utils/fix_data_dir.sh script to see how sorting is done.
It uses:
sort -k1,1 -u
The -u switch does uniq.
I'm not sure what the -k1,1 does.

I process the heroico and usma corpora separately.
I process the heroico answers recordings separately.
Then I bring them together and run the sort -k1,1-u on the lists.
This seems to give me good sorting.

I process USMA native and nonnative data separately.
Then I bring them together.
I run the sort -k1,1 -u on the resultin file.
This does not seem to b working. 

** Goals for Wednesday:
- TODO Accomplishments.
- TODO Mandatory training.

Stop everything and do these 2 goals!

* DAR <2017-08-28 Mon>
** Goals for Next Week:
- TODO Accomplishments
- TODO  Mandatory Training.
- TODO GP French: Reproduce a close as possible the GP French WER results.
I worked on this all day today.
I am mostly simplifying the kaldi scripts that build  the cd gmm hmm system with the French Globalphone corpus.
The scripts are written to handle several of the Globalphone languages.
This makes them overly complicated and hard to read.

I also am skipping some steps.
I am assuming that sox is already installed.
I have to copy the data to disk to get it in a format that can be used by the scripts.

** Goals for Tuesday:
- TODO Accomplishments
- TODO  Mandatory Training.
- TODO GP French: Finish the data prep and start training.
- TODO Spanish Heroico: Do extra careful recipe writing.

* DAR <2017-08-25 Fri>
** Goals for Friday set Thursday:
- TODO Accomplishments
- TODO Mandatory Training.
- TODO GP French. Implement kaldi recipe more closely.
I made some progress on this.
It looks like the recipe assumed there were files processed by shorten.
These files have an extention like: .adc.shn

* <2017-08-24 Thu>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO SOFTunisia: Get stage 17 rough draft to Zac.
This is on hold until Zac fixes the dictionary.
- TODO GP French: After getting results with current system, test a new system with the large LM including subs.
I am returning to the original kaldi gp recipe.
I am finding some strange steps.
The organization of the directories is different than what I have now.
I'd like to follow the recipe closely so I can tell what is causeing our worse WER scores.
- TODO Russian: Ditto. Get subs corpus if available.

** Goals for Friday:
- TODO Accomplishments
- TODO Mandatory Training.
- TODO GP French. Implement kaldi recipe more closely.

* DAR <2017-08-23 Wed>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO SOFTunisia: Get stage 17 rough draft to Zac.
This is on hold until Zac fixes the dictionary.
- TODO GP French: After getting results with current system, test a new system with the large LM including subs.
I am returning to the original kaldi gp recipe.
I am finding some strange steps.
The organization of the directories is different than what I have now.
I'd like to follow the recipe closely so I can tell what is causeing our worse WER scores.
- TODO Russian: Ditto. Get subs corpus if available.

**  Goals for Wednesday set Tuesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO GP French: cd gmm hmm chain model build.
I am not happy yet.
Here are the WER results so far:
%WER 46.16 [ 10293 / 22297, 1922 ins, 903 del, 7468 sub ] exp/gp/tri3b/decode_dev.si/wer_17_1.0
%WER 44.55 [ 9666 / 21698, 1913 ins, 696 del, 7057 sub ] exp/gp/tri3b/decode_eval.si/wer_17_1.0
%WER 44.26 [ 9868 / 22297, 2039 ins, 770 del, 7059 sub ] exp/gp/tri3b/decode_dev/wer_17_1.0
%WER 43.90 [ 9526 / 21698, 1365 ins, 1209 del, 6952 sub ] exp/gp/tri1/decode_eval/wer_17_1.0
%WER 43.52 [ 9442 / 21698, 1768 ins, 828 del, 6846 sub ] exp/gp/tri2b/decode_eval/wer_17_1.0
%WER 43.09 [ 9350 / 21698, 2082 ins, 571 del, 6697 sub ] exp/gp/tri3b/decode_eval/wer_17_1.0

I think the low scores might have something to do with the LM.
I am using the text from the GP prompts and the African data transcripts.

- TODO Russian: tune system.
I only have mono WER results so far.
%WER 56.61 [ 4989 / 8813, 258 ins, 1635 del, 3096 sub ] exp/mono/decode_test/wer_10_0.5

This seems to have gooten worse too.

Maybe the issue is the LM here too.
Is there a subs corpus for Russian?

- Croatian:
I am still getting horrible WER results here too.
%WER 97.53 [ 62125 / 63700, 227 ins, 24154 del, 37744 sub ] exp/mono/decode_train/wer_7_0.0

Notice that this results is on the training data.
Obviously, something is wrong.


- SOFTunisia:
Zac and I talked about the WER for the latest stage.
There seems to be a trend.
The WER gets worse as we go along.
We agreed that on the next stage Zac will modify the dictionary.

** Goals for Thursday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO SOFTunisia: Get stage 17 rough draft to Zac.
- TODO GP French: After getting results with current system, test a new system with the large LM including subs.
- TODO Russian: Ditto. Get subs corpus if available.

* DAR <2017-08-22 Tue>
** Goals for Tuesday set Monday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Croatian: Try to recover big dictionary.
I think I'm wasting my time doing this right now.
I am going to wait until GlobalPhone arrives.
Then I'll have a lexicon to work with.

- TODO Croatian: Build cd gmm hmm and chain models.

- TODO Russian: Figure out what is wrong.
I get 56% WER for monophones which is not horrible, but I still think something is not right.

** Goals for Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO GP: cd gmm hmm chain model build.
- TODO Russian: tune system.

* DAR <2017-08-21 Mon>
**  Goals set Last Week:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO SOFTunisia: Get rough draft transcriptions to Zac.
- TODO Russian: Rerun system build with chain models included.
Here are the WER results including chain models: 
%WER 61.42 [ 5413 / 8813, 325 ins, 2285 del, 2803 sub ] exp/chain/tdnn_sp/decode_test/wer_10_0.0
%WER 55.87 [ 4924 / 8813, 265 ins, 1574 del, 3085 sub ] exp/tri3b/decode_test.si/wer_14_0.5
%WER 55.68 [ 4907 / 8813, 342 ins, 1449 del, 3116 sub ] exp/mono/decode_test/wer_11_0.0
%WER 55.08 [ 4854 / 8813, 273 ins, 1657 del, 2924 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 54.34 [ 4789 / 8813, 344 ins, 1331 del, 3114 sub ] exp/tri1/decode_test/wer_15_0.0
%WER 49.91 [ 4399 / 8813, 248 ins, 1376 del, 2775 sub ] exp/tri3b/decode_test/wer_17_0.5

I still think something is wrong with the Russian build.

- DONE German: Build cd gmm hmm and chain models.
I do not have a test set separated out yet, so I tested on the training data.
Here are the WER results:
%WER 9.76 [ 10137 / 103907, 205 ins, 5528 del, 4404 sub ] exp/mono/decode_train/wer_14_0.0
%WER 3.03 [ 3145 / 103907, 247 ins, 1664 del, 1234 sub ] exp/tri1/decode_train/wer_17_0.0
%WER 2.93 [ 3041 / 103907, 302 ins, 1483 del, 1256 sub ] exp/tri3b/decode_train.si/wer_17_0.0
%WER 2.51 [ 2608 / 103907, 213 ins, 1418 del, 977 sub ] exp/tri2b/decode_train/wer_17_0.0
%WER 2.50 [ 2597 / 103907, 241 ins, 1372 del, 984 sub ] exp/tri3b/decode_train/wer_17_0.0
%WER 1.24 [ 1289 / 103907, 22 ins, 888 del, 379 sub ] exp/chain/tdnn_sp/decode_test/wer_10_0.0

- TODO Croatian: Build cd gmm hmms and chain models.
I worked all day today on this.
There is a problem with the dictionary that was part of the Westpoint hard drive.
The big dictionary is not encoded in utf8.
The multi-byte characters are mangled.
I'm not sure how to recover the dictionary in utf8.
I have a small (2500 entries) dictionary that I am using as a stand in for now.

** Goals for Tuesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Croatian: Try to recover big dictionary.
- TODO Croatian: Build cd gmm hmm and chain models.
- TODO Russian: Figure out what is wrong.

* DAR <2017-08-17 Thu>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- DONE Spanish: cd gmm hmm system build and results.
I had a bug in the script that prepares the dictionary.
I am restarting.
I am also using a different dictionary.

The build went really fast on the GPU machine.
%WER 34.87 [ 5828 / 16713, 537 ins, 1069 del, 4222 sub ] exp/mono/decode_test/wer_9_0.0
%WER 33.05 [ 5523 / 16713, 546 ins, 1809 del, 3168 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 32.75 [ 5473 / 16713, 680 ins, 1128 del, 3665 sub ] exp/tri3b/decode_test.si/wer_17_1.0
%WER 30.77 [ 5143 / 16713, 602 ins, 1168 del, 3373 sub ] exp/tri1/decode_test/wer_16_0.0
%WER 28.40 [ 4746 / 16713, 676 ins, 674 del, 3396 sub ] exp/tri3b/decode_test/wer_17_1.0

The GPU machine seems to be running faster.
I wonder if the fix to the memory helped speed up the GPU machine?

- TODO Entropic Spanish: Is there any 16khz data in this corpus?
I thought there was, since in the .hdr files it says that some (720)  of the recordings were done at 16khz.
But the flac files claim to be sampled at 8khz.
I am skipping this corpus
- TODO Multilang: Take first tiny steps with a few corpora.
- TODO German: Prepare Westpoint corpus.
I took the first steps.
It looks like there is a lot of transcription work done by Milan.
We have a dictionary.
I am not sure if we have transcriptions for the Answers.
I'm afraid we do not.

- DONE SOFTunisia: Get rough draft to Zac and try to get corrections back from him.
As I am getting ready to leave, I am decoding stage 15, speakers CTELLTHREE_015 - CTELLTHREE_022.

** Goals for Friday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO German: prepare data.
- TODO German LMKuln: Search for Answers transcripts.
- TODO SOFTunisia: Get stage 15 rough draft to Zac (via email).

* DAR <2017-08-16 Wed>
**  Goals for Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- DONE SOFTunisia: Get rough draft of stage 13 of Answers to Zac.
I gave the draft to Zac.
He got his corrections back to me at 1:30.
I have the next draft ready to give to him tomorrow.

- TODO Russian: Results for CD GMM HMMs.
%WER 55.87 [ 4924 / 8813, 265 ins, 1574 del, 3085 sub ] exp/tri3b/decode_test.si/wer_14_0.5
%WER 55.68 [ 4907 / 8813, 342 ins, 1449 del, 3116 sub ] exp/mono/decode_test/wer_11_0.0
%WER 55.08 [ 4854 / 8813, 273 ins, 1657 del, 2924 sub ] exp/tri2b/decode_test/wer_17_0.0
%WER 54.34 [ 4789 / 8813, 344 ins, 1331 del, 3114 sub ] exp/tri1/decode_test/wer_15_0.0
%WER 49.91 [ 4399 / 8813, 248 ins, 1376 del, 2775 sub ] exp/tri3b/decode_test/wer_17_0.5

I would expect the triphones to be much better.
I might have old and new results mixed in above.
- TODO Russian: Why are tri1 results worse than mono results?
- DONE Spanish: Prepare Data.
I think I'm mostly done preparing our West Point and Heroico data.
I started working with the entropic corpus.
Bad news: The data is sampled at 8khz.
At least the 100sentences read part of the corpus that I looked at is sampled at 8khz.

** Goals for Thursday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Spanish: cd gmm hmm system build and results.
- TODO Entropic Spanish: Is there any 16khz data in this corpus?
- TODO Multilang: Take first tiny steps with a few corpora.
- TODO German: Prepare Westpoint corpus.
- TODO SOFTunisia: Get rough draft to Zac and try to get corrections back from him.

* DAR <2017-08-15 Tue>
** Goals for Tuesday set Monday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Russian:  Run 2 decodings with 2 different LMs.
I've already run the decoding with the gp lm (see the results from yesterday).
Here are the same results run on the GPU machine.
%WER 89.62 [ 7898 / 8813, 301 ins, 2322 del, 5275 sub ] exp/mono/decode_test/wer_7_0.5
%WER 84.56 [ 7452 / 8813, 284 ins, 2250 del, 4918 sub ] exp/tri1/decode_test/wer_10_1.0
%WER 83.26 [ 7338 / 8813, 324 ins, 2158 del, 4856 sub ] exp/tri2b/decode_test/wer_11_0.5
%WER 82.84 [ 7301 / 8813, 354 ins, 1938 del, 5009 sub ] exp/tri3b/decode_test.si/wer_9_1.0
%WER 76.16 [ 6712 / 8813, 295 ins, 1957 del, 4460 sub ] exp/tri3b/decode_test/wer_12_1.0

Now I am going to run decoding with the lm made from the prompts and answers.
As I am getting ready to leave, I have results for monophones.
%WER 57.84 [ 5097 / 8813, 241 ins, 1765 del, 3091 sub ] exp/tri1/decode_test/wer_15_0.5
%WER 55.68 [ 4907 / 8813, 342 ins, 1449 del, 3116 sub ] exp/mono/decode_test/wer_11_0.0
The LM trained on the prompts and Ansers  makes a big difference.

But something is not right.
The monophone results are better than the triphone tri1 results.

I still do not have the correct sorting order for the files in the SOFPeter corpus.
It looks like Steve's hunch that the length of the file names might be right.
I changed Answers to a and Recordings to r.
Now all the files have the same format, specifically, the length.
xxx_yyyy_{a|r}.wav

- TODO Spanish: Prepare data for training.
- TODO SOFTunisia: Get corrections back from Zac and turn around next rough draft.
Got the corrections back from Zac.
I am decoding the new batch of Answers.

** Goals for Wednesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO SOFTunisia: Get rough draft of stage 13 of Answers to Zac.
- TODO Russian: Results for CD GMM HMMs.
- TODO Russian: Why are tri1 results worse than mono results?
- TODO Spanish: Prepare Data.

* DAR <2017-08-14 Mon>
**  Goals for Next Week:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Spanish:  Data preparation to build cd gmm hmm system.
- TODO Russian: Results for cd gmm hmm.
Here is what I got from the run on Friday:
%WER 95.21 [ 8391 / 8813, 281 ins, 2548 del, 5562 sub ] exp/mono/decode_test/wer_8_1.0
%WER 85.10 [ 7500 / 8813, 227 ins, 2514 del, 4759 sub ] exp/tri1/decode_test/wer_11_1.0
%WER 81.58 [ 7190 / 8813, 313 ins, 2199 del, 4678 sub ] exp/tri2b/decode_test/wer_11_1.0
%WER 81.28 [ 7163 / 8813, 283 ins, 2239 del, 4641 sub ] exp/tri3b/decode_test.si/wer_12_1.0
%WER 76.76 [ 3934 / 5125, 192 ins, 1215 del, 2527 sub ] [PARTIAL] exp/tri3b/decode_test/wer_11_1.0

This seems really bad.
Something is probably wrong.


I spent the day going through the run.sh script.
There was a problem with the utt2spk file.
I had to rename the files and directories.
The utt2spk file has to be written so that if you sort it by utterance id you get the same as if you sort it by speaker id.
To achieve this you want the speaker id to be a prefix of the utterance id.
So now I have directories 001 - 107 indexed by speakers.
In each of these directories I have files with names like:
001_Anssers_0001.wav 
This seems to have fixed the problem.

I am training monophones right now.
What should I do for the LM?
I have the prompts that I can use to make an lm and I have the lm from GlobalPHone.

- TODO Russian: Chain models?
- TODO SOFTunisia: Get rough drafts to Zac (3 during the week).
I got him stage 12 today.
- TODO GP: Get results without Yaounde.

** Goals for Tuesday:
- TODO Accomplishments.
- TODO Mandatory Training.
- TODO Russian:  Run 2 decodings with 2 different LMs.
- TODO Spanish: Prepare data for training.
- TODO SOFTunisia: Get corrections back from Zac and turn around next rough draft.

* DAR <2017-08-11 Fri>
**  Goals for Friday set Thursday:
- TODO Mandatory Training.
- TODO Accomplishments.
- TODO Proposal.
- TODO Russian: Train acoustic models.
- TODO Russian: Transcripts for Westpoint I corpus.
- TODO Spanish: Start data prep.
- TODO GP: Results with no yaounde.

* DAR <2017-08-10 Thu>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments.
- TODO Proposal.
- TODO Mandatory Training.
- TODO Russian: prepare the lexicon.
I figured out that LC_ALL needs to be set to C to get ɛ to be sorted.
I think I'm done with the SOFPeter lexicon.
The WEstpoint I corpus lexicon needs work.
The transcripts are written in latex 7-bit ascii encoding. 

- TODO Russian: Prepare an lm.
Where should I get the text data?
i can make an lm on just the prompts and Answers transcripts.
- TODO Russian: Continue training acoustic models.
- TODO Russian: Decode with trained acoustic models new lexicon and lm.
- DONE SOFTunisia: Get next rough draft to Zac.
I got him stage 11 with a rough draft for speakers CTELLONE_010 thru CTELLONE_018.

- TODO Spanish: Start data preparation.
- TODO GP: Run test with models built only on GP.

** Goals for Friday:
- TODO Mandatory Training.
- TODO Accomplishments.
- TODO Proposal.
- TODO Russian: Train acoustic models.
- TODO Russian: Transcripts for Westpoint I corpus.
- TODO Spanish: Start data prep.
- TODO GP: Results with no yaounde.
* DAR <2017-08-09 Wed>
** Goals for Wednesday set Tuesday:
- TODO Accomplishments.
- TODO Proposal
- TODO Mandatory Training.
- TODO Gp + Yaounde: Final chain model tuning. 
Here are the results from yesterday's run:
%WER 36.48 [ 1164 / 3191, 135 ins, 298 del, 731 sub ] exp/chain/tdnn_sp_online/decode_central_accord/wer_10_0.0
%WER 36.04 [ 1150 / 3191, 113 ins, 318 del, 719 sub ] exp/chain/tdnn_sp/decode_central_accord/wer_11_0.0
%WER 35.82 [ 1143 / 3191, 88 ins, 270 del, 785 sub ] exp/chain/tdnn_sp/decode_central_accord_1/wer_10_0.0
%WER 34.15 [ 2732 / 7999, 138 ins, 998 del, 1596 sub ] exp/chain/tdnn_sp/decode_niger/wer_13_0.0
%WER 33.75 [ 2700 / 7999, 152 ins, 945 del, 1603 sub ] exp/chain/tdnn_sp_online/decode_niger/wer_12_0.0
%WER 30.87 [ 10152 / 32888, 669 ins, 1719 del, 7764 sub ] exp/chain/tdnn_sp/decode_test/wer_9_0.0
%WER 30.82 [ 10137 / 32888, 677 ins, 1718 del, 7742 sub ] exp/chain/tdnn_sp_online/decode_test/wer_9_0.0
%WER 30.18 [ 6730 / 22297, 365 ins, 902 del, 5463 sub ] exp/chain/tdnn_sp/decode_dev/wer_9_0.0
%WER 30.14 [ 6721 / 22297, 365 ins, 909 del, 5447 sub ] exp/chain/tdnn_sp_online/decode_dev/wer_9_0.0

Ignore the decode_central_accord_1 result. It is from an earlier run.
Notice that these results are quite a bit worse. 
I did not expect this.
I increased the chunks per mini batch and the WER went up.

- DONE Russian: phony dictionary and lm.
I have trained monophones.
I found work I had done at Westpoint on Russian.
There is a utf8 dictionary.
I started preparing the lexicon with this dictionary.
For some reason the ɛ character does not get processed by some tools.

- TODO Spanish: Investigate Corpora.
I also found work I had done at Westpoint for Spanish.
It includes a dictionary and aparently transcripts for all the Answers data.

- DONE SOFTunisia: Get next rough draft to Zac.
I gave him a rough draft of all the utterances remaining on CTELLFOUR.

I am running the decoding for the next batch.

** Goals for Wednesday:
- TODO Accomplishments.
- TODO Proposal.
- TODO Mandatory Training.
- TODO Russian: prepare the lexicon.
- TODO Russian: Prepare an lm.
- TODO Russian: Continue training acoustic models.
- TODO Russian: Decode with trained acoustic models new lexicon and lm.
- TODO SOFTunisia: Get next rough draft to Zac.
- TODO Spanish: Start data preparation.
- TODO GP: Run test with models built only on GP.

* DAR <2017-08-08 Tue>
**  Goals for Tuesday set Monday:
- TODO Mandatory training.
- TODO Accomplishments and Proposal.
- DONE Get thumb drive with rough draft to Zac.
I Gave it to Zac and he got it back to me this afternoon.
- DONE Russian: Rename files.
This took a lot of work.
But now it looks like I have the waveform data almost ready for training.
- TODO Accent Id:
- DONE SOFTunisia: Chain model tuning.
Here are the results after halving the chunk widths:
%WER 42.37 [ 1352 / 3191, 99 ins, 412 del, 841 sub ] exp/chain/tdnn_sp/decode_central_accord/wer_13_0.0
%WER 41.59 [ 1327 / 3191, 100 ins, 402 del, 825 sub ] exp/chain/tdnn_sp_online/decode_central_accord/wer_13_0.0
%WER 37.15 [ 2972 / 7999, 157 ins, 1021 del, 1794 sub ] exp/chain/tdnn_sp/decode_niger/wer_15_0.0
%WER 36.58 [ 2926 / 7999, 139 ins, 1033 del, 1754 sub ] exp/chain/tdnn_sp_online/decode_niger/wer_17_0.0
%WER 35.82 [ 1143 / 3191, 88 ins, 270 del, 785 sub ] exp/chain/tdnn_sp/decode_central_accord_1/wer_10_0.0
%WER 33.05 [ 10871 / 32888, 588 ins, 2286 del, 7997 sub ] exp/chain/tdnn_sp/decode_test/wer_11_0.0
%WER 32.83 [ 10797 / 32888, 592 ins, 2274 del, 7931 sub ] exp/chain/tdnn_sp_online/decode_test/wer_11_0.0
%WER 31.04 [ 6921 / 22297, 316 ins, 1109 del, 5496 sub ] exp/chain/tdnn_sp/decode_dev/wer_9_0.0
%WER 31.04 [ 6920 / 22297, 323 ins, 1091 del, 5506 sub ] exp/chain/tdnn_sp_online/decode_dev/wer_9_0.0

Notice that the WER scores are worse than the previous ones.
Lowering the chunk widths increases the WERs.

My next experiment run will pump up the chunk widths and lower the chunks per mini batch.

Here are the resulsts for chunks per mini batch down to 16,8,2 and chunk widths pumped up to 280,200,320
%WER 35.82 [ 1143 / 3191, 88 ins, 270 del, 785 sub ] exp/chain/tdnn_sp/decode_central_accord_1/wer_10_0.0
%WER 33.72 [ 1076 / 3191, 109 ins, 250 del, 717 sub ] exp/chain/tdnn_sp/decode_central_accord/wer_10_0.0
%WER 33.34 [ 1064 / 3191, 104 ins, 250 del, 710 sub ] exp/chain/tdnn_sp_online/decode_central_accord/wer_10_0.0
%WER 32.47 [ 2597 / 7999, 105 ins, 1003 del, 1489 sub ] exp/chain/tdnn_sp/decode_niger/wer_13_0.5
%WER 31.90 [ 2552 / 7999, 157 ins, 826 del, 1569 sub ] exp/chain/tdnn_sp_online/decode_niger/wer_12_0.0
%WER 30.39 [ 6775 / 22297, 375 ins, 933 del, 5467 sub ] exp/chain/tdnn_sp_online/decode_dev/wer_9_0.0
%WER 30.31 [ 9969 / 32888, 655 ins, 1648 del, 7666 sub ] exp/chain/tdnn_sp/decode_test/wer_9_0.0
%WER 30.31 [ 6758 / 22297, 367 ins, 929 del, 5462 sub ] exp/chain/tdnn_sp/decode_dev/wer_9_0.0
%WER 30.19 [ 9929 / 32888, 590 ins, 1833 del, 7506 sub ] exp/chain/tdnn_sp_online/decode_test/wer_10_0.0

This looks better.

I am running one more experiment with the chunks per mini batch set to 32,16,8
** Goals for Wednesday:
- TODO Accomplishments.
- TODO Proposal
- TODO Mandatory Training.
- TODO Gp + Yaounde: Final chain model tuning. 
- TODO Russian: phony dictionary and lm.
- TODO Spanish: Investigate Corpora.
- TODO SOFTunisia: Get next rough draft to Zac.

* DAR <2017-08-07 Mon>
** Goals for Next Week:
- TODO Accomplishments and Proposal.
- TODO GP + Yaounde: Chain Model tuning.
Here is the run that I started Friday:
%WER 38.04 [ 1214 / 3191, 102 ins, 399 del, 713 sub ] exp/chain/tdnn_sp/decode_central_accord/wer_13_0.0
%WER 35.82 [ 1143 / 3191, 88 ins, 270 del, 785 sub ] exp/chain/tdnn_sp/decode_central_accord_1/wer_10_0.0
%WER 35.70 [ 2856 / 7999, 170 ins, 984 del, 1702 sub ] exp/chain/tdnn_sp/decode_niger/wer_14_0.0
%WER 32.10 [ 10556 / 32888, 598 ins, 2215 del, 7743 sub ] exp/chain/tdnn_sp/decode_test/wer_11_0.0
%WER 30.65 [ 6835 / 22297, 338 ins, 1060 del, 5437 sub ] exp/chain/tdnn_sp/decode_dev/wer_9_0.0

I set the chunks per mini batch to 128,64,32 and I got a crash.
I left that parameter there and halved the chunk width to 35,25,40 and  the training did not crash.
Results?

- DONE SOFTunisia: Get a rough draft of the next chunk of data to Zac.
Finally, I have something I can give to Zac.
It's not pretty, but it's a start.

- TODO: Investigate Russian, Spanish and Arabic resources for building ASR system for TransApps.
I'm making progress on the Russian.
The hardest part is to get the file names changed so that I can get the utt2spk files and spk2utt to sort correctly.
This involves renaming files.
- TODO Mandatory Training.
- TODO Accent Id: Use older kaldi recipe that does not use A DNN to build the UBM and iVector extractor.

** Goals for Tuesday:
- TODO Mandatory training.
- TODO Accomplishments and Proposal.
- TODO Get thumb drive with rough draft to Zac.
- TODO Russian: Rename files.
- TODO Accent Id:
- TODO SOFTunisia: Chain model tuning.

* DAR <2017-08-04 Fri>
**  Goals for Friday set Thursday:
- TODO Accomplishments and Proposal
- TODO GP + Yaounde: Chain model tuning.
Lowering the chunks per mini batch did not work.
I am trying  training with lower chunk sizes of 70,50,80
Good news: the traininng is running.
Here is the configuration:
{'alignment_subsampling_factor': 3,
 'apply_deriv_weights': False,
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'chunk_left_context': 0,
 'chunk_left_context_initial': 0,
 'chunk_right_context': 0,
 'chunk_right_context_final': 0,
 'chunk_width': '70,50,80',
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'deriv_truncate_margin': None,
 'dir': 'exp/chain/tdnn_sp',
 'dropout_schedule': None,
 'egs_command': 'queue.pl',
 'egs_dir': None,
 'egs_opts': '--frames-overlap-per-eg 0',
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train_sp_hires',
 'final_effective_lrate': 0.0001,
 'frame_subsampling_factor': 3,
 'frames_per_iter': 3000000,
 'initial_effective_lrate': 0.001,
 'l2_regularize': 5e-05,
 'lat_dir': 'exp/chain/tri3b_train_sp_lats',
 'leaky_hmm_coefficient': 0.1,
 'left_deriv_truncate': None,
 'left_tolerance': 5,
 'lm_opts': '--num-extra-lm-states=2000',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_param_change': 2.0,
 'momentum': 0.0,
 'num_chunk_per_minibatch': '64,32,16',
 'num_epochs': 3.0,
 'num_jobs_final': 1,
 'num_jobs_initial': 1,
 'online_ivector_dir': 'exp/nnet3/ivectors_train_sp_hires',
 'preserve_model_interval': 100,
 'presoftmax_prior_scale_power': -0.25,
 'proportional_shrink': 60.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'right_tolerance': 5,
 'samples_per_iter': 400000,
 'shrink_saturation_threshold': 0.4,
 'shrink_value': 1.0,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'transform_dir': 'exp/chain/tri3b_train_sp_lats',
 'tree_dir': 'exp/chain/tree_a_sp',
 'use_gpu': True,
 'xent_regularize': 0.1}

- TODO KSU: incorporate YE into SOFTunisia.
- TODO: Accent ID: USe WSJ to train ivector extractor.
I looked at an older version of the kaldi recipes ro LR.
They do not use a separate corpus to train the UBM and ivector extractor.
I will use this method first.

- TODO SOFTunisia: Fix Stage 8.

* DAR <2017-08-03 Thu>
**  Goals for Thursday set Wednesday:
- TODO Accomplishments and Proposal.
- TODO GP + Yaounde: Implement Dan Povey's suggestions to improve the chain models.
I am experimenting with a smaller lm.

- SOFTunisia:
I am fixing the script that builds the tri3b models after each new batch of corrected drafts comes in from Zac.
I am removing the probabilistic dictionary from the scripts for now.

- Accent Id:
I am running the wsj script.
It had crashed yesterday.
I restarted it at stage 5.
It is now training tri4b which is tri3b with the probabilistic dictionary.

** Goals for Friday:
- TODO Accomplishments and Proposal
- TODO GP + Yaounde: Chain model tuning.
- TODO KSU: incorporate YE into SOFTunisia.
- TODO: Accent ID: USe WSJ to train ivector extractor.
- TODO SOFTunisia: Fix Stage 8.

* DAR <2017-08-02 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Mandatory Training
I did the suicide prevention training.
- TODO Proposal and Accomplishments
- TODO Accent ID: 
The plan is to start with a simple experiment.
We assume there are only 2 classes of speakers: African and European.
Given an utterance, the system will classify it as either African or European.

- DONE GP + Yaounde: Chain model results.
The chain model training finished last night.
I guess I ran the training script as a separate command and not from a script.
The script goes on to make a decoding graph and to run the decoder on the test sets.
I am running from that stage now.
%WER 35.82 [ 1143 / 3191, 88 ins, 270 del, 785 sub ] exp/chain/tdnn_sp/decode_central_accord_1/wer_10_0.0

I contacted Yenda.
I asked him for advice on what parameters to change.
He CCed Dan Povey.
Dan says I should not reduce the dimension of the hidden layers.
He says I could set the number of jobs to 1 and reduce the number of epochs.
The number of frames per iteration is not relevant.
He says he would not reduce the chunks per mini batch.
- TODO SOFTunisia: Fix Acoustic model training lists.


** Goals for Thursday:
- TODO Accomplishments and Proposal.
- TODO GP + Yaounde: Implement Dan Povey's suggestions to improve the chain models.

* DAR <2017-08-01 Tue>
**  Goals for Tuesday set Monday:
- TODO Mandatory Training.
- TODO Proposal and Accomplishments.
- TODO SOFTunisia: Fix the acoustic model training lists.
- TODO GP + Yaounde: Get chain models running.
The scripts are paranoid about overwriting files that already exist. 
This wastes a lot of time.
Yesterday evening it was the tree final.mdl file that already existed.

I've been trying to get the chain model training to run all day.
It looks like the GPU does not have enough memory for the models I am trying to train.
As I am getting ready to leave, I have the chain model training running on the GPU.
It is on iteration 68 of 144.
For the record here are arguments that I used for this run:
2017-08-01 08:15:16,450 [./steps/nnet3/chain/train.py:259 - train - INFO ] Arguments for the experiment
{'alignment_subsampling_factor': 3,
 'apply_deriv_weights': False,
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'chunk_left_context': 0,
 'chunk_left_context_initial': 0,
 'chunk_right_context': 0,
 'chunk_right_context_final': 0,
 'chunk_width': '80,50,90',
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'deriv_truncate_margin': None,
 'dir': 'exp/chain/tdnn_sp',
 'dropout_schedule': None,
 'egs_command': 'queue.pl',
 'egs_dir': None,
 'egs_opts': '--frames-overlap-per-eg 0',
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train_sp_hires',
 'final_effective_lrate': 0.0001,
 'frame_subsampling_factor': 3,
 'frames_per_iter': 2000000,
 'initial_effective_lrate': 0.001,
 'l2_regularize': 5e-05,
 'lat_dir': 'exp/chain/tri3b_train_sp_lats',
 'leaky_hmm_coefficient': 0.1,
 'left_deriv_truncate': None,
 'left_tolerance': 5,
 'lm_opts': '--num-extra-lm-states=1000',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_param_change': 2.0,
 'momentum': 0.0,
 'num_chunk_per_minibatch': '32,16,8',
 'num_epochs': 4.0,
 'num_jobs_final': 2,
 'num_jobs_initial': 1,
 'online_ivector_dir': 'exp/nnet3/ivectors_train_sp_hires',
 'preserve_model_interval': 100,
 'presoftmax_prior_scale_power': -0.25,
 'proportional_shrink': 60.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'right_tolerance': 5,
 'samples_per_iter': 400000,
 'shrink_saturation_threshold': 0.4,
 'shrink_value': 1.0,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'transform_dir': 'exp/chain/tri3b_train_sp_lats',
 'tree_dir': 'exp/chain/tree_a_sp',
 'use_gpu': True,
 'xent_regularize': 0.1}

- TODO GP: Get chain models running.
- TODO KSU: Prepare YE data.
- TODO Accent Id: prepare GP and African Accented training and test sets.
I worked a lot on this today.
I am at the point where features are being extracted from the training data.
The language recognition recipe scripts use vtln warping.
I am getting errors for this step.
I am going to try to skip vtln for now.
The language recognition recipe I am looking at uses a DNN to initialize the UBM which is later used to train an ivector extractor.
The ubm is a full covariance gmm.
Usually for tractability, GMMs have diagonal covariance matrix.
The LR recipe trains the DNN on a large corpus.
What should we do here?
Train on wsj maybe?

** Goals for Wednesday:
- TODO Mandatory Training
- TODO Proposal and Accomplishments
- TODO Accent ID: 
- TODO GP + Yaounde: Chain model results.
- TODO SOFTunisia: Fix Acoustic model training lists.
 
 * DAR <2017-07-31 Mon>
**  Goals for Monday set Friday:
- TODO Mandatory Training.
- TODO Project Proposal and Accomplishments 
-TODO GP: Get results.
Only chain model results are missing.
I am skipping the boosted mmi runs.
I am going to wait until I get the chain model running for GP + Yaounde before running chain models for GP alone.

- TODO GP + Yaounde: Chain models: get a demo running and pass it on to Mike Le.
The run failed when invoking the chain model training script.
I am knocking down the mini batch size to 32,16,8 and retrying.
Still failed.
I am now trying with number of jobs knocked down from 24 to 4.
- TODO SOFTunisia: Get rough draft to Zac with new simplified training strategy.
The 6th stage is still failing.
There are problems with the way I have been making the 4 acoustic model training lists: wav.scp, utt2txt, spk2utt and text.

- ksu: I did some work preparing the YE data.
- Accent Id: I did some work on preparing GP and African Accented data.

** Goals for Tuesday:
- TODO Mandatory Training.
- TODO Proposal and Accomplishments.
- TODO SOFTunisia: Fix the acoustic model training lists.
- TODO GP + Yaounde: Get chain models running.
- TODO GP: Get chain models running.
- TODO KSU: Prepare YE data.
- TODO Accent Id: prepare GP and African Accented training and test sets.
 
* DAR <2017-07-28 Fri>
** Goals for Friday set Thursday:
- TODO SOFTunisia: Prepare rough draft for Zac.
I worked a lot on this today.
I am going to give up on the scripts that add probabilities to the dictionary or now.
I am going to go with a simplified strategy for now.
I am not going to worry about modifying the dictionary.
I am not even going to add OOVs to the dictionary.
I have a script that adds the new transcripts to the lm and that seems to work, so I'll keep it.
Otherwise, I'm going to only add the new data and their transcripts to the next batch of training data.
I'll work on writing scripts that deal with adding OOVs to the dictionary, but this might take a while.
- TODO GP: Build system with ARL's LM and dictionary.
I am still working on getting a good script to do this.

- TODO GP: Test gp system  on niger and central accord in addition to GP eval and dev.
- TODO GP + Yaounde: Chain models?

Here are the WERs for ca16 including sgms.
the chain models are still missing.

%WER 47.23 [ 1507 / 3191, 116 ins, 370 del, 1021 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 33.69 [ 1075 / 3191, 176 ins, 202 del, 697 sub ] exp/tri3b/decode_ca16.si/wer_15_0.0
%WER 32.91 [ 1050 / 3191, 127 ins, 257 del, 666 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 31.37 [ 1001 / 3191, 135 ins, 272 del, 594 sub ] exp/tri2b/decode_ca16/wer_16_0.0
%WER 22.56 [ 720 / 3191, 125 ins, 123 del, 472 sub ] exp/tri3b/decode_ca16/wer_18_0.0
%WER 18.77 [ 599 / 3191, 106 ins, 89 del, 404 sub ] exp/sgmm4b/decode_ca16/wer_12_0.0

The script is finally getting to the chain model training step.
I only had to restart the script once.

** Goals for Monday:
- TODO Mandatory Training.
- TODO Project Proposal and Accomplishments 
-TODO GP: Get results.
- TODO GP + Yaounde: Chain models: get a demo running and pass it on to Mike Le.
- TODO SOFTunisia: Get rough draft to Zac with new simplified training strategy.

* DAR <2017-07-27 Thu>
**  Goals for Thursday set Wednesday:
- TODO GP: Build system with ARL's LM and dictionary.
I started this.
It might need more work tomorrow.
- TODO GP: Test gp system  on niger and central accord in addition to GP eval and dev.
I added decoding for niger and ca16.
Probably will need more work tomorrow.
- TODO GP + Yaounde: Check why results are strange.
This was a false alarm.
The tri3b ca16 score I was concerned about was for a speaker  independent model set.
I now know that the tri3b (lda mllt sat) systems use 2 passes of decoding.
I really do not think  that tri3b is a feasible model for s2s apps.

Here are the results from yesterday:
%WER 54.28 [ 4342 / 7999, 210 ins, 1208 del, 2924 sub ] exp/mono/decode_niger/wer_11_0.0
%WER 53.65 [ 11963 / 22297, 705 ins, 1984 del, 9274 sub ] exp/mono/decode_dev/wer_9_0.5
%WER 51.00 [ 16773 / 32888, 1337 ins, 2718 del, 12718 sub ] exp/mono/decode_test/wer_9_0.0
%WER 49.46 [ 10732 / 21698, 648 ins, 1665 del, 8419 sub ] exp/mono/decode_gp_eval/wer_9_0.5
%WER 47.23 [ 1507 / 3191, 116 ins, 370 del, 1021 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 45.79 [ 3663 / 7999, 226 ins, 1230 del, 2207 sub ] exp/tri1/decode_niger/wer_18_0.0
%WER 43.13 [ 3450 / 7999, 221 ins, 1202 del, 2027 sub ] exp/tri2b/decode_niger/wer_16_0.0
%WER 42.67 [ 3413 / 7999, 281 ins, 1022 del, 2110 sub ] exp/tri3b/decode_niger.si/wer_16_0.0
%WER 36.72 [ 12077 / 32888, 999 ins, 1998 del, 9080 sub ] exp/tri1/decode_test/wer_13_0.5
%WER 36.26 [ 11925 / 32888, 1126 ins, 1814 del, 8985 sub ] exp/tri3b/decode_test.si/wer_14_0.5
%WER 35.94 [ 8013 / 22297, 804 ins, 685 del, 6524 sub ] exp/tri3b/decode_dev.si/wer_12_1.0
%WER 35.59 [ 7935 / 22297, 636 ins, 801 del, 6498 sub ] exp/tri1/decode_dev/wer_11_1.0
%WER 35.29 [ 11606 / 32888, 1007 ins, 1989 del, 8610 sub ] exp/tri2b/decode_test/wer_16_0.0
%WER 34.53 [ 7699 / 22297, 621 ins, 738 del, 6340 sub ] exp/tri2b/decode_dev/wer_12_1.0
%WER 34.08 [ 2726 / 7999, 168 ins, 924 del, 1634 sub ] exp/tri3b/decode_niger/wer_19_0.5
%WER 33.89 [ 7353 / 21698, 740 ins, 485 del, 6128 sub ] exp/tri3b/decode_gp_eval.si/wer_10_1.0
%WER 33.69 [ 1075 / 3191, 176 ins, 202 del, 697 sub ] exp/tri3b/decode_ca16.si/wer_15_0.0
%WER 33.28 [ 7222 / 21698, 647 ins, 528 del, 6047 sub ] exp/tri1/decode_gp_eval/wer_9_1.0
%WER 32.91 [ 1050 / 3191, 127 ins, 257 del, 666 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 32.87 [ 7330 / 22297, 732 ins, 539 del, 6059 sub ] exp/tri3b/decode_dev/wer_12_0.5
%WER 32.27 [ 7001 / 21698, 520 ins, 570 del, 5911 sub ] exp/tri2b/decode_gp_eval/wer_12_1.0
%WER 31.37 [ 1001 / 3191, 135 ins, 272 del, 594 sub ] exp/tri2b/decode_ca16/wer_16_0.0
%WER 31.19 [ 10258 / 32888, 748 ins, 1614 del, 7896 sub ] exp/tri3b/decode_test/wer_15_1.0
%WER 30.95 [ 6715 / 21698, 510 ins, 495 del, 5710 sub ] exp/tri3b/decode_gp_eval/wer_13_1.0
%WER 22.56 [ 720 / 3191, 125 ins, 123 del, 472 sub ] exp/tri3b/decode_ca16/wer_18_0.0

The run on the gpu machine is still doing sgmm training.
If this is not relevant to transapps, I think we should drop it. It takes up a lot of time.

** Goals for Friday:
- TODO SOFTunisia: Prepare rough draft for Zac.
- TODO GP: Build system with ARL's LM and dictionary.
- TODO GP: Test gp system  on niger and central accord in addition to GP eval and dev.
- TODO GP + Yaounde: Chain models?

* DAR <2017-07-26 Wed>
**  Goals for Wednesday set Tuesday:
- TODO gp: Run end to end.
This did not finish yet.
As I am getting ready to leav, the build is almost at the point where it starts the chain model training.
It is running lattice alignment to get alternat pronunciations.
This means it has finished doing ivector extraction.

I will restart this build when the current one is finished.
I am going to give up on trying to reproduce the kaldi results.
Instead I am going to run the gp build with the same lm as the one I'm using for the gp+yaounde build.
I just checked, and I am not including the dev and eval prompts in our dictionary, only the training prompts.

- TODO gp + yaounde: Run end to end.
This build has not finished either.
for the record, here are the currently available results:
%WER 54.28 [ 4342 / 7999, 210 ins, 1208 del, 2924 sub ] exp/mono/decode_niger/wer_11_0.0
%WER 53.65 [ 11963 / 22297, 705 ins, 1984 del, 9274 sub ] exp/mono/decode_dev/wer_9_0.5
%WER 51.00 [ 16773 / 32888, 1337 ins, 2718 del, 12718 sub ] exp/mono/decode_test/wer_9_0.0
%WER 49.46 [ 10732 / 21698, 648 ins, 1665 del, 8419 sub ] exp/mono/decode_gp_eval/wer_9_0.5
%WER 47.23 [ 1507 / 3191, 116 ins, 370 del, 1021 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 45.79 [ 3663 / 7999, 226 ins, 1230 del, 2207 sub ] exp/tri1/decode_niger/wer_18_0.0
%WER 43.13 [ 3450 / 7999, 221 ins, 1202 del, 2027 sub ] exp/tri2b/decode_niger/wer_16_0.0
%WER 36.72 [ 12077 / 32888, 999 ins, 1998 del, 9080 sub ] exp/tri1/decode_test/wer_13_0.5
%WER 35.59 [ 7935 / 22297, 636 ins, 801 del, 6498 sub ] exp/tri1/decode_dev/wer_11_1.0
%WER 35.29 [ 11606 / 32888, 1007 ins, 1989 del, 8610 sub ] exp/tri2b/decode_test/wer_16_0.0
%WER 34.53 [ 7699 / 22297, 621 ins, 738 del, 6340 sub ] exp/tri2b/decode_dev/wer_12_1.0
%WER 33.69 [ 1075 / 3191, 176 ins, 202 del, 697 sub ] exp/tri3b/decode_ca16.si/wer_15_0.0
%WER 33.28 [ 7222 / 21698, 647 ins, 528 del, 6047 sub ] exp/tri1/decode_gp_eval/wer_9_1.0
%WER 32.91 [ 1050 / 3191, 127 ins, 257 del, 666 sub ] exp/tri1/decode_ca16/wer_16_0.0
%WER 32.27 [ 7001 / 21698, 520 ins, 570 del, 5911 sub ] exp/tri2b/decode_gp_eval/wer_12_1.0
%WER 31.37 [ 1001 / 3191, 135 ins, 272 del, 594 sub ] exp/tri2b/decode_ca16/wer_16_0.0

This looks different than what I got yesterday.
The WER for tri3b on the ca16 set is different.
Yesterday is was %WER 22.94 [ 732 / 3191, 130 ins, 133 del, 469 sub ] exp/tri3b/decode_ca16/wer_20_0.0
Today it is: %WER 33.69 [ 1075 / 3191, 176 ins, 202 del, 697 sub ] exp/tri3b/decode_ca16.si/wer_15_0.0
There must be something wrong.

I'll have to look into this tomorrow.

False alarm, those are the speaker independent results.
I am assuming si stands for speaker independent.
I do not have the results for the sat tri3b decoding of ca16 yet.


 
- TODO Write report on findings.


** Goals for Thursday:
- TODO GP: Build system with ARL's LM and dictionary.
- TODO GP: Test gp system  on niger and central accord in addition to GP eval and dev.
- TODO GP + Yaounde: Check why results are strange.

* DAR <2017-07-25 Tue>
**  Goals for Tuesday set Monday:
- DONE Mandatory Training (Anti Terrorism level I)
I attended the Alcohol and Drug Abuse Awareness training.

- TODO GP: run system build to reproduce kaldi results in separate directory.
- TODO GP+Yaounde: ditto
niger:
Total number of utterances: 986
Total number of hours: 1.0

Here are results from yesterday's run. I'm not sure the correct lm was used.
%WER 53.91 [ 4312 / 7999, 214 ins, 1242 del, 2856 sub ] exp/mono/decode_niger/wer_12_0.0
%WER 45.39 [ 3631 / 7999, 240 ins, 998 del, 2393 sub ] exp/tri1/decode_niger/wer_19_0.0
%WER 44.61 [ 3568 / 7999, 310 ins, 992 del, 2266 sub ] exp/tri3b/decode_niger.si/wer_19_0.0
%WER 42.14 [ 3371 / 7999, 153 ins, 1118 del, 2100 sub ] exp/tri2b/decode_niger/wer_19_1.0
%WER 35.12 [ 2809 / 7999, 223 ins, 816 del, 1770 sub ] exp/tri3b/decode_niger/wer_20_0.5
%WER 30.97 [ 2477 / 7999, 178 ins, 847 del, 1452 sub ] exp/sgmm4b/decode_niger/wer_15_0.0
%WER 30.34 [ 2427 / 7999, 140 ins, 806 del, 1481 sub ] exp/chain/tdnnarl_sp/decode_niger/wer_12_0.0

- note: chain models were the best.

CA16:
Here are results from yesterday's run:
%WER 48.79 [ 1557 / 3191, 101 ins, 389 del, 1067 sub ] exp/mono/decode_ca16/wer_12_0.0
%WER 35.22 [ 1124 / 3191, 103 ins, 303 del, 718 sub ] exp/tri1/decode_ca16/wer_15_0.5
%WER 34.88 [ 1113 / 3191, 144 ins, 257 del, 712 sub ] exp/tri3b/decode_ca16.si/wer_20_0.0
%WER 31.06 [ 991 / 3191, 139 ins, 219 del, 633 sub ] exp/tri2b/decode_central_accord/wer_15_0.0
%WER 27.61 [ 881 / 3191, 94 ins, 232 del, 555 sub ] exp/chain/tdnnarl_sp/decode_ca16/wer_12_0.0
%WER 22.94 [ 732 / 3191, 130 ins, 133 del, 469 sub ] exp/tri3b/decode_ca16/wer_20_0.0
%WER 20.28 [ 647 / 3191, 95 ins, 179 del, 373 sub ] exp/sgmm4b_mmi_b0.1/decode_ca16_it4/wer_16_0.0
%WER 19.87 [ 634 / 3191, 92 ins, 175 del, 367 sub ] exp/sgmm4b_mmi_b0.1/decode_ca16_it3/wer_16_0.0
%WER 19.12 [ 610 / 3191, 90 ins, 155 del, 365 sub ] exp/sgmm4b_mmi_b0.1/decode_ca16_it2/wer_16_0.0
%WER 18.80 [ 600 / 3191, 109 ins, 102 del, 389 sub ] exp/sgmm4b_mmi_b0.1/decode_ca16_it1/wer_11_0.0
%WER 18.36 [ 586 / 3191, 86 ins, 125 del, 375 sub ] exp/sgmm4b/decode_ca16/wer_12_0.5

- Observation:
The sgmm models blow away the chain models on the ca16 set.

test set	sgmm	chain
ca16	18.36	27.61
niger	30.94	30.34
gp+niger+ca16	28.44	28.90

The chain and sgmm models are very close on the niger test set and the combination of the gp, niger and ca16 test set.
What about on the gp eval and dev?

I want to run this build all over again just to be sure about the lm I am using.

- TODO Investigate Babel multilang recipes.
- TODO KSU: Add Yemen speakers.

** Goals for Wednesday:
- TODO gp: Run end to end.
- TODO gp + yaounde: Run end to end.
- TODO Write report on findings.

* <2017-07-24 Mon>
** Goals for Monday set Friday:
- TODO Mandatory Training (508 compliant Anti Terrorism Level I)
- TODO GP : Reproduce as close as possible kaldi results.

system build details: 
training data:
Total number of utterances: 8818
total hours of data: 22.7 

Eval:
total number of utterances: 821
Total number of hours: 2.0

WER:
%WER 47.65 [ 10340 / 21698, 688 ins, 1513 del, 8139 sub ] exp/gp/mono/decode_eval/wer_9_0.5
%WER 35.17 [ 7631 / 21698, 803 ins, 496 del, 6332 sub ] exp/gp/tri1/decode_eval/wer_12_0.0
%WER 34.30 [ 7442 / 21698, 727 ins, 536 del, 6179 sub ] exp/gp/tri2b/decode_eval/wer_16_0.0

Dev:
total number of utterances: 839
Total number of hours: 2.1

WER:
%WER 51.55 [ 11494 / 22297, 714 ins, 1880 del, 8900 sub ] exp/gp/mono/decode_dev/wer_9_0.5
%WER 37.14 [ 8280 / 22297, 791 ins, 840 del, 6649 sub ] exp/gp/tri1/decode_dev/wer_14_0.0

I am going to rerun this system build and evaluation.
The problem is I am not sure what lm is being used.
I ran this build in the same directory with the gp+yaounde system build and it uses our lm.
I need to run these 2 system builds in separate directories.

- TODO GP +Yaounde: Compare chain model results with sgmm results.
Results:
model	test
mono	51.14
tri1	38.43
tri2b(lda+mllt)		35.85
tri3b.si(lda+mllt+sat speaker independent)	37.77 
tri3b(lda+mllt+sat)	32.29
chain	28.90
sgmm	28.44 

The subspace gaussian mixture models were better than the chain models.

Here are all the results:
%WER 54.02 [ 12045 / 22297, 714 ins, 2102 del, 9229 sub ] exp/mono/decode_dev/wer_9_0.5
%WER 51.14 [ 16820 / 32888, 964 ins, 3433 del, 12423 sub ] exp/mono/decode_test/wer_9_0.5
%WER 38.43 [ 12640 / 32888, 991 ins, 2098 del, 9551 sub ] exp/tri1/decode_test/wer_14_0.5
%WER 37.77 [ 12423 / 32888, 1302 ins, 1718 del, 9403 sub ] exp/tri3b/decode_test.si/wer_13_0.5
%WER 35.85 [ 11790 / 32888, 1034 ins, 1786 del, 8970 sub ] exp/tri2b/decode_test/wer_14_0.5
%WER 32.29 [ 10621 / 32888, 954 ins, 1459 del, 8208 sub ] exp/tri3b/decode_test/wer_16_0.5
%WER 29.71 [ 9770 / 32888, 735 ins, 1654 del, 7381 sub ] exp/sgmm4b_mmi_b0.1/decode_test_it4/wer_12_1.0
%WER 29.48 [ 9695 / 32888, 810 ins, 1501 del, 7384 sub ] exp/sgmm4b_mmi_b0.1/decode_test_it3/wer_12_0.5
%WER 29.34 [ 6541 / 22297, 322 ins, 886 del, 5333 sub ] exp/chain/tdnnarl_sp/decode_dev/wer_9_0.0
%WER 29.17 [ 9595 / 32888, 736 ins, 1541 del, 7318 sub ] exp/sgmm4b_mmi_b0.1/decode_test_it2/wer_11_1.0
%WER 28.90 [ 9504 / 32888, 515 ins, 1737 del, 7252 sub ] exp/chain/tdnnarl_sp/decode_test/wer_9_0.5
%WER 28.74 [ 9451 / 32888, 743 ins, 1385 del, 7323 sub ] exp/sgmm4b_mmi_b0.1/decode_test_it1/wer_10_1.0
%WER 28.44 [ 9354 / 32888, 704 ins, 1384 del, 7266 sub ] exp/sgmm4b/decode_test/wer_11_1.0

Here are specific details about the system build:
training data:
Total number of utterances: 15117
utterances from gp: 8818 
utterances from yaounde: 6299 
Total number of hours:  31.2

Test:
total number of utterances: 2322
Number of utterances from gp: 821
number of utterances from niger: 986
Number of Utterances from CA16: 515
Total number of hours: 3.3

- Note:
I ran the gp+yaounde monosystem on the dev set.
The dev set is made up of only gp utterances.
The WER was  54.02
When I ran the gp mono system on the same set I got 51.55

gp	gp+yaounde
51.55	54.02

So the Yaounde data hurts the gp system by 2.5 percent.

I am also going to rerun this system build and evaluation so I can be sure about which lm is being used.

CA16:
Total number of utterances: 515
Total number of hours:  0.3 
%WER 48.79 [ 1557 / 3191, 101 ins, 389 del, 1067 sub ] exp/mono/decode_ca16/wer_12_0.0

I need to run the gp system on the larger test set and the gp+yaounde system on the gp eval set.

- TODO Investigate Babel multilang recipes.
- TODO KSU: Add Yemen speakers.

** Goals for Tuesday:
- TODO Mandatory Training (Anti Terrorism level I)
- TODO GP: run system build to reproduce kaldi results in separate directory.
- TODO GP+Yaounde: ditto
- TODO Investigate Babel multilang recipes.
- TODO KSU: Add Yemen speakers.

* DAR <2017-07-21 Fri>
** Goals set Wednesday: 
- DONE Mandatory training.
I read the Constitution Day (508 Version) mandatory training. I'm ready to take the BAR exam!
- TODO GP + Yaounde: Share models (tri3b and chain) with Transapps.
I have decided to reproduce the results published in the kaldi gp recipe for fr.
I do not have the lexicon at the moment, so I cannot reproduce the exact same experiment.
My first attemp:
Mono:
%WER 47.59 [ 10325 / 21698, 940 ins, 1196 del, 8189 sub ] exp/gp/mono/decode_eval/wer_9_0.0
The results in the kaldi recipe:
Mono:
%WER 45.16 [ 10073 / 22306, 684 ins, 2010 del, 7379 sub ] exp/FR/mono/decode_dev_tgpr_sri/wer_8


- TODO Babel: Investigate multilang training.

- SOFTunisia: 
%WER 60.25 [ 3712 / 6161, 174 ins, 421 del, 3117 sub ] exp/mono/decode_westpoint_native_eval/wer_12_1.0


** Goals for Monday:
- TODO Mandatory Training (508 compliant Anti Terrorism Level I)
- TODO GP : Reproduce as close as possible kaldi results.
- TODO GP +Yaounde: Compare chain model results with sgmm results.
- TODO Investigate Babel multilang recipes.
- TODO KSU: Add Yemen speakers.

* DAR <2017-07-20 Thu>
** Goals for Thursday set Wednesday:
- DONE Mandatory training.
I read the Constitution Day (508 Version) mandatory training. I'm ready to take the BAR exam!
- TODO GP + Yaounde: Share models (tri3b and chain) with Transapps.
- TODO Babel: Investigate multilang training.

* DAR <2017-07-19 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Mandatory Training:
I am reading the 508 compliant version of Constitution Day training.
This is a 131 page document.
Does everyone actually do this training?
- TODO GP + Yaounde: Get chain models working.
The training failed on iteration 7.
The training is suppposed to run for 41 iterations.
Training finished.
Decoding now.

Here are the results including chain models:
%WER 54.02 [ 12045 / 22297, 714 ins, 2102 del, 9229 sub ] exp/mono/decode_dev/wer_9_0.5
%WER 51.14 [ 16820 / 32888, 964 ins, 3433 del, 12423 sub ] exp/mono/decode_test/wer_9_0.5
%WER 38.43 [ 12640 / 32888, 991 ins, 2098 del, 9551 sub ] exp/tri1/decode_test/wer_14_0.5
%WER 37.77 [ 12423 / 32888, 1302 ins, 1718 del, 9403 sub ] exp/tri3b/decode_test.si/wer_13_0.5
%WER 35.85 [ 11790 / 32888, 1034 ins, 1786 del, 8970 sub ] exp/tri2b/decode_test/wer_14_0.5
%WER 32.29 [ 10621 / 32888, 954 ins, 1459 del, 8208 sub ] exp/tri3b/decode_test/wer_16_0.5
%WER 29.34 [ 6541 / 22297, 322 ins, 886 del, 5333 sub ] exp/chain/tdnnarl_sp/decode_dev/wer_9_0.0
%WER 28.90 [ 9504 / 32888, 515 ins, 1737 del, 7252 sub ] exp/chain/tdnnarl_sp/decode_test/wer_9_0.5

I guess I should train sgmm and other model types.
 
- ToDO SOFTunisia: Run cd gmm hmm with Zac's Answers from CTELLFIVE.
This is more involved than I thought originally.
I have to add code to my scripts that processs the training data.
My previous scripts only assumed that I would be training on files containing Recordings_Arabic in their names.

- TODO SOFTunisia: Produce rough draft transcripts for next chunk of Answers.
- TODO Babel: Move from cd gmm hmm to babel multilang.
The tri5 training is finished for Lithuanian.
Now what?

There is a multilang script under nnet2.
Here is the comment at the beginning of the script:
# train_multilang2.sh is for multi-language training of neural nets.  It
# takes multiple egs directories which must be created by get_egs2.sh, and the
# corresponding alignment directories (only needed for training the transition
# models).

# for the n languages, we share all the hidden layers but there are separate
# final layers.  On each iteration of training we average the hidden layers
# across all jobs of all languages, but average the parameters of the final,
# output layer only within each language.  The script starts from a partially
# trained model from the first language (language 0 in the directory-numbering
# scheme).  See egs/rm/s5/local/online/run_nnet2_wsj_joint.sh for example.
#
# This script requires you to supply a neural net partially trained for the 1st
# language, by one of the regular training scripts, to be used as the initial
# neural net (for use by other languages, we'll discard the last layer); it
# should not have been subject to "mix-up" (since this script does mix-up), or
# combination (since it would increase the parameter range to a too-large value
# which isn't compatible with our normal learning rate schedules).

Here is the comment at the top of the rm recipe referenced above:
# This is the latest version of training that combines RM and WSJ, in a setup where
# there are no shared phones (so it's like a multilingual setup).
# Before running this script, go to ../../wsj/s5, and after running
# the earlier stages in the run.sh (so the baseline SAT system is built),
# run the following:
#
# local/online/run_nnet2.sh --stage 8 --dir exp/nnet2_online/nnet_ms_a_partial --exit-train-stage 15
#
# (you may want to keep --stage 8 on the above command line after run_nnet2.sh,
# in case you already ran some scripts in local/online/ in ../../wsj/s5/ and
# the earlier stages are finished, otherwise remove it).

** Goals for Thursday:
- TODO Mandatory training.
- TODO GP + Yaounde: Share models (tri3b and chain) with Transapps.
- TODO Babel: Investigate multilang training.

* DAR <2017-07-18 Tue>
** Goals for Tuesday set Monday:
- TODO Mandatory Training:
- TODO SOFTunisia: Test on the cleaned ksu data.
Here are the WER results for each model set up to tri3b:
find -f -name "best_wer" | xargs cat | grep WER | sort -gr
%WER 89.14 [ 1831 / 2054, 38 ins, 435 del, 1358 sub ] exp/mono/decode_test/wer_10_0.5
%WER 87.73 [ 1802 / 2054, 24 ins, 455 del, 1323 sub ] exp/tri1/decode_test/wer_16_1.0
%WER 86.76 [ 1782 / 2054, 44 ins, 401 del, 1337 sub ] exp/tri3b/decode_eval/wer_17_1.0
%WER 85.98 [ 1766 / 2054, 63 ins, 378 del, 1325 sub ] exp/tri2a/decode_test/wer_16_0.5
%WER 85.54 [ 1757 / 2054, 44 ins, 382 del, 1331 sub ] exp/tri2b/decode_test/wer_17_1.0

- DONE SOFTunisia: Get the cd gmm hmm build and ksu test set running on the GPU machine.
I have it running, but results are even worse than on the laptop:
%WER 93.97 [ 1635 / 1740, 25 ins, 183 del, 1427 sub ] exp/mono/decode_test/wer_12_1.0
I think I might be using an older version of the data or a corrupted version.

I am rerunning The cd gmm hmm build with Zac's corrections to the the Answers from CTELLFIVE.

- TODO GP + Yaounde: Build the system similar to what Transaps is using.
I have chain models building now.
The build fails on the second iteration.
I think the problem is with memory on the GPU.
I am rerunning the chain model training with the following change:
Before:
--trainer.num-chunk-per-minibatch=256,128,64
After:
--trainer.num-chunk-per-minibatch=128,64,32

Tomorrow I'll know if this works.

- Babel: I want to run the babel_multilang recipe.
It looks like before you run babel_multilang you need to run babel.
I actually have the babel cd gmm hmm build running now.
Yenda just answered me, and yes, the babel system needs to be guilt first.
** Goals for Wednesday:
- TODO Mandatory Training:
- TODO GP + Yaounde: Get chain models working.
- ToDO SOFTunisia: Run cd gmm hmm with Zac's Answers from CTELLFIVE.
- TODO SOFTunisia: Produce rough draft transcripts for next chunk of Answers.
- TODO Babel: Move from cd gmm hmm to babel multilang.

* DAR <2017-07-17 Mon>
** Goals for Next Week:
- TODO Mandatory Training.

- DONE KSU: prepare  the data from tunisian speakers for a good test set.
Zac took a look at the transcripts and recordings.
As I feared, the  recordins with questions in the file names are not transcripts of what the speaker said. They are the questions used to prompt answers.
Everything else looks like transcripts of recited speech.
I got rid of English digits and punctuation. 
- TODO SOFTunisia: Build  a cd gmm only on native training data.
- TODO SOFTunisia: Test the cd gmm hmm system on the new KSU Tunisian test set.
Heere are the WER results for the first test using monophones:
%WER 89.14 [ 1831 / 2054, 38 ins, 435 del, 1358 sub ] exp/mono/decode_test/wer_10_0.5
Better transcripts should improve this, but it is what it is.
I guess there is a lot of room for improvement.

** Goals for Tuesday:
- TODO Mandatory Training:
- TODO SOFTunisia: Test on the cleaned ksu data.
- TODO SOFTunisia: Get the cd gmm hmm build and ksu test set running on the GPU machine.
- TODO GP + Yaounde: Build the system similar to what Transaps is using.

* DAR <2017-07-13 Thu>
**  Goals for Thursday set Wednesday:
- TODO Mandatory Training.
- TODO SofTunisia: Change to only train and evaluate on native data.
Monophone scores went down slightly, but not as much as I expected. 
%WER 47.30 [ 2914 / 6161, 206 ins, 224 del, 2484 sub ] exp/mono/decode_eval/wer_10_1.0

I need a Tunisian test set.
What about the Tunisians from the KSU corpus?
There are 4 Tunisians in the KSU corpus.
NS26
NS152
NS155
NS156

I spent the whole day preparing the KSU Tunisian speaker data.
I end up with 512 utterances.
It looks right now like I have transcripts for each utterance.
I need an Arabic speaker to verify that the transcripts correspond to the utterances.

- TODO GP + Yaounde: Improve cd gmm hmm results before moving on to chain models.

** Goals for Friday:
- TODO Mandatory Training.
- TODO Incorporate KSU Tunisian speakers into SofTunisia as test data.

* DAR <2017-07-12 Wed>
** Goals for Wednesday set Tuesday:
- TODO Mandatory training.
- TODO SofTunisia: Chain models. Why are alignments made with tri3b models and with speed perturbed data?nes
I found that the WERs were higher for SOFTunisia.
For example here are results for my most recent run on mono and tri1:
%WER 60.15 [ 5687 / 9454, 341 ins, 519 del, 4827 sub ] exp/mono/decode_eval/wer_12_1.0
%WER 57.82 [ 5466 / 9454, 400 ins, 389 del, 4677 sub ] exp/tri1/decode_eval/wer_17_1.0

These are much worse than before.
What is going on?
I suspect the addition of the Westpoint data. 
I looked at my scripts and found that indeed I am including the nonnative Westpoint data in my training.
I am also including it in my eval and dev sets.
Now I am going to run it with only native data from both SOFTunisia and Westpoint and evaluate only on native Westpoint.
Looking on the bright side, I feel like the work I put into separating the data with scripts is paying off.
I just have to make minor changes to simple scripts to rebuild systems with only native data.

There seem to be problems with the script that builds i-vectors.
I'm going to copy the list of steps  and go through them again:
List of steps for building chain models:
A utt2dur file is written.
A duration for each utterance.
wav-to-duration.cc:92) Mean duration was 4.12628, min and max durations were 1.21725, 19.2263
I don't think this utt2dur file gets used, it only gets written because of the speed perturbation that follows.
Speed and volume perturbed versions of the training data are created with sox.
speed factors:
0.9
1.1
The speed perturbed versions are combined.
We end up with over 42k utterances.
The new list files are written to data/train_sp

A copy of the data/train_sp is made and named data/train_sp_hires
hires stands for high resolution.
High resolution mfcc features are extracted from this data.
Here is the config file for the mfcc hires extraction:
# config for high-resolution MFCC features, intended for neural network training
# Note: we keep all cepstra, so it has the same info as filterbank features,
# but MFCC is more easily compressible (because less correlated) which is why 
# we prefer this method.
--use-energy=false   # use average of log energy, not energy.
--num-mel-bins=40     # similar to Google's setup.
--num-ceps=40     # there is no dimensionality reduction.
--low-freq=20     # low cutoff frequency for mel bins... this is high-bandwidth data, so
                  # there might be some information at the low end.
--high-freq=-400 # high cutoff frequently, relative to Nyquist of 8000 (=7600) 

Note that the mfcc files are written to a default directory:
data/train_sp_hires/data

Volume perturbation is performed on the high resolution data.
cmvn stats are also computed for the high resolution data.
Selection of segments is performed.
Segments in the high resolution data that were already in the original data. 
I do not understand this.
We end up with 14k utterances -- probably the same original utterances before the perturbation.
It looks like the reason we generate the high resolution data is to train a lda + mllt transform that will produce the diagonal GMM for the diagonal UBM.
Here is data printed after the lda+mllt transform is computed:
exp/nnet3/tri5: nj=4 16.18h data log-like=-51.21 states=2308 gauss=4400 tree-impr=5.19 lda-sum=18.28 mllt:impr,logdet=1.13,1.48
Notice that only 18 hours of dta is being used.

The diagonal UBM is trained.
The diagonal UBM is trained on a quarter of the training data.
The UBM is trained to have 512 gaussians.
The UBM is one big gmm.
4 passes of training are performed.

The ivector extractor is trained.
the ivector extractor is trained on all the training data.
The ivectors have a default dimension of 100.

Ivectors are extracted online. 
Ivectors are extracted from the speed perturbed high resolution training data.
There is a strange data setup at this point.
Utterances are put into pairs.
Each pair is considered a separate speaker.
Apparently, i-vector extraction  works better when there are many speakers?
There end up being many diverse i-vectors.
The ivector is set to 0 at the beginning of each pseudo speaker (utterance pair).
Luckily, there are scripts that do all this.
Here is interesting trace: 
utils/data/modify_speaker_info.sh: copied data from data/train_sp_hires to exp/nnet3/ivectors_train_sp_hires/train_sp_hires_max2, number of speakers changed from 555 to 21546
21k speakers.

Ivectors are extracted from the non-perturbed test data.

Data is speed perturbed again but at low resolution.
Alignments are produced on the speed perturbed low resolution training data.
We used the tri3b models to do alignment, that is why we did low resolution speed perturbation. 

A file specifying the model topology is written.
Apparently, chain models have a special topology.
The alignments are reproduced as lattices. 
The tree is rebuilt.
The config file for chain mdoel training is written.

Alignment lattices are generated with alternate pronunciations.
A tree is built.
The config file for chain model training is written.
A phone language model is created.


- GP + Yaounde: Improve the cd gmm hmm WER results before moving on to chain models.

** Goals for Thursday:
- TODO Mandatory Training.
- TODO SofTunisia: Change to only train and evaluate on native data.
- TODO GP + Yaounde: Improve cd gmm hmm results before moving on to chain models.

* DAR <2017-07-11 Tue>
** Goals for Tuesday set Monday:
- TODO Mandatory Training: Constitution Day Training
- TODO GP + Yaounde: Chain Models.
List of steps for building chain models:
A utt2dur file is written.
A duration for each utterance.
wav-to-duration.cc:92) Mean duration was 4.12628, min and max durations were 1.21725, 19.2263
I don't think this utt2dur file gets used, it only gets written because of the speed peertubation that follows.
Speed and volume perturbed versions of the training data are created with sox.
speed factors:
0.9
1.1
The speed perturbed versions are combined.
We end up with over 42k utterances.
The new files are written to data/train_sp

A copy of the data/train_sp is made and named data/train_sp_hires
hires stands for high resolution.
High resolution mfcc features are extracted from this data.
Here is the config file for the mfcc hires extraction:
# config for high-resolution MFCC features, intended for neural network training
# Note: we keep all cepstra, so it has the same info as filterbank features,
# but MFCC is more easily compressible (because less correlated) which is why 
# we prefer this method.
--use-energy=false   # use average of log energy, not energy.
--num-mel-bins=40     # similar to Google's setup.
--num-ceps=40     # there is no dimensionality reduction.
--low-freq=20     # low cutoff frequency for mel bins... this is high-bandwidth data, so
                  # there might be some information at the low end.
--high-freq=-400 # high cutoff frequently, relative to Nyquist of 8000 (=7600) 

Volume perturbation is performed on the high resolution data.
cmvn stats are also computed for the high resolution data.
Selection of segments is performed.
Segments in the high resolution data that were already in the original data. 
I do not understand this.
We end up with 14k utterances -- probably the same original utterances before the perturbation.
It looks like the reason we generate the high resolution data is to train a lda + mllt transform that will produce the diagonal GMM for the diagonal UBM.
Here is data printed after the lda+mllt transform is computed:
exp/nnet3/tri5: nj=4 16.18h data log-like=-51.21 states=2308 gauss=4400 tree-impr=5.19 lda-sum=18.28 mllt:impr,logdet=1.13,1.48
Notice that only 18 hours of dta is being used.

The diagonal UBM is trained.
The diagonal UBM is trained on a quarter of the training data.
The UBM is trained to have 512 gaussians.
The UBM is one big gmm.
4 passes of training are performed.

The ivector extractor is trained.
the ivector extractor is trained on all the training data.
The ivectors have a default dimension of 100.

Ivectors are extracted online. 
Ivectors are extracted from the speed perturbed high resolution training data.
There is a strange data setup at this point.
Utterances are put into pairs.
Each pair is considered a separate speaker.
Apparently, i-vector extraction  works better when there are many speakers?
There end up being many diverse i-vectors.
The ivector is set to 0 at the beginning of each pseudo speaker (utterance pair).
Luckily, there are scripts that do all this.
Here is interesting trace: 
utils/data/modify_speaker_info.sh: copied data from data/train_sp_hires to exp/nnet3/ivectors_train_sp_hires/train_sp_hires_max2, number of speakers changed from 555 to 21546
21k speakers.

Ivectors are extracted from the non-perturbed test data.

Data is speed perturbed again but at low resolution.
Alignments are produced on the speed perturbed low resolution training data.
We used the tri3b models to do alignment, that is why we did low resolution speed perturbation. 

A file specifying the model topology is written.
Apparently, chain models have a special topology.
The alignments are reproduced as lattices. 
The tree is rebuilt.
The config file for chain mdoel training is written.

Alignment lattices are generated with alternate pronunciations.
A tree is built.
The config file for chain model training is written.
A phone language model is created.


- TODO SofTunisia:  Chain models.

** Goals for Wednesday:
- TODO Mandatory training.
- TODO SofTunisia: Chain models. Why are alignments made with tri3b models and with speed perturbed data?
- GP + Yaounde: Improve the cd gmm hmm WER results before moving on to chain models.

* DAR <2017-07-10 Mon>
**  Goals for Monday set Friday:
- TODO Mandatory Training.
- TODO GP Yaounde: Fix LM Rescore command line.
lmrescoring requires more than 1 lm.
I do not have another lm right now, so I'm going to skip lmrescoring.

Here are the results for decoding the test set with the cd gmm hmm model sets:
find exp -type f -name "best_wer" | xargs cat | grep WER | sort -gr
%WER 75.17 [ 24723 / 32888, 923 ins, 5496 del, 18304 sub ] exp/mono/decode_test/wer_13_0.5
%WER 45.50 [ 14964 / 32888, 1033 ins, 3036 del, 10895 sub ] exp/tri3b/decode_test/wer_16_0.5
%WER 39.51 [ 12993 / 32888, 978 ins, 2274 del, 9741 sub ] exp/tri1/decode_test/wer_14_0.5
%WER 37.13 [ 12211 / 32888, 978 ins, 2021 del, 9212 sub ] exp/tri2b/decode_test/wer_14_0.5

The best results are with the tri2b models.
I do not understand why the tri3b speaker adapted models are not better than the tri2b mllt lda models.
Even the tri1 models are better than the tri3b models.

I am moving on to chain model training.

Speed and volume perturbed versions of the training data are created with sox.
The sp affix stands for speed perturbed.
sp used to stand for short pause.
High resolution mfcc features are extracted from this data.
A system is trained on the high resolution data to get an lda+mllt  transform.
The lda+mllt transform is used to produce the diagonal GMM.
The diagonal UBM is trained.
The ivector extractor is trained.
Ivectors are extracted online. on the speed perturbed high resoluiton training data
Ivectors are extracted from the non-perturbed test data.
Data is speed perturbed again but at low resolution.
For alignment?
Alignments are produced on the speed perturbed low resolution training data.
I file specifying the model topology is written.
Apparently, chain models have a special topology.
The alignments are reproduced as lattices. 
The tree is rebuilt.
The config file for chain mdoel training is written.



- Softunisia:
I am running similar stages for SOFTunisia and gp + Yaounde.
Here are cd gmm hmm results for SOFTunisia:

%WER 39.12 [ 34670 / 88622, 2153 ins, 4785 del, 27732 sub ] exp/mono/decode_train/wer_10_0.5
%WER 34.45 [ 30527 / 88622, 1735 ins, 3780 del, 25012 sub ] exp/tri1/decode_train/wer_13_1.0
%WER 27.52 [ 24391 / 88622, 1794 ins, 2407 del, 20190 sub ] exp/tri3b/decode_train/wer_15_1.0
%WER 25.99 [ 23035 / 88622, 1682 ins, 1981 del, 19372 sub ] exp/tri2a/decode_train/wer_15_1.0
%WER 22.11 [ 19593 / 88622, 1700 ins, 1312 del, 16581 sub ] exp/tri2b/decode_train/wer_14_1.0

Again the best results are with tri2b not tri3b.
These numbers are much lower than the GP Yaounde models.
Why?
I am training the monophones on all the training data.

** WAR:
John Morgan is investigating a new kind of acoustic model for Automatic Speech Recognition (ASR) called chain models. 
His understanding is that chain models combine Time Delayed Neural Networks (TDNN),  Long Short-term Memory models (LSTM), i-vectors and a new objective function. 
This week he is trying to understand why the i-vectors are trained with 3-way speed perturbed waveform data. 
Morgan is interested in chain models because preliminary experiments he has performed yield results significantly better than the subspace gaussian mixture models (SGMM) which until recently were the best performers. 

** Goals for Tuesday:
- TODO Mandatory Training: Constitution Day Training
- TODO GP + Yaounde: Chain Models.
- TODO SofTunisia:  Chain models.

* DAR <2017-07-07 Fri>
**  Goals for Friday set Thursday:
- TODO Mandatory training.
I started on the Constitution Day training.
- TODO KSU: Convert rtf files from CP1256 to UTF-8.
I get arabic when I run catdoc -w INFILE > OUTFILE 
from the command line, but I get question marks instead when I run it from a perl script.
I'm trying to set binmode in the perl script.
binmode(STDIN, ":utf8");
binmode(STDOUT, ":utf8");
I am moving to doing this in bash scripts.

- TODO GP + Yaounde: Pron probs.
The cd gmm hmm build finished successfully on the GPU machine.
The WERs were not good.
Starting over.Following minilibrispeech.
I have to look very carefully at the arguments to the scripts.
Notice the nosp string appended to the lang directories.
After the pron probs script run the output lang directory no longer has the nosp.
  
- TODO SOFTunisia: Get transcripts to Zac.

** Goals for Monday:
- TODO Mandatory Training.
- TODO GP Yaounde: Fix LM Rescore command line.

* DAR <2017-07-06 Thu>
**  Goals for Thursday set Wednesday:
- TODO GP + Yaounde: Figure out what is wrong with the decoding fst.
The decoding runs on the gpu machine.
I am running into a problem when I run the scripts that rebuild the lexicon.
I think it has to do with the option 	--position-dependent-phones false \
that I use  with the prepare_lang script.
I restarted at the prepare lang   step with the option set to true.
The monophone training is extremely slow.
- TODO SOFTunisia: Get transcripts to Zac.
- TODO KSU: Create transcripts in folds for non-noise utterances.
Some rtf files are actually text files encoded with the Arabic Windows CP1256 code page.
I use iconv -f CP1256 INFILE -t UTF-8 > OUTFILE
to write the new file in UTF-8.
Other files are real rtf files and the text they contain is in UTF-8.
I use 
catdoc -s UTF-8 -w INFILE > OUTFILE
to get the text out of these files.

- TODO Mandatory training.

** Goals for Friday:
- TODO Mandatory training.
- TODO KSU: Convert rtf files from CP1256 to UTF-8.
- TODO GP + Yaounde: Pron probs
- TODO SOFTunisia: Get transcripts to Zac.

* DAR <2017-07-05 Wed>
**  Goals for Wednesday set Monday:
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
I am still stuck on decoding the monophones.
There is an error:
ERROR: FstImpl::ReadHeader: FST not of type vector: <unspecified>
Something seems to be wrong with the way the  fst is being made.

- TODO Mandatory training.

- ksu: Apparently, there are no transcripts for session 3.
It looks like I have utf-8 transcripts for about 25k utterances.

** Goals for Thursday:
- TODO GP + Yaounde: Figure out what is wrong with the decoding fst.
- TODO SOFTunisia: Get transcripts to Zac.
- TODO KSU: Create transcripts in folds for non-noise utterances.
- TODO Mandatory training.

* DAR <2017-07-03 Mon>
** Goals for Monday set Friday:
- TODO KSU: write scripts to copy rtf and flac files to disk. 
Made a little progress on this.
There are flac files that are missing.
I put some code in my script that avoids crashing and error messages from sox when files are missing.

- TODO SOFTunisia: Chain models.
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
I worked on gp - yaounde today.
There were some problems with test data.
Some weirdness training the monophones too.
Why does it take so long to train monophones?

- TODO Transapps: Get models to transapps.

** Goals for Wednesday:
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
- TODO Mandatory training.
 
* DAR <2017-06-30 Fri>
** Goals for Friday set Thursday:
- TODO KSU: Demux and downsample.
Splitting the data into dev, eval and train and by country.
I made progress on this.
1. Get the rtf files
Count all the rtf files with the following command:
find /mnt/corpora/LDC2014S02/KSU_Arabic_Speech_DB/data/ -type f -name "*rtf" | wc -l
there are 13072 rtf files.
BTW: this command took around a minute to run.
I chose the speakers for each fold by hand.
This should be easy to modify if we want to change the folds later.
There are 110929 flac files.
With scripts I wrote 1 file with the following fields:
speaker id	country	fold	rtf file
I'd like to have a file with the extra field flac file:
speaker id	country	fold	rtf file	flac file

I spent the whole afternoon working on a script to copy the flac and txt files to disk.

- TODO SOFTunisia: Chain models.
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.

** Goals for Monday:
- TODO KSU: write scripts to copy rtf and flac files to disk. 
- TODO SOFTunisia: Chain models.
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.

* DAR <2017-06-29 Thu>
** Goals for  Thursday set Wednesday:
- TODO SOFTunisia: CD GMM HMM on softunisia and westpoint.
Still more bugs.
For some reason the training data subsetting script is failing.
It somehow deletes the utt2spk file.
I am skipping the subsetting step.
I'm going to train on all of the training data from the beginning.
Interesting bug:
I ran into yet another bug.
The alignment was failing again.
I looked carefully at the log file.
There wer mfcc vectors missing.
The mfcc extractor is pretty stupid.
It uses the final directory name as a label.
I had my data arranged by corpus like:
westpoint/train
softunisia/train
When I run the mfccc extractor on these directories the later one hozes the previous one.
This is stupid.
Anyway ...
I need to go back and rename my directory structures.
I am renaming westpoint/train to westpoint_train.

I ran all the basic cd gmm hmm steps up to tri3b.
I am currently trying to set up decoding.

- TODO GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO KSU: Demux data.
I found a way to do this with sox.
- TODO KSU: downsample.
I can do this at the same time as demuxing.
- DONE Mandatory training.
I did the records management training.

** Goals for Friday:
- TODO KSU: Demux and downsample.
- TODO SOFTunisia: Chain models.
- TODO SOFTunisia: Get transcripts for Zak.
- TODO GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.

* DAR <2017-06-28 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Meeting with Voxtek (10:15 )
- TODO SofTunisia: Build cd gmm hmm with SOFTunisia and Westpoint.
There was still a bug in the Westpoint prep scripts.
The problem is that there are 2 sub corpora in the Westpoint corpus.
The nonnative recordings have a transcript in an arabtex file with a .txt file extention.
The transcript appears on one line in those files.

I'm restarting the building process.
I think everything is good for the softunisia data prep.
The westpoint conversion to 16k looks good.
There wer still bugs in the westpoint data prep scripts.
I think I've squashed them now.
There were problems with the processing of the subs data for the lm.
The OOVs were not being considered.
The dictionary building needed some work too.

As I am preparing to leave, I've started training monophones.
I have all of SOFTunisia and most of Westpoint in training. 
Before I leave I'm going to start the tri1 training.
The monophone training finished successfully.
I started the alignment  with monophones and a bug surfaced.
The utt2spk file for the training data was empty.
I fixed this and restarted.
- TODO Get some draft transcripts of Answers to Zaq.
- GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO KSU: Demux data.
- TODO KSU: downsample.

** Goals for  Thursday:
- TODO SOFTunisia: CD GMM HMM on softunisia and westpoint.
- TODO GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO KSU: Demux data.
- TODO KSU: downsample.
- TODO Mandatory training.

* DAR <2017-06-27 Tue>
**  Goals for Tuesday set Monday:
- TODO Make folds for KSU corpus.
Split into dev, eval and train folds by hand.
country	dev number	dev speakers	eval number	eval speakers	train number	train speakers	total
YE	2	NS32 NS33	2	NS13 NS15	10	NS1 NS2 NS3 NS4 NS5 NS6 NS7 NS8 NS9 NS10	15
SY	1	NS43	1	NS44	7	NS11	NS14	NS17	NS21	NS25	NS27	NS35	9
EG	2	NS36 NS40	2	NS46	NS153	9	NS12	NS19	NS22	NS23	NS24	NS28	NS29	NS30	NS31	13
DZ	0		0		4	NS151 NS154 NS16 NS47	4
SD 	0		0		4	NS18 NS52 NS42 NS39	4
TN	0		0		4	NS155 NS156 NS26 NS152	4
LB	0		0		1	NS41	1
PL	0		0		1	NS45	1
PK	2	NS211 NS212	2	NS213 NS214	4	NS201 NS205 NS206 NS209	8
IN	1	NS220	2	NS221 NS228	6	NS202 NS203 NS204 NS207 NS208 NS210	9
ML	0		0		2	NS215 NS230	2
AF	0		0		4	NS237 NS217 NS216 NS248	4
NP	1	NS218	1	NS219	5	NS222 NS223 NS224 NS225 NS229	7
NG	0		0		3	NS226 NS239 NS245	3
UG	0		0		3	NS227 NS242 NS249	3
BD	0	0	1	NS231	1
AG	0		0		1	NS232	1
ID	1	NS233	1	NS238	7	NS240 NS241 NS244 NS259 NS262 NS263 NS264	9
BG	0		0		2	NS234 NS261	2
RS	0		0		1	NS235	1
KE	0		0		2	NS236 NS253	2
PH	0		0		2	NS254 NS256	2
GW	0		0		1	NS255	1
BJ	0		0		1	NS243	1
CF	0		0		1	NS246	1
BN	0		0		1	NS247	1
CI	0		0		1	NS250	1
SN	0		0		1	NS251	1
LR	0		0		1	NS252	1
TH	0		0		1	NS257	1
TG	0		0		1	NS260	1
SA	4	S2 S3 S4 S5	8	S10 S11 S12 S13 S14 S15 S16 S17	139		152

Finished making folds? 
- GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO SofTunisia: Incorporate Westpoint and KSU.
I worked most of the afternoon on this.
There werr bugs in the Westpoint data prep scripts.
I'm almost happy with this now.

** Goals for Wednesday:
- TODO Meeting with Voxtek (10:15 )
- TODO SofTunisia: Build cd gmm hmm with SOFTunisia and Westpoint.
- TODO Get some draft transcripts of Answers to Zaq.
- GP Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO KSU: Demux data.
- TODO KSU: downsample.

* DAR <2017-06-26 Mon>
** Goals for Monday set Thursday:
- TODO Investigate and prepare King Saud University corpus.
Split into dev, eval and train folds by hand.
YE: 15 male speakers.
10 for train: NS1, NS2, ... NS10
3 for eval: 
2 for dev: NS32 NS33
SY: 9 male speakers.
1 for dev: NS43
1 for eval: NS44
7 for train: NS11	NS14	NS17	NS21	NS25	NS27	NS35	
EG: 13 male speakers.
2 for dev:       NS36	NS40	
2 for eval:       NS46	NS153	
9 for train: NS12	NS19	NS22	NS23	:NS24	:NS28	:NS29	:NS30	:NS31	 
DZ: 
1 for dev: NS151
1 for eval: NS154
2 for train: NS16	NS47	
SD: 
1 for dev: NS42
1 for eval: NS52
2 for  train: NS18	NS39	
TN: 4 male speakers.
1 for dev: NS155
1 for eval: NS156
2 for train: NS26 NS152
LB: 2 male speakers.
1 for train.
PL: 1 male speaker.
1 for train.
PK: 8 speakers.
2 for dev: NS211 NS212
2 for eval: NS213 NS214
4 for train: NS201	     NS205	NS206	NS209	

- GP + Yaounde: build chain models?

- TODO Transapps: Get models to transapps.
- TODO SofTunisia: Incorporate Westpoint and KSU.
- DONE Transgender Mandatory Training.

** WAR:
John Morgan is investigating a new corpus of spoken Modern Standard Arabic for use as data for developing a baseline Speech Recognition System for African Accented Arabic. 
The corpus was collected by the King Saud University and includes speakers from 28 nationalities. 
John Morgan also has a extra curricular activity to report. 
He and Dr. Phillip David completed the Garret County Grand Fondo "Diabolical Double" for the second year in a row on a tandem bicycle. 
The Diabolical Double is a 200km ride with 16800 feet of climbing. 
Dr. David rode as captain and Morgan as the stoker of the tandem.
Dr. David and Morgan lower their time from last year by 1 hour by finishing in 12.5 hours.

** Goals for Tuesday:
- TODO Make folds for KSU corpus.
- GP + Yaounde: build chain models?

- TODO Transapps: Get models to transapps.
- TODO SofTunisia: Incorporate Westpoint and KSU.

* DAR <2017-06-22 Thu>
** Goals for Monday:
- TODO Investigate and prepare King Saud University corpus.
- TODO GP + Yaounde: build chain models?
- TODO Transapps: Get models to transapps.
- TODO SofTunisia: Incorporate Westpoint and KSU.
- TODO Transgender Mandatory Training.

* DAR <2017-06-21 Wed>
** Goals for Wednesday set Tuesday:
- TODO Investigate King Saud University corpus.
Total number of recordings (.flac files):
110929
I checked one file with the file command and got:
/mnt/corpora/LDC2014S02/KSU_Arabic_Speech_DB/data/Males/Session_1/NS1/1.Office/10.Questions_1/Computer_Mic_Front.flac: FLAC audio bitstream data, 16 bit, stereo, 48 kHz, 1659360 samples
16bit
stereo
Sampling rate 48khz
There is a major division between Saudis and Non Saudis.

- TODO GP + Yaounde: build cd gmm hmm
This is now going well.
I just had to concentrate a little on this task.

- TODO Transapps: Get models to transapps.

Goals for Monday:
- TODO Investigate and prepare King Saud University corpus.
- TODO GP + Yaounde: Chain models?
- TODO Transapps: Get models to transapps.
- TODO SOFTunisia: Incorporate KSU and Westpoint

* DAR <2017-06-20 Tue>
**  Goals for Tuesday set Monday:
- TODO GALE: Get transcripts in UTF8.
I'm going to drop this goal.
I listened to some GALE Arabic recordings -- they are from broadcast news.
- TODO Incorporate GALE into SOFTunisia build.
- TODO SOFTunisia: build cd gmm hmm.
- TODO GP + Yaounde: cd gmm hmm.
- TODO Transgender Mandatory Training.
- TODO Get models to Transapps.
- DONE Visit Blossom Point.

** Goals for Wednesday:
- TODO Investigate King Saud University corpus.
- TODO GP + Yaounde: build cd gmm hmm
- TODO Transapps: Get models to transapps.
 DAR <2017-06-19 Mon>
**  Goals for Monday set Friday:
- TODO Westpoint: cd gmm hmm models.
I have separated the data into native and nonnative folds.
I also consolidated the native and nonnative train sets into one training set.
I am currently training monophones on the native train set.
I trained monophones only on the nonnative data,   but alignment failed.
Thus, I won't train only on nonnative data.
I trained monophones on both native and nonnative data.
Tests:
dev	native	44.76
dev	nonnative	83.39
eval	native	
eval	nonnative	80.66

- TODO SOFTunisia: cd gmm hmm.
I am integrating the Westpoint corpus into the SOFTunisia build.
I spent all afternoon working on the GALE corpus.
I'd like to incorporate it into the SOFTunisia build.
The GALE transcripts are in buckwalter.
The original scripts are in utf8.
I found the spot where the utf8 is converted into buckwalter.

- TODO GP + Yaounde: cd gmm hmm.
- TODO Transgender Mandatory Training.
- TODO Get models to Transapps.


** Goals for Tuesday:
- TODO GALE: Get transcripts in UTF8.
- TODO Incorporate GALE into SOFTunisia build.
- TODO SOFTunisia: build cd gmm hmm.
- TODO GP + Yaounde: cd gmm hmm.
- TODO Transgender Mandatory Training.
- TODO Get models to Transapps.
 DAR <2017-06-16 Fri>
**  Goals for Thursday set Wednesday:
- DONE MFLTS meeting at Mitre.

- TODO SOFTunisia: Build cd gmm hmm incrementally
Starting from step 0.
Recall that the SOF Tunisia corpus was split into dev, eval and train folds.
Machine CTELLONE was used for dev.
Machine CTELLTWO was used for eval.
Machines CTELLTHREE, CTELLFOUR and CTELLFIVE were used for train.
The training fold  has 5097 files.
Stage 0 has 25 files.
Consolidate train  and human corrections stage 0.
5122 files for training. 
MFCC features are extracted from this data.
CMVN Stats are computed for this data.
Since we have stages 0-7 we loop over them now.
For stage 1 we added another 25 files.
There are now 5047 files in the training data set.
Stage 2 add 125 files.
There are now 5272 training files.
Stage 3 had 99 files.
There are 5371 training files at stage 3. 
There are 100 new files in stage 4.
There are 5471 training files at stage 4. 
There are 174 new files in stage 5.
There are 5645 training files at stage 5.
There are 150 new files at stage 6.
There are 5795 training files at stage 6.
There are 149 new files at stage 7.
There are 5944 training files at stage 7.

The training fails at stage 3.
I need to do this over by hand.

I figured out that the problem was that I was skipping stage 1.
I am rerunning starting at stage 1.

Decoding the dev and eval sets was failing.
I figured out that I had to delete the old cmvn.scp file.

- GP + Yaounde Fix
I'm starting from scratch.
1. Data prep
Several corpora get prepared:
a. yaounde Recordings
b. Yaounde Answers.
c. Niger
d. gp

The yaounde Recordings and gp get concatenated for training.
gp has a dev fold
For testing, the Niger, gp tes and the central accord 515 get concatenated for test.
2. Dictionary
3. lang directory.
4. lm
5. extract mfcc
6. train and decode mono

- TODO Transgender Mandatory Training.

** Goals for Monday:
- TODO Westpoint: cd gmm hmm models.
- TODO SOFTunisia: cd gmm hmm.
- TODO GP + Yaounde: cd gmm hmm.
- TODO Transgender Mandatory Training.
- TODO Get models to Transapps.

* DAR <2017-06-14 Wed>
** Goals for Wednesday set Tuesday:
- TODO Skillport lesson 3.
- TODO TransApps: Transfer more models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
I tried to start this again.
- TODO Westpoint:   build cd gmm hmm system just on the Westpoint corpus.
I gotsome work done on this item.
The mono and tri1 models are trained.
I evaluated the mono models.
I am evaluating the tri1 models.
- TODO SOFTunisia: Cycle  through transcription process with Zac.
Starting from step 0.
Recall that the SOF Tunisia corpus was split into dev, eval and train folds.
Machine CTELLONE was used for dev.
Machine CTELLTWO was used for eval.
Machines CTELLTHREE, CTELLFOUR and CTELLFIVE were used for train.
The training fold  has 5097 files.
Stage 0 has 25 files.
Consolidate train  and human corrections stage 0.
5122 files for training. 
MFCC features are extracted from this data.
CMVN Stats are computed for this data.
Since we have stages 0-7 we loop over them now.
For stage 1 we added another 25 files.
There are now 5047 files in the training data set.
Stage 2 add 100 files.

The training fails at stage 2.
I need to do this over by hand.
- TODO Transgender Mandatory Training.

** Goals for Thursday:
- TODO MFLTS meeting at Mitre.

* DAR <2017-06-13 Tue>
** Goals for Tuesday set Monday:
- TODO Skillport lesson 3.
- DONE ARL Coloquium: Prepare.
- DONE ARL Coloquium: Put slides on S drive
- DONE Give presentation
- TODO Transgender Mandatory Training.

** Goals for Wednesday:
- TODO Skillport lesson 3.
- TODO TransApps: Transfer more models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint:   build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Cycle  through transcription process with Zac.
- TODO Transgender Mandatory Training.

* DAR <2017-06-12 Mon>
**  Goals for Monday set Friday:
- TODO Skillport lesson 3.
- DONE ARL Coloquium: Prepare.
I spent half the day working on the slides.
- TODO ARL Coloquium: Put slides on S drive
- TODO TransApps: Transfer more models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint:   build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Cycle 3 times through transcription process with Zac.
- TODO Transgender Mandatory Training.
- DONE Get JAWS installed.

** Goals for Tuesday:
- TODO Skillport lesson 3.
- TODO ARL Coloquium: Prepare.
- TODO ARL Coloquium: Put slides on S drive
- TODO TransApps: Transfer more models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint:   build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Cycle 3 times through transcription process with Zac.
- TODO Transgender Mandatory Training.
- TODO Get JAWS installed.

* DAR <2017-06-09 Fri>
** Goals for Friday:
- TODO Get credit for Transgender mandatory training.
- TODO Skillport lesson 3.
- TODO ARL Coloquium: Prepare slides.
- TODO GP + Yaounde : Fix broken building steps at model tri2.
- TODO Westpoint: Start CD GMM HMM training
- TODO SOFTunisia:  Fix step 3 
- TODO SOFTunisia: Do Stage 8 with chain models.
- TODO SOFTunisia: Decode stage 8 data with chain models and get rough draft to Zac.
-TODO SOFTunisia: chain model decoding with lm rescoring.

* DAR <2017-06-08 Thu>
** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO ARL Coloquium: Prepare slides.
- TODO GP + Yaounde : Fix broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
I made a little progress on this.
I think I wrote all the wav.scp utt2spk spk2utt and text acoustic mdoel training lists for  the native nonnative dev eval and train folds.
- TODO SOFTunisia: step through   stages 0-7    with mfcc features.
I'm getting stuck for some reason on step 3.

- TODO SOFTunisia: Do Stage 8 with chain models.
As I'm getting ready to leave, I have the chain model trainin on the GPU and I'm hoping it will decode the unsupervised data. 
- TODO SOFTunisia: Decode stage 8 data with chain models and get rough draft to Zac.
-TODO SOFTunisia: chain model decoding with lm rescoring.

** Goals for Friday:
- TODO Get credit for Transgender mandatory training.
- TODO Skillport lesson 3.
- TODO ARL Coloquium: Prepare slides.
- TODO GP + Yaounde : Fix broken building steps at model tri2.
- TODO Westpoint: Start CD GMM HMM raining
- TODO SOFTunisia: step Fix step 3 
- TODO SOFTunisia: Do Stage 8 with chain models.
- TODO SOFTunisia: Decode stage 8 data with chain models and get rough draft to Zac.
-TODO SOFTunisia: chain model decoding with lm rescoring.

* DAR <2017-06-07 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Skillport lesson 3.
- DONE ARL Colloquium: Get title to Gorden Videen.
Strategies For Developing Speech TO Speech Apps for Soldiers.
- TODO ARL Coloquium: Prepare.
I'm making slides.
- TODO TransApps: transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: start over from the beginning.
The CD GMM HMM tri5 models are built.
I'm going to step through the 8 stages we ran so far with mfcc features.
Then I'm going to move to chain models.
- TODO SOFTunisia: chain model: How real are the results I got today?
I have the chain model training running on my laptop.
I thought it only ran with a GPU.
It will probably take forever on my laptop.

-TODO SOFTunisia: chain model decoding with lm rescoring.

** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO ARL Coloquium: Prepare slides.
- TODO GP + Yaounde : Fix broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: step through   stages 0-7    with mfcc features.
- TODO SOFTunisia: Do Stage 8 with chain models.
- TODO SOFTunisia: Decode stage 8 data with chain models and get rough draft to Zac.
-TODO SOFTunisia: chain model decoding with lm rescoring.

* DAR <2017-06-06 Tue>
** Goals for Tuesday set Monday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
I'm going to the colloquium today with Gorden.
- TODO TransApps: transfer models.
I transfered the exp and data directories to run the gp-yaounde mono system.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Fix stage 7.
- TODO SOFTunisia: chain model: How real are the results I got today?
-TODO SOFTunisia: chain model decoding with lm rescoring.
I fixed the lm rescoring commands in the chain model scripts.
I'm not sure if I did this correctily, just a quick fix.

** Goals for Wednesday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: start over from the beginning
- TODO SOFTunisia: chain model: How real are the results I got today?
-TODO SOFTunisia: chain model decoding with lm rescoring.
 DAR <2017-06-05 Mon>
** Goals for Monday set Friday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- DONE TransApps: Connect to their server and transfer models.
Finally, the ports are open.
Dereck gave me a password.
I transfered the central_accord test data to the transapps site.
I filled up the disk partition Dereck gave me with the monophone models.

- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Cycle 3 times through transcription process with Zac.
The change I tried did not work.
I tried lowering the number of leaves and number of gaussians.

- TODO SOFTunisia: chain model build with ivectors.
I am making progress.
I am actually at the decoding stage.
I'm not sure if there were  errors in the training. 
The decoding finished.
%WER 7.53 [ 753 / 9999, 60 ins, 114 del, 579 sub ] exp/chain/tdnn1d_sp/decode_tgpr_eval/wer_8_0.0
This has got to be to good to be true.

** Goals for Tuesday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Fix stage 7.
- TODO SOFTunisia: chain model: How real are the results I got today?
-TODO SOFTunisia: chain model decoding with lm rescoring.

* DAR <2017-06-02 Fri>
**  Goals for Friday set Thursday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get  title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO Westpoint: Make acoustic model training lists.
This is turning out to be much harder than I expected.
I am trying to automate every step.
sometimes I take steps out of order and then when I go to run everything from scratch, I realize files that are required are missing and it's all broken.
I'll be happy once  I finish with the data preparation for this corpus.

- TODO SOFTunisia: Get chain model scripts running on GPU machine.
I am starting from the very beginning on the GPU machine.
I think the problem is with the feature vectors.
I think the chain models are set up to work with mfcc feature vectors.
I was using plp pitch feature vectors.
I am continuing to use plp pitch feature vectors on my laptop to do the cycle work with Zac.

Zac called me yesterday evening.
He says the transcripts are bad.
i think it is because I pumped up the number of gaussians parameter.
I am putting the parameter back to 4096 from 8192 and rerunning.

- TODO GP + Yaounde Fix: broken building steps at model tri2.


** Goals for Monday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: separate into native, nonnative, dev, eval, and train  folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Cycle 3 times through transcription process with Zac.
- TODO SOFTunisia: chain model build with ivectors.
* DAR <2017-06-01 Thu>
**  Goals for Thursday set Wednesday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get  title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
I think we've been forgotten.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: prepare acoustic model training lists
I made some progress.
Since I introduced the distinction between native and nonnative recordings, I had  to work through all the steps in getting the .wav files aligned with the .txt files containing the utf8 transcripts.
I'm almost finished with this step.
This is tricky, since different groups of speakers have transcripts in files with different extentions.

- Zac: He got me the corrections for stage 6 this afternoon.
I'm going through stage 7.
I automated some more steps.
I have scripts that :
- get the new speaker names.
- Concatenate the new speakers to the old speaker list.
- Concatenate the new transcripts to the old transcripts.

The step that builds the decoding graph fst takes a long time.
Otherwise the turn around is pretty quick.

- WER for stage 6:
eval:
%WER 12.06 [ 1206 / 9999, 96 ins, 155 del, 955 sub ] exp/tri5_human_corrected_6_ali/decode_eval/wer_17_0.0
dev:
%WER 12.32 [ 1302 / 10567, 115 ins, 126 del, 1061 sub ] exp/tri5_human_corrected_6_ali/decode_dev/wer_17_0.0

The WERs have gotten worse?

Almost everything is automated now.

** Goals for Friday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get  title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO Westpoint: Make acoustic model training lists.
- TODO SOFTunisia: Get chain model scripts running on GPU machine.
- TODO GP + Yaounde Fix: broken building steps at model tri2.

* DAR <2017-05-31 Wed>
**  Goals for Tuesday set Friday:
- DONE Orientation and Mobility at Silver Spring Train Station.


** Goals for Wednesday set Friday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get photo and title to Gorden Videen.
Got photo to Gorden.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: separate folds and build cd gmm hmm system just on the Westpoint corpus.
I separated the folds.
I've decided to do this in a better way.
Our nonnative Arabic is a valuable resource.
I am separating the folds by native and nonnative in addition to by train dev and eval.
I spent most of the day working on the preparation of the Westpoint corpus.
I was not even close to finishing last week.
Just separating the data into folds requires concentration.
The problem is that there are at least 2 naming conventions in the file from the Westpoint corpus.
For the native data the arabtex transcripts are on one line in a .txt file.
For the nonnative data, the arabtex transcripts are written one word per line the first line is !ENTER and the last line is !EXIT.
This means I have to process them differently.

- TODO SOFTunisia: Incorporate the Westpoint data into the SOFTunisia training or testing.
- TODO SOFTunisia: chain, tdnn_lstm and sgmm builds.

- I have too many things going on at once.
I tried to concentrate on the Westpoint corpus today.

- Zac got me his corrections for stage 5 of the SOFTunisia  Answers. 
I started working on stage 6.
I did the retraining, got the transcripts and handed them over to Zac, all in one afternoon.

** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get  title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: prepare acoustic model training lists

* DAR <2017-05-26 Fri>
** Goals for Friday set Thursday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Get stage 5 transcripts to Zac.
I got the stage transcripts.
I put them on a thumb drive.
I put the thumb drive in an envelope and put it under Hazrat's door.
 
- TODO SOFTunisia: Chain models?
I am starting over with chain models.
I'll have to concentrate more on this to get it to work.

- TODO Westpoint: decode on training data.
I decoded on the train set.
The WER was 100%.
I'm pretty sure the problem is that the reference transcripts I have are voweled and the hypotheses that are getting written by the system are not voweled.
I can convert the arabtex to utf8 with no vowels:
Encode::Arabic::ArabTeX->demode('novocalize');
After fixing the transcripts I get:
%WER 47.13 [ 27340 / 58006, 1893 ins, 2002 del, 23445 sub ] exp/mono/decode_train/wer_11_1.0
- TODO Westpoint: make dev eval and train folds.
I spent most of the day on this.
I am almost done.

- TODO ARL Colloquium: Get Title and photo to Gorden Videen
- TODO ARL Colloquium: Start preparing slides.
- TODO TransApps: Connect to there server? 

** Goals for Tuesday:
- TODO Orientation and Mobility at Silver Spring Train Station.
- TODO Skillport lesson 3.
- TODO ARL Colloquium: Get photo and title to Gorden Videen.
- TODO ARL Coloquium: Prepare.
- TODO TransApps: Connect to their server and transfer models.
- TODO GP + Yaounde Fix: broken building steps at model tri2.
- TODO: Westpoint: separate folds and build cd gmm hmm system just on the Westpoint corpus.
- TODO SOFTunisia: Incorporate the Westpoint data into the SOFTunisia training or testing.
- TODO SOFTunisia: chain, tdnn_lstm and sgmm builds.

* DAR <2017-05-25 Thu>
**  Goals for Thursday set Wednesday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Try chain models instead of nnet3 tdnn lstm.
I copied the chain model script from wsj and with a couple of fixes it is still running on the GPU machine.

- SOFTunisia: Zac got me stage 4 transcriptions.
I am going through cycle 5.
I decoded the eval and dev sets with the stage 4 models:
Eval: 
%WER 11.95 [ 1195 / 9999, 114 ins, 145 del, 936 sub ] exp/tri5_human_corrected_4_ali/decode_eval/wer_17_0.0
Dev:
%WER 12.23 [ 1292 / 10567, 120 ins, 140 del, 1032 sub ] exp/tri5_human_corrected_4_ali/decode_dev/wer_17_0.5

As I am getting ready to leave, the decoding for the unsupervised data is running.

- TODO Westpoint: figure out what is wrong.
I am working on the westpoint santiago dictionary.
Phone map:
1. Remove g.
There was only 1 instance of this phone.
اَلشَّگَرَة	ah sh   ah g ah r  ah
I removed this entry.
2. G to g
This corresponds to the "gain" arabic character (maybe a uvular fricative? like the French r).
The qcri dictionary uses a g for this phone as far as I can tell.
3. Q -> G
I think this is the glottal stop.
4. ae -> a
Not sure.
5. C -> E
This is the ayn (Pharyngeal stop? or something like that).
6. iy -> i
7. ih -> i
8. ah -> a
9. TH -> V
10. th -> v
11. sh -> $
12. aw -> a w
13. ey -> a y (I do not think this is correct).
14. ay -> a y (not correct?)

- TODO Westpoint: split labeled data into folds.

- TODO ARL Colloquium: Get Title and photo to Gorden Videen
- TODO ARL Colloquium: Start preparing slides.
- TODO TransApps: Can I logon?
- TODO GP + Yaounde: move on to build the rest of the CD GMM HMM system.
I ran the tri2 build again with 3000 10000 leaves gaussians parameter settings.
%WER 39.97 [ 13145 / 32888, 1130 ins, 2238 del, 9777 sub ] exp/tri2/decode_test/wer_16_1.0
I looked at the settings in the babel config files.
It looks like the gaussians should be set higher, so I'm trying again with:
2048 16384
%WER 37.81 [ 12434 / 32888, 1039 ins, 2077 del, 9318 sub ] exp/tri2/decode_test/wer_14_1.0

** Goals for Friday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Get stage 5 transcripts to Zac.
- TODO SOFTunisia: Cain models?
- TODO Westpoint: decode on training data.
- TODO Westpoint: make dev eval and train folds.
- TODO ARL Colloquium: Get Title and photo to Gorden Videen
- TODO ARL Colloquium: Start preparing slides.
- TODO TransApps: Connect to there server? 

* DAR <2017-05-24 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Try chain models instead of nnet3 tdnn lstm.
- TODO Westpoint: build cd gmm hmm on only westpoint data.
- DONE SOFTunisia: Cycle 4 of Answers transcriptions with Zac. (Zac is sic, so this is not a priority).
Steps:
1. Make the new acoustic modeling lists wav.scp, text, utt2spk and spk2utt.
This involves several substeps:
Ideally, all the steps would be automated, but there are a couple of steps I do by hand.
By hand:
I have a list of the new speakers that have been transcribed.
I append the latest speakers to this list.
I have a tsv file with all the new transcriptions. 
After removing the " hyp", I append Zac's newest transcription to this file.
I also have a .txt transcription file that only contains the text from all the new transcriptions. 
This gets used in training the lm.
I append Zac's latest transcription text to this file. 
2. Then I run a script that automatically uses these lists to make the 4 acoustic modeling lists. 
4. Consolidate the previous acoustic model training lists with the new lists.
4. run the plp pitch extraction script that extracts features from all the files in the consolidated lists.
5. Align (fmllr) the new training data.
Do I need to do this?
I think the alignments are used in reestimating the pron probs.
I think it uses transforms from a previous step to align the current training data set.
6. Prepare a new data/lang directory.
7. Count pronunciations.
8. Make new  dict directory with pron probs
From utils/dict_dir_add_pronprobs.sh
This script takes pronunciation counts, e.g. generated by aligning your training data and getting the prons using steps/get_prons.sh, and creates a modified dictionary directory with pronunciation probabilities. 
If the [input-sil-counts] parameter is provided, it will also include silprobs in the generated lexicon.

I was afraid this step was not working, but I stepped through the log file and it looks good.

9. LM: prepare and format to produce G.fst.
Preparation includes concatenateing the new transcripts to the subs data and the SOFTunisia Recordings prompts.
This step runs surprisingly fast compared to French lm training  for the gp+yaounde system .
There are 414k lines  in the training text file.

10. Run SAT on the new training data.
I noticed that I have been using 1024 and 4096 as the leaves and gaussians parameters.

11. Make decoding graph fst and Decode.
I made a mistake here.
I started decoding the training data.
What I really want to decode is the new stage 4 set of speakers 12 13 14 and 15.
I would have to run a separate plp pitch extraction to get the feats.scp  and cmvn.scp files.
Instead, I just decode all the unsupervised data, for  which I have already extracted features. 
fmllr decoding.
Get first fmllr transforms
Generate latices.
Reestimate fmllrs.
Acoustic rescoring.

I gave the next batch of wav files from speakers 12 13 14 and 15 together with rough draft transcriptions of the data to Zac via Hazrat.

- TODO GP + Yaounde: Tune tri2 leaves and gaussians parameters.
Here are the results for leaves and gaussians set to 4096 and 16384:
%WER 45.94 [ 15110 / 32888, 1133 ins, 2636 del, 11341 sub ] exp/tri1/decode_test/wer_14_0.5

- TODO TransApps: Can I logon?
Still waiting for networking to give the ok.
Apparently there are 3 groups that need to approve the exception to policy.


- I attended the Asian and Pacific Islander Heritage Month event in the cafeteria.

** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Try chain models instead of nnet3 tdnn lstm.

- TODO Westpoint: figure out what is wrong.
- TODO Westpoint: split labeled data into folds.
- TODO ARL Colloquium: Get Title and photo to Gorden Videen
- TODO ARL Colloquium: Start preparing slides.
- TODO TransApps: Can I logon?
- TODO GP + Yaounde: move on to build the rest of the CD GMM HMM system.

* DAR <2017-05-23 Tue>
** Goals for Tuesday set Monday:
- TODO Skillport lesson 3.
- DONE Mandatory TARP training. 
Hazrat took me to the auditorium for TARP.
- TODO SOFTunisia: Fix tdnn lstm decoding on unsupervised data.
Here is the result after decoding the training set:
%WER 92.89 [ 77329 / 83244, 1533 ins, 28529 del, 47267 sub ] exp/nnet3/tdnn_lstm_sp/decode_train/wer_8_0.0
Why is this so bad?

- TODO SOFTunisia: investigate incorporation of new words into pronouncing dictionary.
- TODO TransApps: Connect to account on their linux box and transfer models (monophones?)
It looks like tthe authorization to open the port was granted.

- TODO GP + Yaounde: Build CD GMM HMM.
The tri2 decoding with the 1024 4096 leaves and gaussians parameters set was running when I left yesterday.
Here are the WER scores:
%WER 40.77 [ 13410 / 32888, 1290 ins, 2001 del, 10119 sub ] exp/tri2/decode_test/wer_15_0.5
This is down from 43.78.
I am running again with 2048 and 8192 parameter settings.
Results are still slightly better.
%WER 39.66 [ 13045 / 32888, 1009 ins, 2290 del, 9746 sub ] exp/tri2/decode_test/wer_16_1.0

- TODO SOFTunisia: Incorporate Westpoint data into SOFTunisia training or testing.
I do not have data back from Zac yet, so I'm going to work on the westpoint data separately.
I'm creating a bare shared repo on the GPU machine for the westpoint system.
1. convert the raw waveform data to .wav formt.
This also puts the .wav files in a local directory.
2. Get the list of .txt files. It looks like most of the .txt files are actually in arabtex.
3. Convert the arabtex to utf8.
4. Prepare acoustic modleing lists.
5. Prepare dictionary: I'm using the same dictionary as for the SOFTunisia build.
6. Make data/lang directory.
7. LM: same as SOFTunisia except westpoint prompts instead of SOFTunisia Recordings.
8. Extract plp pitch features.
9. Monophone training.

- TODO ARL Colloquium: Get Title and photo to Gorden Videen

** Goals for Wednesday:
- TODO Skillport lesson 3.
- TODO SOFTunisia: Try chain models instead of nnet3 tdnn lstm.
- TODO Westpoint: build cd gmm hmm on only westpoing data.
- TODO SOFTunisia: Cycle 4 of Answers transcriptions with Zac. (Zac is sic, so this is not a priority).
- TODO GP + Yaounde: Tune tri2 leaves and gaussians parameters.
- TODO TransApps: Can I logon?

* DAR <2017-05-22 Mon>
**  Goals for Monday set Friday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Zac: incorporate Zac's latest corrections into stage 3 of training and decoding. 
I got the transcriptions for speakers 3 through 7 from Zac via Hazrat.
1. Inserted 5 new speakers into speakers.txt file.
2. Appended new transcriptions to transcriptions.tsv and transcriptions.txt files.
3. Created file lists for acoustic model training with new data.
4. Created langp directory for new training data.
I don't think I took this step in the right place last cycle. 
This might explain why Zac found an OOV repeated in the second cycle.
I still don't understand everything that is being done with some of these scripts.
5. Ran pron prob reestimation scripts.
6. Prepared lm with new data
7. Formatted lm and created new G.fst.
8. Trained tri models with SAT.
9. Decoded unsupervised data with FMLLR.
10. I gave Hazrat rough draft transcriptions for 4 speakers 8, 9, 10 and 11. He took them to Zac on the thumb drive.

- Problem:
We want to incorporate the new words (OOVs) that Zac transcribes into our pronouncing dictionary.

I also ran the same models on the training data.
Here is the WER:
%WER 10.56 [ 2929 / 27748, 432 ins, 294 del, 2203 sub ] exp/tri5_human_corrected_3_ali/decode_train/wer_17_0.0

It probably should be better.

- TODO TransApps: transfer some models to them via sftp or scp.
Derek gave me an account on his linux box.
I cannot connect.
Got help from Justin.
Problem: we are blocking port 4567.
Justin requested an exception to open this port for me.

- TODO SOFTunisia: Incorporate Westpoint data into SOFTunisia training.
- DONE ARL Colloquium preparation (abstract and bio)
Abstract:
There are many scenarios in which US Army Soldiers need to communicate with people who do not speak English. 
Speech to Speech  (S2S) devices provide a solution to this problem. 
Soldiers in the field have recently been requesting access to S2S devices to communicate with Soldiers from allied forces who speak accented versions of French and Arabic. 

The Applications Team (ATeam) in the Multilingual Computing and Analytics Branch (MCAB) is investigating methods for improving the performance of S2S devices when they are used to communicate  with speakers of low-resourced languages like Pashto and subpopulations of speakers of global languages like French. 
The ATeam works on the three components of an S2S device: Automatic Speech Recognition (ASR), Machine Translation (MT) and Text to Speech (TTS).

ATeam member, John Morgan will talk about the challenges posed by the linguistic environments encountered by US Soldiers for mathematically modeling the ASR and MT components of S2S devices. 
He will present the latest adaptation work he is doing with the well-established Gaussian Mixture Model Hidden Markov Model (GMM-HMM) framework and his attempts to improve on these methods with more recent AI inspired Neural Network models.

Bio:
John Morgan is a Mathematician who has worked in the fields of Computational Linguistics and Natural Language Processing for 20 years, first at West Point in the Center for Technology Enhanced Language Learning and for the past 9 years at ARL in the Multilingual Computing and Analytics Branch. 
He has a B.S. degree in Mathematics from the University of California, Irvine and an M.A. in Mathematics from UC Berkeley. 
He also was a graduate Computer Science student studying Natural Language Processing at UMD.
His work has focused on algorithms for the adaptation of models to sub populations of linguistic groups. 
While at West Point he developed a method for adapting an Automatic Speech Recognition system so that it would tolerate speech from non-native foreign language learners. 
He recently moved his focus from the investigation of Machine translation models for Afghan languages to Automatic Speech Recognition modeling of African Accented French and Arabic. 

Gorden says it's good.
I still need to get him a photo and a title.

- GP + Yaounde models for TransApps:
Monday morning: I had started the decoding for the tri2 models with new settings for  number of leaves and number of gaussians. 
New Settings: 800 2000
New WER: 43.78 down from 49.32.
Should I try even higher settings?
I am trying with 1024 and 4096
The mkgraph script takes several hours to finish.
Why?
Our LM must be huge.

- SOFTunisia: Fix decoding with tdnn lstm:
When I try to run the decoding on the unsupervised data I get  an error that the feature dimensions for input to the neural network is expected to be 40 but my input has dimension 16.
It looks like I can decode the training data.
So there must be a problem with the way I am processing the unsupervised test set data.
The problem could be with the i-vector extraction on the unsupervised data.

** Goals for Tuesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: Fix tdnn lstm decoding on unsupervised data.
- TODO SOFTunisia: investigate incorporation of new words into pronouncing dictionary.
- TODO TransApps: Connect to account on their linux box and transfer models (monophones?)
- TODO GP + Yaounde: Build CD GMM HMM.
- TODO SOFTunisia: Incorporate Westpoint data into SOFTunisia training or testing.
- TODO ARL Colloquium: Get Title and photo to Gorden Videen

* DAR <2017-05-19 Fri>
**  Goals for Friday set Thursday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO TransApps: Figure out how to get models to TransApps.
Joe forwarded me to a sysadmin.
It looks like we can use sftp or scp, but apparently I need to give them my public key.
Not sure if I'll be able to do this.
I don't really have mail on either the GPU machine or my laptop.
Asking Justin for help.
key-gen -t rsa -b 4096 -C "john.j.morgan50.civ@mail.mil"
This creates the .ssh/id_rsa.pub
Justin retrieved it from my laptop then he sent it to my enterprise machine.
I then saved it and  forwarded to the transapps sysadmin Dereck.

- TODO GP + Yaounde : Chain Models?
I have decoded with tri1 and tri2 models.
tri1: %WER 45.94 [ 15110 / 32888, 1133 ins, 2636 del, 11341 sub ] exp/tri1/decode_test/wer_14_0.5
tri2: %WER 49.32 [ 16221 / 32888, 1143 ins, 2900 del, 12178 sub ] exp/tri2/decode_test/wer_12_0.5
Something went wrong here.
Possible problem:
I changed the number of gaussians and the number of leaves parameters (not sure why?) from 600 150 to 300 1000 respectively.
I'm rerunning the tri2 training with the parameters set to 800 2000.
Next, I ran the pron probs reestimation scripts.
I also ran the lm training again to get a new G.fst.
Ran alignment.
Decoding tri2 now.
It's taking for ever on the gpu machine.
Why is it taking so long?
It bogged down my laptop when I ran it there.
Why?

- TODO Colloquium Presentation: Write Bio and Abstract. Start writing an outline.

- SOFTunisia:
Working on nnet3 build.
fmllr alignment for both train and unsupervised data.
i-vector extraction failed on my laptop, but it seems to be succeeding on the GPU machine.

- problem:
I am at the point where I run the rnn training.
The kaldi script I am trying to copy uses the option:
--ali-dir=exp/tri5_ali_train_sp
I do not have this directory yet.
I do have:
--ali-dir=exp/tri4_ali_train_sp
I am trying to find where this directory was created.
I'm pretty sure it got created in the alignment run.
I set the output directory to exp/tri5_ali_train_sp in the alignment script.
I wrote the config file.
Training is running on the GPU.
Training finished.
Decoding still fails.

** Goals for Monday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Zac: incorporate Zac's latest corrections into stage 3 of training and decoding. 
- TODO TransApps: transfer some models to them via sftp or scp.
- TODO SOFTunisia: Incorporate Westpoint data into SOFTunisia training.
- TODO ARL Colloquium preparation (abstract and bio)

* DAR <2017-05-18 Thu>
** Goals for Wednesday set Tuesday:
- DONE Doctor's appointment.

** Goals for Thursday set Tuesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: Decode unsupervised data with new human corrected 2 system.

I had started the LM formatting when I left on Tuesday.
It had finished when I came into work this morning and had produced the G.fst file.
It puts the G.fst in the data/langp/tri5_human_corrected_1_ali folder.
This is the new lang directory I will use for decoding with the new human corrected stage 2 models. 
Actually, I do not need the G.fst and until decoding, it is not used in training.
I can start the acoustic model training before finishing the lm training and G.fst compile.
AM training finished and I started the decoding process.
I decode the unsupervised data.
Before decoding, the decoding fst graph has to be constructed.
The mkgraph takes the following arguments:
1.     data/langp/tri5_human_corrected_1_ali
This folder gets produced in the pron prob reestimation step.
2.    exp/tri5_human_corrected_1_ali
This is the output of the alignment that  is run after AM training with the human corrected stage 1 data.
3.    exp/tri5_human_corrected_2/graph
The HCLGa.fst file is written to this folder.

- Problem:
When I went to decode with the decoding fst graph produced above, I got an error.
The decoding scripts expect some components to exist in specific places. 
There has to be a "tree" directory in the same directory where the decoding is done.
The "tree" directory is in the place where the alignments were done.
So the new arguments to the mkgraph script will be:
1.     data/langp/tri5_human_corrected_1_ali same as above
2.    exp/tri5_human_corrected_1_ali same as above
3.    exp/tri5_human_corrected_1_ali/graph
The graph/HCLGa.fst file has to end up under  the  ...

- problem:
I found a problem in a previous step.
When I ran the  AM training, I was using the wrong alignment source directory.
I was using:
exp/tri5_ali
This is the alignment directory from the first base building stage.
I am rerunning the AM training with the following alignment source directory:
exp/tri5_human_corrected_1_ali
This directory was produced by running alignment with models that were trained with the stage 1 human corrections.
It is important not   to skip this step since the main idea is that we incorporate the information from previous human corrections into the current   AM training stage.


- DONE Get speaker #3 hypotheses to Zac.
After decoding finished, I gave Zac (via Hazrat) speakers 3 4 5 6 and 7.

- TODO Get nnet3 rnn system decoding working again.
I've been going through the scripts to build the nnet3 rnn system.
I was missing the scripts corresponding to the unsupervised  data which should be analogous to the scripts for the train data. 
I made scripts for the unsupervised data for: 
1. speed perturbation, 
2. volume perturbation,
3. plp extraction, 
4. - problem:

I do not think the mllt lda training that is done on the supervised train (Recordings) data can be done for the unsupervised (Answers) data.
Can I skip this step for unuspervised?

5. ubm training.
This step depends on the mllt lda training in the previous step.
So, it looks like I will not be able to extract the ubm for unsupervised.

6. i-vector extractor training
This totally bogged down my laptop. I might have to do it on the GPU machine exclusively.

7. Extract i-vectors.
I guess I can use this for both the train and unsupervised data. Maybe?

** Goals for Friday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO TransApps: Figure out how to get models to TransApps.
- TODO GP + Yaounde : Chain Models?
- TODO Colloquium Presentation: Write Bio and Abstract. Start writing an outline.

* DAR <2017-05-16 Tue>
**  Goals for Tuesday set Monday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Zac: Get transcript of speaker #2.
- TODO SOFTunisia: try tri4 models, tri3 if they don work.
I'm trying to figure out what the problem is.
My guess:
When I add the new data, for some purposes, I am adding a new speaker, while for other purposes I am not.
I wanted to skip the step where I extract the plp pitch features for the supervised (Recordings) data since this step has already been taken.
The feature extraction step produces some files that get used in training.
Specifically, feats.scp and cmvn.scp.
Since I wanted to skip the feature extraction for the supervised data and only extract features for the human corrected data, I thought I could just concatenate and sort the feats.scp and cmvn.scp files from the train (supervised) and new human corrected data to get the feats.scp and cmvn.scp files for the new train + human corrected data training set.
When I concatenate and sort these 2 files I do not get correct files.
There is 1 entry in the cmvn.scp file for each speaker.
After concatenating I get 69 speakers.
Before concatenating there were only 68 speakers.
I think this is where the problem is occurring.
I am going to  extract features from the whole supervised + human corrected data set so that correct feats.scp and cmvn.scp files are generated.
This seems to be working.
So, I either figure out how to create correct feats.scp and cmvn.scp files without repeating the feature extraction step, or I run the feature extraction step on the whole concatenated data set.
For now, I'm going to stick with what works.
Training/retraining with SAT tri5 on the old train supervised (Recordings) data plus the new human corrected data succeeded.
The probability reestimation scripts  ran successfully.
When I go to decode, I get an error because the G.fst file is missing.
The G.fst file is the compiled fst for the lm.
Here is where I neeed to think about the lm which Steve reminded me to do a couple of days ago.
I guess I need to append the human corrected transcripts to the lm data and rerun the training and fst compile.
I did this and it seems to have worked.
I ran the tri5 training on the new training set.
I reeestimated the dictionary probs.
I decoded the unsupervised data with the new models.
I gave the output hypotheses to Zac.
He says it looks very good.
He is returning his corrected version to me.

What steps should I take to retrain?
1. Insert Zac's transcriptions into the local/src/human_corrections/transcriptions.tsv file.
2. Remove the " hyp" from Zac's transcriptions.
The " hyp" was part of the per_utt file where I get the hypotheses.
I could remove the " hyp" before I give them to Zac.
3. Add the new speaker to the speakers.txt list.
4. Run the scripts that prepare the acoustic modleing lists: wav.scp, text, utt2spk snd spk2utt.
5. Consolidate the old acoustic modleing list and the  new acoustic modeling list and store them in a new directory.
6. Extract plp pitch features for the new consolidated training set.
7. Align the training data with the new  human corrected models. 
8. Reestimate again pron probs with the alignments.
9. Prepare the lm with the new transcripts.
10. Format the G.fst with the new lm.
11. Decode the unsupervised data with the new system.

** Goals for Wednesday:
- TODO Doctor's appointment.

** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: Decode unsupervised data with new human corrected 2 system.
- TODO Get speaker #3 hypotheses to Zac.
- TODO Get nnet3 rnn system decoding working again.
,
* DAR <2017-05-15 Mon>
**  Goals for Monday set Friday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: write the raw2wav.sh script for the West Point Arabic Speech data.
I have something done, but I need to write the files with the correct file names for kaldi processing.

- TODO SOFTunisia: Make a test set with  West Point Arabic Speech .wav data.
- TODO GP + Yaounde: Decode test set at each model stage.
- TODO SOFTunisia: Write scripts to continue each model branch. Branches: sgmm, nnet2 dnn bnf, nnet3 rnn.
- TODO GP + Yaounde: Write scripts to continue each model branch. Branches: sgmm, nnet2 dnn bnf, nnet3 rnn.


- Priority for tomorrow:
Retrain the SOFTunisia models with Zac's human corrected transcripts.
I tried using tri5 models today.
I am failing.
I think the reason is that by using tri5 I am forced to use transforms that the SAT training method  produces.
There is  probably a transform for each speaker.
This is going to cause a problem when I bring in the new data.
Tomorrow I'll try with tri4 models.
There might even be a problem with tri4 models since they also do adaptation.
They do feature space adaptation.
If tri4 models do not work, I'll retreat to tri2.
My priority tomorrow is to get transcripts for Zac for speaker #2.

** Goals for Tuesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Zac: Get transcript of speaker #2.
- TODO SOFTunisia: try tri4 models, tri3 if they don work.

* DAR <2017-05-12 Fri>
**  Goals for Friday set Thursday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
I tried to sign in to the TARP training online following the steps Anna Dye sent out.
I got through step 7 and failed on step 8.

- DONE Change my AKO password.
- SOFTunisia: TODO Rerun model building and retraining with human corrections.
I put all the scripts for building the CD GMM HMM into 1 script file.
I am rerunning the CD GMM HMM build again with this script.
I had to do this because there were problems with the way I was naming the Answers data.
this problem was going to cause further problems when I consolidated the human transcriptions with the old supervised data.
Hopefully, this problem is solved now.

- Test set for SOFTunisia:
I looked at the /mnt/corpora/arabic_speech folder.
The waveform files are in .raw format.
I am writing a script to convert these files to .wav format.
The following sox command seems to work:
sox   \
    -r 16k \
    -e signed \
    -b 16 \
    -c 1  \
    /mnt/corpora/arabic_speech/f001arabic1/s1_001.raw  \
    1.wav


- TODO SoFTunisia: Get the rough draft to Zac.
- TODO GP + Yaounde: Decode at all model builds. Speciffically, decode with sgmm2 models that are training now.
- TODO GP + Yaounde: Move to a new model: Chain, bnf nnet?
- TODO Vietnamese: Contact Yenda about compiling kaldi with GPU on clsp cluster.

** Goals for Monday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: write the raw2wav.sh script for the West Point Arabic Speech data.
- TODO SOFTunisia: Make a test set with  West Point Arabic Speech .wav data.
- TODO GP + Yaounde: Decode test set at each model stage.
- TODO SOFTunisia: Write scripts to continue each model branch. Branches: sgmm, nnet2 dnn bnf, nnet3 rnn.
- TODO GP + Yaounde: Write scripts to continue each model branch. Branches: sgmm, nnet2 dnn bnf, nnet3 rnn.

* DAR <2017-05-11 Thu>
**  Goals for Thursday set Wednesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFtunisia: Write scripts to easily incorporate new human corrected (Zac) data into retraining and decoding.
Where should I start training with the new human corrected data?
Here is a list of the scripts I run to take each step: 
The steps for building a standard cd gmm hmm go to step number 30.
First basic data preparation steps
01prep_data.sh
02prep_lists.sh
03get_dict.sh
04prep_lang.sh
05get_subs_data.sh
06get_subs_oovs.sh
07skip_subs_oovs.sh

Make the Language Model

08prep_lm.sh
09format_lm.sh

Extract front end feature vectors

10extract_plp.sh
11cnvn.sh

Start building models
12subset_data.sh
13mono.sh
14align_mono.sh
15tri1.sh
16align_tri1.sh
17tri2.sh
18reestimate_lang_tri2.sh
19align_tri2.sh
20tri3.sh
21reestimate_lang_tri3.sh
22align_tri3.sh
23tri4.sh
24reestimate_lang_tri4.sh
25align_tri4.sh
26tri5.sh
27reestimate_lang_tri5.sh
28align_tri5.sh
29reestimate_lang_tri5.sh

At this point building can diverge into several branches
30copy_langp_test.sh
30run-2a-nnet-cpu.sh
30run-2a-nnet-ensemble-gpu.sh
31run-2b-bnf.sh

Build the SGMMs
31ubm.sh
32sgmm2.sh
33align_sgmm2.sh
34reestimate_lang_sgmm2.sh
35denlats.sh
36sgmm5_mmi.sh

37prepare_data_subsets.sh

Extract feature vectors for the Unsupervised data. I probably should do this during the initial data prep stage.
38extract_plp_unsupervised.sh
39cnvn_unsupervised.sh

Decode unsupervised data with tri4 models
40decode_unsupervised_tri4.sh

Start the nnet3 rnn building
41speed_perturb.sh
42volume_perturb.sh
43hires.sh
44make_plp.sh
45select_hires.sh
46train_hires_mllt_lda.sh
47ubm.sh
48train_i-vector_extractor.sh
49extract_ivectors.sh
50align.sh
51config.sh
52train_rnn.sh

I wrote scripts to prepare the acoustic model training lists for the human corrected data.
I found a problem with how I was dealing with the unsupervised data.
Since I was not using any of the Answers/unsupervised data for training, it did not appear as a problem before.
Now that I am consolidating both file names from the Recordings/supervised and Answers/unsupervised  into acoustic model training list it appeared as a problem.
I am consolidating the original Recordings/supervised data with the human corrected/Answers/previously unsupervised data for a new training session.
The problem is with sorting.
I was not worrying about the names I was assigning to the files when I was not using the Answers data for training.
it is complicated, but the kaldi scripts are very sensitive to the sorting order of the wav files.
Anyway, I think I fixed the problem with the file names.
Unfortunately, I really should start all over with the model building.

- TODO SoFTunisia: Get the rough draft to Zac.
- TODO GP + Yaounde: Decode at all model builds. Speciffically, decode with sgmm2 models that are training now.
- TODO GP + Yaounde: Move to a new model: Chain, bnf nnet?
- TODO Vietnamese: Contact Yenda about compiling kaldi with GPU on clsp cluster.

** Goals for Friday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Change my AKO password.
- TODO Rerun model building and retraining with human corrections.
- TODO SoFTunisia: Get the rough draft to Zac.
- TODO GP + Yaounde: Decode at all model builds. Speciffically, decode with sgmm2 models that are training now.
- TODO GP + Yaounde: Move to a new model: Chain, bnf nnet?
- TODO Vietnamese: Contact Yenda about compiling kaldi with GPU on clsp cluster.

** Goals for Monday:

* DAR <2017-05-10 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: Does alignment work with plp pitch features?
Answer: yes. At least the alignment went through after I used plp + pitch instead of mfcc.
It looks like the script that makes the configuration file succeeded.
I'm trying to run the script that  runs the rnn training.
There was a problem with the number of jobs exceeding the number of egs?
I set nj to 1 and it got passed that error.
No. The problem is hat a final number of jobs is set to 10 and there is an eg archive for each of the 8 jobs set by the jn variable.
I set the final jobs variable to 8.
Good! it looks like the rnn training is running on the GPU.
This was not too hard to get running.
Hoepfully, this will produce an nnet3 system for SOFTunisia.
Running into memory allocation errors.
I'm fiddling with the number of jobs variables.
Leaving nj at 8.
I made the following modification:
--trainer.rnn.num-chunk-per-minibatch=128,64 \
to
--trainer.rnn.num-chunk-per-minibatch=64,32 \
This got me passed the memory allocation problem for now.
RNN training finished.
I started decoding the unsupervised Ansers data with the RNN models.
I need to setup my scripts to enable retraining with the new human corrected data.
I need to order the scripts I ran to get nnet3 running.
41. speed perturb
42. volume perturb
43. make plp features for low resolution speed perturbed data?
44.  

As I'm preparing to leave, I have the rnn decoding and the tri5 decoding running.
Check on these runs tomorrow morning.
 
- TODO  Get Zac a rough draft of speaker 2 even if it is using tri4 models.
I prepared Zac's transcription of the first speaker CTELLFIVE 01.
I have output of the Answers data from several decodeings.

- DONE GP + Yaounde: resume on laptop at step 29. On GPU resume at step 27.
Moving forward with the incremental build.
I've finished tri5.
Next comes the sgmm building phase.
The first step is to make the UBM.
I am now running the sgmm2 training.
What does the 2 stand for?
2 pass maybe?
This is running still as I am preparing to leave.
Check on the results of this run tomorrow.

** Goals for Thursday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFtunisia: Write scripts to easily incorporate new human corrected (Zac) data into retraining and decoding.
- TODO SoFTunisia: Get the rough draft to Zac.
- TODO GP + Yaounde: Decode at all model builds. Speciffically, decode with sgmm2 models that are training now.
- TODO GP + Yaounde: Move to a new model: Chain, bnf nnet?
- TODO Vietnamese: Contact Yenda about compiling kaldi with GPU on clsp cluster.

* DAR <2017-05-09 Tue>
** Goals for Tuesday set Monday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: start from local/nnet3/select_hires.sh
I ran the select command
Next mllt and lda trained on mfcc hires.
train a diagonal UBM.
train the i-vector extractor.
prepare the directory for speed perturbed low resolution data.
Extract mfcc features  
align 

The alignment step is failing.
Maybe I need to extract plp with pitch features?
I am testing this as I am getting ready to leave.

I also started decodeing with the tri4 models.
I might want to get a rough transcript for speaker 2 to Zac with this decoding?

- DONE gp + yaounde: Run step 16 on my laptop to try to debug pron problem.
The problem was in step 17.
I had skipped the step where the tri3 prons are added to the dictionary.
I've taken the next few steps up to step 28 for tri5 models.

** Goals for Wednesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: Does alignment work with plp pitch features?
- TODO  Get Zac a rough draft of speaker 2 even if it is using tri4 models.
- TODO GP + Yaounde: resume on laptop at step 289. On GPU resume at step 27.

* DAR <2017-05-08 Mon>
** Goals for Monday set Friday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Start with step 15 train tri3 models in gp + yaounde system.
- TODO SOFTunisia: what will it take to get a rough draft of speaker 2 to Zac?
I have the basic cd gmm hmm system trained.
I moving ahead from this point with nnet3.
I am setting the test data to the unsupervised SOFTunisia Answers data.
I hope this will produce transcripts for these data at some point soon.
Recall that I had taken around 35 steps to build an sgmm system.
I am starting The nnet3 model building from tri4 models that were built in a previous step 23 and 24.
tri4 models are adapted with feature space mllt and lda.
Could I start from tri5 speaker adapted training SAT models?
The first thing that has to be done in the nnet3 build is to extract i-vectors.
To extract i-vectors, an ubm is required.
mfcc features are made for Low Resolution Speed-Perturbed data for alignments.
fmllr alignment
I am separating the script that produces the ivectors into sevral scripts, one for each step.
1. Make speed perturbed data
2. Volume perturbation?
3. Extract high resolution (hires) mfcc features.
4. Do some kind of selection (not sure this is really doing anything)?
5.  Extract ivectors on speed perturbed data.
6. Make low resolution speed perturbed data.
7. Extract ivectors on low resolution speed perturbed data.


- TODO Vietnamese: run on CLSP cluster?
I downloaded and installed kaldi on a directory on the clsp cluster.
I do not know where the cuda toolkit is on their cluster.
Justin does not know either.
I'll have to ask for help to install kaldi with GPU.
I'll ask Yenda tomorrow.

** Goals for Tuesday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO SOFTunisia: start from local/nnet3/select_hires.sh
- TODO gp + yaounde: Run step 16 on my laptop to try to debug pron problem.

* DAR <2017-05-05 Fri>
**  Goals for Friday set Thursday:
- TODO Incremental build of gp + yaounde with semisupervised training and bnf and nnet3.
I'm running the feature extraction stet.
It looks like I've overcome the sorting problem in the training lists. 
Recall that there are 4 list files required:
1. wav.scp
2. text
3. utt2spk
4. spk2utt
Once you have these lists you can run feature extraction on the data referred to in the lists.
I haven't found the place where I originally separate fields with a space instead of a tab.
I'm doing it in the yaounde build.
I am starting the monophone training step.
Here are the steps I've done so far:
0. Data prep. The result of this are the 4 acoustic modeling list I itemized above.
1. Dictionary. I am not doing anything with the dictionary now, I am just copying our  previoius French dictionary. 
2. Lan preparation. This will be repeated after each model build starting at tri2.
3. lm. Again I am mostly copying the fr lm we had built previously.
4. Feature extraction. plp + pitch features.
5.  Training data subsetting.
6. Monophone training. I am also decoding a test set in case I want to give these models to transaps with decoding parameters.
7. Align with monophones. I guess we do not redo the lang directory at this point.
8.  Train tri1 model set.
9. Align with tri1 models.
10 Train tri2 models.
11. Get pronunciations with tri2. I do not understand what this step does yet.
12. Add prons. Again I do not understand this step yet.
13. Redo the lang directory using the results of steps 11 and 12.
14.  Align using tri2 models.
Next I need to train the tri3 model set.



- TODO Babel Vietnamese Semisupervised training with BNF and KWS.

It is still running.
Maybe I could run this on the clsp cluster?
Yenda might want to see results with nnet3 instead of nnet2 or nnet?

- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 

** Goals for Monday:
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- TODO Start with step 15 train tri3 models in gp + yaounde system.
- TODO SOFTunisia: what will it take to get a rough draft of speaker 2 to Zac?
- TODO Vietnamese: run on CLSP cluster?

* DAR <2017-05-04 Thu>
** Goals for Thursday set Wednesday:
- TODO Incremental build of gp + yaounde with semisupervised training and bnf and nnet3.
I organized the data directories for the corpora very neatly.
I found some problems.
I had some old versions of the niger wav file lists that caused the scripts to use file paths that did not exist.
I also found an interesting sorting problem.
I've probably run into this before.
I had written the yaounde utt2spk files with a singel white space as the field separator.
I used a tab to seperate the fields in the gp corpus.
This caused  kaldi to choke.
It has to sort the 2 fields.
It must include the field separator in the sort?
Anyway, I solved those problems and I think I'm done with data preparation.

- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 

- SOFTunisia semisupervised training with bnf and nnet3 is stuck on the sgmm build.

** Goals for Friday:
- TODO Incremental build of gp + yaounde with semisupervised training and bnf and nnet3.
- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 
- SOFTunisia semisupervised training with bnf and nnet3 is stuck on the sgmm build.

* DAR <2017-05-03 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Rebuild incrementally GP + Yaounde + CA for transapps team
I'm making a lot of progress on this.
I'm trying to organize the data in a way that is easy to understand.
Under the data directory I have folders for each corpus:
1. gp
2. yaounde
3. central accord
4. niger

- TODO TARP mandatory training
- TODO SOFTunisia semisupervised training with BNF and nnet3.
22. Align with tri3 models.
23. Train lda and mllt adapted tri4 models.
24. Reestimate pronunciation probs with tri4 models.
25. align with tri4 models.
26. Train speaker adapted SATtri5 models.
27. Reestimate pronunciations probs with tri5 models.
28. Align with tri5 models.
29. Reestimate again with aligned tri5 models? (strange)
30. Copy a test directory?
31. Train a UBM.
32. Train a subspace gaussian mixture model SGMM5.
33. 
- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
I think the decoding is still running, it's been several days.

- TODO Skillport lesson 3.

** Goals for Thursday:
- TODO Incremental build of gp + yaounde with semisupervised training and bnf and nnet3.
- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
- TODO Skillport lesson 3.
- TODO Mandatory TARP training. 

* DAR <2017-05-02 Tue>
**  Goals for Tuesday set Monday:
- TODO SOFTunisia semisupervised training with BNF and nnet3.
I'm starting from the beginning with a script for each step.
The dictionary preparation script needs to be fixed.
I'm going to assume I have the utf-8 dictionary instead of taking all the steps to convert the qcri buckwalter dictionary to utf8.
I realized that on my laptop I had to install kaldi.
I had done a make clean and had not finished the install.
I recompiled and installed kaldi on my laptop.
Steps:
1. prepare supervised data
2. Prepare acoustic model training lists (wav.scp, text, utt2spk and spk2utt)
This is probably only for the basic cd gmm hmm model training. 
3. Get the qcri dictionary ( I am skipping all the preparation steps it took to get this dicitonary).
4. Prepare the data/lang directory ( this runs the script to build the lexicon fst)
5. Get the subs data for lm training
6. Get a list of subs OOVs.
7. Skip subs segments with OOVs (should we be doing this?)
8. Prepare lm.
9. Format lm (This creates the G.fst fr grammar fst)
10. Extract plp features with pitch
11. Compute cmvn statistics.
12 subset the training data to build the models incrementally.
13. train monophones on very small subset of data 2500 utterances
14. Align using monophones
15. Train triphones on small data set 3000 utterances
16. Align with tri1 models.
17. Train tri2 models on most of the data.
18. Reestimate pronunciation probabilities. These reestimation steps take a long time.
19. Align with tri2 models.
20. Train tri3 models on all the SOF Tunisia data.
21 Reestimate lexicon probs with tri3 models.
22.  

- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
Decoding is running again for ever ...
- TODO Skillport lesson 3.
- DONE Meeting with transapps team.
I was impressed.
- TODO TARP mandatory training

** Goals for Wednesday:
- TODO Rebuild incrementally GP + Yaounde + CA for transapps team
- TODO TARP mandatory training
- TODO SOFTunisia semisupervised training with BNF and nnet3.
- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
- TODO Skillport lesson 3.

* DAR <2017-05-01 Mon>

**  Goals for Monday set Friday:
- TODO Babel Vietnamese semisupervised training with BFF.
The decoding that was running last week finished over the week-end.
There is a lot of work on kws in the scripts I am running now.
There are a couple of kws lists I could not find and that are refered to in the scripts.
I am copying other similar files to the missing names.
I also reached a couple of scripts that do g2p with sequitur.
Justin had to install sequitur for kaldi.

- TODO Semi supervised training with BNF to obtain transcripts for Zac
I ran alignment with the dnn nnet2 script.
I've decided I'm going to start the SOFTunisia build from the beginning.
I want to know what I'm doing at each step.
Up till now I'm mostly running scripts that worked on the Vietnamese corpus.
I also want to move from nnet2 to nnet3 as Yenda suggested.


- TODO Skillport Security+ lesson 3.

** Goals for Tuesday:
- TODO SOFTunisia semisupervised training with BNF and nnet3.
- TODO Babel Vietnamese Semisupervised training with BNF and KWS.
- TODO Skillport lesson 3.
- TODO Meeting with transapps team.
- TODO TARP mandatory training

* DAR <2017-04-28 Fri>
**  Goals for Friday set Thursday:
- TODO Skillport lesson on Vulnerabilities and penetration testing.
- TODO Semi supervised training on Babel Vietnamese.
- TODO Semi supervised training with bnf on SOFTunisia, specifically data prep.
.stm file:
segment time mark input file

** Goals for Monday:
- TODO Babel Vietnamese semisupervised training with BFF.
- TODO Semi supervised training with BNF to obtain transcripts for Zac
- TODO Skillport Security+ lesson 3.

* DAR <2017-04-27 Thu>
**  Goals for Thursday set Wednesday:
- DONE Escort Carmit around ARL. 
- TODO Skillport lesson on Vulnerabilities and penetration testing.
- TODO Semi supervised training on Babel Vietnamese.
Decoding is still running.
This is frustratingly slow.

- TODO Semi supervised training with bnf on SOFTunisia.
Making progress.
The p-norm training with nnet2 finished.
The next step according to the Vietnamese scripts is to train an automatic segmentor.
What is this?
Does it segment the Vietnamese character strings?
Does it segment the acoustic data?
Looking at the script, it seems llike it segments the acoustic data and not the text data.
I have to do more data preparation in the SOFTunisia directory before I can move forward.
I made progress making the data/raw_train directory which is required by the segmentor training script.

** Goals for Friday:
- TODO Skillport lesson on Vulnerabilities and penetration testing.
- TODO Semi supervised training on Babel Vietnamese.
- TODO Semi supervised training with bnf on SOFTunisia, specifically data prep.

* DAR <2017-04-26 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Semisupervised training with bnf for Babel Vietnamese.
The decoding runs without bnf ran overnight and are still running this morning.

- TODO Semisupervised training with bnf for SOFTunisia.
For some reason there is an argument to the reestimate_lang.sh script that is different in the SOFTunisia build than in the Vietnamese build.
The script looks for a file called silence_phones.txt.
This file isunder data/local in the Vietnamese build and under data/local/dict in the SOFTunisia.
I changed this argument in all but the last run of the reestimate_lang.sh script.
So I had to restart the run-1-main.sh script this morning for the SOFTunisia build.
The run-1-main.sh script finished. 
This script builds models mono, tri1, tri2 ...tri5 and  sgmm5.
I started the script run-2a-nnet-ensemble-gpu.sh.
this trains an nnet2 system.
Should we be using nnet3?
I sent Yenda this question.
I am now doing nnet2 training on the SOFTunisia data.
It is using the GPU.

- TODO Security+ on lesson skillport
I can't figure out how to move to the next lesson. This is a nightmare.
I found another lesson in the Skillport Security+ course.
It is on Vulnerabilities and Penetration Testing.

- DONE Play with openNMT.
Justin had installed OpenNMT and torch on the GPU machine.
I had to activate torch from my .bashrc.
The demo training is running.
It says it is doing sequence to sequence training with attention. This is cool!
I don't think I have it set to use the gpu.
It will run 154 iterations.

** Goals for Thursday:
- TODO Escort Carmit around ARL. 
- TODO Skillport lesson on Vulnerabilities and penetration testing.
- TODO Semi supervised training on Babel Vietnamese.
- TODO Semi supervised training with bnf on SOFTunisia.

* DAR <2017-04-25 Tue>
**  Goals for Tuesday set Monday:
- TODO Semisupervised training with bnf on Babel Vietnamese.
This is taking a long time to run. 
I am at a decoding stage.
I don't think I'm decoding with bnf yet.

- TODO Semisupervised training with bnf on SOFtunisia
The script reestimate_langp.sh gets used in the Babel Vietnamese semisupervised with bnf recipe.
What does this script do?
I am looking at it in the SOFTunisia build.
It has Yenda's name on it.
It gets run after each model build: i.e., mono, tri1, tri2, tri3, etc. 
Does it reestimate probabilities associated with each word in the pronouncing dictionary?
i'm getting a warning at the tri4 and  tri5 training stages.
WARNING (gmm-init-model[5.1.68~1390-7223]:InitAmGmm():gmm-init-model.cc:55) Tree has pdf-id 15 with no stats; corresponding phone list: 46 47 48 49 

- TODO Security + skillport lesson.


- OpenNMT:
I am installing Torch as part of the openNMT install.

** Goals for Wednesday:
- TODO Semisupervised training with bnf for Babel Vietnamese.
- TODO Semisupervised training with bnf for SOFTunisia.
- TODO Security+ on lesson skillport
- TODO Play with openNMT.

* DAR <2017-04-24 Mon>
**  Goals for Monday set Friday:
- TODO Semisupervised training with bottle neck features on IARPA Babel Vietnamese corpus.
The bottle neck features were trained.
Then lda + mllt training and SAT training were done.
Now I'm running sgmm training.
- TODO Replicate above experiment on SOFTunis.
To get started I am separating the folds for the supervised (Recordings_Arabic) data as follows:
dev: All the data from CTELLONE,
eval: All the data from CTELLTWO,
train: All the data from machines CTELLTHREE, CTELLFOUR and CTELLFIVE.
The sampling frequncy is set 8k in the Vietnamese configuration files I had to change this to 16k for SOFTunisia.
I have monophone training running.

- TODO Security + skillport lesson.

** Goals for Tuesday:
- TODO Semisupervised training with bnf on Babel Vietnamese.
- TODO Semisupervised training with bnf on SOFtunisia
- TODO Security + skillport lesson.

* DAR <2017-04-21 Fri>
**  Goals for Friday set Thursday:
- DONE SHARP training at 11:30.
- TODO Run scripts for semi supervised training on the IARPA babel Vietnamese corpus.
The run-2a-nnet-ensemble.sh script finished.
It created the tri6b_nnet model set.
Here is the comment at the head of the script:
# train an ensemble of neural networks with pnorm nonlinearities.
# An ensemble of nets are first differently initialized, and then trained using the
# same data during each iteration. 
# In each training iteration, one term is added to
# the objf, which is beta times the cross-entropy between the current net's posterior
# output and the geometrically averaged posterior outputs of the ensemble of nets.
# The beta values obey an exponentially increasing schedule (determined by initial_beta
# and final_beta).

I started the run-2b-bnf.sh script.
Here is the comment in that script
# This script trains a fairly vanilla network with tanh nonlinearities to generate bottleneck features


- TODO Start writing and investingating publication.

** Goals for Monday:
- TODO Semisupervised training with bottle neck features on IARPA Babel Vietnamese corpus.
- TODO Replicate above experiment on SOFTunis.
- TODO Security + skillport lesson.

* DAR <2017-04-20 Thu>
**  Goals for Thursday set Wednesday:
- TODO Remove dependence on queue in second run script in Vietnamese babel build.
I think I achieved this.
I had to set the --cmd option in the train_pnorm_ensemble.sh script.
That sent the correct option to the get_lda.sh script.
I found another --cmd option that referes to queue.pl in train_pnorm_ensemble.sh.
Where is the $cmd variable set for the train_pnorm_ensemble.sh script?
The main run 1 script had also crashed on mmi training of the sgmms for the same reason.
I set the --cmd to run.pl and the training is now running.
The sgmm mmi training finished.
That was quick.
I made a lot of progress today on this Vietnamese semisupervised training example.
There's a lot of processing yet to go.
The goal is to replicate this process for our SOFTunis corpus and maybe GP + Yaounde.


- TODO Security+ on skillport.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

** Goals for Friday:
- TODO SHARP training at 11.
- TODO Run scripts for semi supervised training on the IARPA babel Vietnamese corpus.
- TODO Start writing and investingating publication.

* DAR <2017-04-19 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Setup Vietnamese babel scripts (get some from jhu clsp server)
I spent more time than I expected downloading files from CLSP.
I am running the Vietnamese babel kaldi build in a separate directory instead of under the /home/tools/kaldi/egs/babel/s5d directory.
I am copying over only the files that I need to run the build.
The first main run is going well.
As I am getting ready to leave it is running the denominator lattice training.
I am working on a second run script that only requires the tri5 models.
So it can be run now since the main run is passed that step.
I am having trouble here because the script is hardwired to use queue.pl on a cluster.

- TODO Security+ on skillport.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

** Goals for Thursday:
- TODO Remove dependence on queue in second run script in Vietnamese babel build.
- TODO Security+ on skillport.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

* DAR <2017-04-18 Tue>
** Goals for Tuesday set Friday
- TODO Security+ MeasureUp exam.
- TODO Semi supervised transcripts for next SOFtunisia Answers speaker.
I am renaming many of my files to supervised and unsupervised from Recordings and Answers to be consistent with the babel scripts .
I want to incorporate the bottle nect features that the semisupervised babel recipe uses.
The utils/prepare_lang.sh script takes a long time to run with the larger dictionary.
It creates the l.fst and other FSTs.
The l.fst is the lexicon fst not the lm fst.
The lm fst is called G.fst for grammar fst.
The lm training does not take long.
I can't really decode on my laptop with these models.
If I do, I can't do anything else.
In the babel scripts for doing semisupervised training there is a script to get examples.
This script referrs to files that have "best_path" in their names.
I can find the log files with these names, but I haven't found 1 consolidated file.
There's a script in the babel recipes named  best_path_weights.sh.
I reaed more on semisupervised training in the babel recipe directory.
There is a script that runs the semisupervised recipe.
It uses Vietnamese.
I think the best way to folow there recipe is to actuallly run it.
We have the Vietnamese corpus under /mnt/corpora/babel.
I'm asking Justin to unzip the package containing the Vietnamese data.
It is called:
IARPA-babel107b-v0.7-build-copr.zip
I unzipped it into my home directory.
I'm starting to run the script for babel Vietnamese in the kaldi tools directory.
I'll have to go to the jhu clsp server to get some scripts.
It looks like Yenda owns these scripts.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

** Goals for Wednesday:
- TODO Setup Vietnamese babel scripts (get some from jhu clsp server)
- TODO Security+ on skillport.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

* DAR <2017-04-17 Mon>
**  Goals for Monday set Friday:
- TODO Security+ MeasureUp exam.
- TODO Semi supervised transcripts for next SOFtunisia Answers speaker.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

** Goals for Tuesday:

* DAR <2017-04-14 Fri>
**  Goals for Friday set Thursday:
- TODO Debug arpa2fst warnings.
Here is the comment at the beginning of the utils/prepare_lang.sh script
#                      Arnab Ghoshal
#                2014  Guoguo Chen
#                2015  Hainan Xu
#                2016  FAU Erlangen (Author: Axel Horndasch)
# This script prepares a directory such as data/lang/, in the standard format,
# given a source directory containing a dictionary lexicon.txt in a form like:
# word phone1 phone2 ... phoneN
# per line (alternate prons would be separate lines), or a dictionary with probabilities
# called lexiconp.txt in a form:
# word pron-prob phone1 phone2 ... phoneN
# (with 0.0 < pron-prob <= 1.0); note: if lexiconp.txt exists, we use it even if
# lexicon.txt exists.
# and also files silence_phones.txt, nonsilence_phones.txt, optional_silence.txt
# and extra_questions.txt
# Here, silence_phones.txt and nonsilence_phones.txt are lists of silence and
# non-silence phones respectively (where silence includes various kinds of
# noise, laugh, cough, filled pauses etc., and nonsilence phones includes the
# "real" phones.)
# In each line of those files is a list of phones, and the phones on each line
# are assumed to correspond to the same "base phone", i.e. they will be
# different stress or tone variations of the same basic phone.
# The file "optional_silence.txt" contains just a single phone (typically SIL)
# which is used for optional silence in the lexicon.
# extra_questions.txt might be empty; typically will consist of lists of phones,
# all members of each list with the same stress or tone; and also possibly a
# list for the silence phones.  This will augment the automatically generated
# questions (note: the automatically generated ones will treat all the
# stress/tone versions of a phone the same, so will not "get to ask" about
# stress or tone).
#

# This script adds word-position-dependent phones and constructs a host of other
# derived files, that go in data/lang/.


The utils/prepare_lang.sh script writes a list of words. 
I was confused because there are 500k words in this list which is different from the 2000k words in the qcri dictionary.
I sorted and uniqed the qcri dictionary words and I come up with 500k words too.
What about the words in the subs sample I'm using for the lm?
There are a total of 300k types in the subs sample I am using.
There are a total of 300k words in subs that are not in the data/lang/words.txt file.
After sorting and uniqing, I get 134k types in subs that are not in data/lang/words.txt.

I wrote a script that removes segments in my subs sample that have words that do not appear in the qcri dictionary.
I end up with 400k segments in my new subs sample down from 600k.
Now I only get 3 warnings from arpa2fst.
Why do I even get 3?
اذالم
اذالم
اذالم
Are these the same word?

- TODO Setup semi supervised  round 1 scripts.
- DONE skillport Security+ lesson 2  
Michelle got me through the lesson test.
I got a certificate of completion.
I don't believe this is correct.
It was too easy and I don't feel like I know enough about Security+.

** Goals for Monday:
- TODO Security+ MeasureUp exam.
- TODO Semi supervised transcripts for next SOFtunisia Answers speaker.
- TODO GP + Yaounde build revisit.
- TODO G2P for Arabic

* DAR <2017-04-13 Thu>
**  Goals for Thursday set Wednesday:
- TODO Incorporate new transcriptions into second round of human in the loop semisupervised training of SOFTunis ASR system.
8864 Recordings utterance files.
List all the .wav Recordings files in a file.
Make the wav.scp file for the Recordings.
Make the utt2spk file for the Recordings.
Make the text file for the Recordings.
List all the Answers .wav files in a file.
2980 Answers utterance files.
- New plan:
Separate into train and test folds
Train first on Recordings and test on Answers.
120 speakers
Get feedback on Answers transcriptions from Zac.
If the transcriptions are good, use them as semi supervised training labels.
Otherwise only use the transcriptions from the Speaker Zac transcribed by hand.
I passed the --max-arpa-warning=-1 to the arpa2fst program.
Now the arpa2fst program gives me a lot of warnings
The warnings claim an Arabic word is not in the word symbol table.
Where are these words coming from?
In the data/lang/words.txt file there are over 500k words.
Where do these words come from.
I think the warning words are words that are in the dictionary file but not in the words.txt file.
So there is no need to include them in the fst.
At least this is what arpa2fst assumes.
It looks like I have arpa2fst running now since it produces a G.fst file.
The decoder was crashing before because there was no G.fst file under data/lang.
I think the 500k words in data/lang/words.txt is the lexicon for the lm.
The lm is trained on subs.
Are the warnings OOVs?
Words that are in subs but not in the qcri dictionary?

- TODO Get help to pass skillport Security+ lesson 2 matching test


** Goals for Friday:
- TODO Debug arpa2fst warnings.
- TODO Setup semi supervised  round 1 scripts.
- TODO skillport Security+ lesson 2  

* DAR <2017-04-12 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Skillport Security+ Lesson 2 test.
- DONE LM for Arabic for SOFTunisia
The lm now is pretty big.
There are advantages and disadvantages to having such a large lm.
The results will probably be better with the larger lm.
However, the decoding graph takes a long time to build.
I am working on more restrictions in the script that filters the subs corpus.
I am only letting segments with between 8 and 16 words through.
This still lets a lot of segments through.
I guess maybe around 3 million.
I am further filtering by skipping segments with words that have characters that are not in Arabic script.
I end up with 600k segments.

- DONE Get hypotheses for Answers for speaker 1 to give to Zack.
I did this, but I realized later that I was using the tiny dictionary with only the 1481 words that appear in the Recordings prompts.
I fixed this and I am rerunning the system build with the larger qcri dictionary.
Realized that there is a phone "U" that is in qcri but not the workds that appear in the REcordings prompts.
Zac got back with transcriptions for the Answers by speaker 1 machine CTELLFIVE. 
Now I have to figure out how to incorporate the new transcriptions into the training process.
How did I do this for gp + yaounde?
I have to ponder this for a while.
I will have 3 kinds of transcriptions:
1. The Recordings transcriptions which should be correct.
2. The automatically generated transcriptions for the Answers from the decoding pass.
3. The hand corrected transcriptions done by Zac.

Should I incorporate the automatic transcriptions into training?

** Goals for Thursday:
- TODO Incorporate new transcriptions into second round of human in the loop semisupervised training of SOFTunis ASR system.
- TODO Get help to pass skillport Security+ lesson 2 matching test

* DAR <2017-04-11 Tue>
**  Goals for Tuesday set Monday:
- TODO Semi supervised training of SOFTunis system: get first rough draft of 1 speaker's Answers and pass them on to Zack.
I need to spend more time on the LM.
Steve got me the  parallel data for Arabic English from the Subtitles corpus.
Justin put the 2 files under /mnt/corpora/subs.
I am reusing the script I had to restrict the training data from the subs corpus to only segments of length between 5 and 25 words.
When I run the build scripts I am seeing warnings coming from the arpa2fst program.
I am skipping the levenshtein distance program for finding words close to OOVs.
 
- TODO Skill port Security+ course lesson 2.
 I started taking the test.
I got  1 question correct then I run into trouble again with the matching question that I had in lesson 1.
I'll have to get help again from Michelle to get me past this point.

** Goals for Wednesday:
- TODO Skillport Security+ Lesson 2 test.
- TODO LM for Arabic for SOFTunisia
- TODO Get hypotheses for Answers for speaker 1 to give to Zack.

* DAR <2017-04-10 Mon>
**  Goals for Monday set Friday:
- TODO Skillport Security+ course lesson 2.
I went through part of lesson 2 on categories of malware.

- TODO Apply semi supervised method from Gp + Yaounde to SOFTunis Answers.
I added the Answers into the scripts  to build the SOFTunisia system.
I'm going to run the decoder on the Answers data with the models built on the Recordings data.
As I'm getting ready to leave, the build is running on the monophone stage.
Hopefully, tomorrow I'll have the results of decoding the Answers. 
- DONE JAWS upgrade.
Steve submitted the request to cap.mil.

** Goals for Tuesday:
- TODO Semi supervised training of SOFTunis system: get first rough draft of 1 speaker's Answers and pass them on to Zack.
- TODO Skill port Security+ course lesson 2.
 
* DAR <2017-04-07 Fri>
**  Goals for Friday set Thursday:
- TODO Attend Voxtex presentation.
- TODO JAWS upgrade
- TODO Try to get help to move forward with skillport Security+ course.

- SOFTunisia 
I ran through the model building process:
mono: %WER 37.53 
tri1: %WER 26.81 

So there was improvement, but this is still really bad.
The tri2a decoding fails for some reason.
I find warnings in exp/tri2a/log/init_model.log like this:
WARNING (gmm-init-model:InitAmGmm():gmm-init-model.cc:83) Very small count for state 2: 45; corresponding phone list: 10 11 12 13 
WARNING (gmm-init-model:InitAmGmm():gmm-init-model.cc:83) Very small count for state 169: 81; corresponding phone list: 42 43 44 45 

I reduced the 2 parameters to the steps/train_deltas.sh script that limit the number of leaves and the total number of gaussians to 1000 and 4000.
OK! that looks like it helped.
The init_model.log file now has no WARNING lines.
I need to do the same thing for the tri1 models, they produce the same warnings and the num_leaves and total_gaussian parameters are set too high too.
I'm basically tuning the parameters here.
The training seems to go faster too.
I found another problem.
I was including all the .wav files in the corpus.
We only have transcriptions for  the Recordings (at least that is my understanding at the moment).
I was using the Answers too with transcripts  from the Recordings.
So for the Answer file numbered 51, I was using Recording prompt number 51 as its transcription.
Without the Answers there are 8864 utterences.
Here are results with these changes:
Mono: %WER 12.26 
tri1: %WER 3.92 
tri2b: %WER 3.69 
tri3b: %WER 5.97 
This is more like it.
There might still be something I can improve for tri3b.

** Goals for Monday:
- TODO Skillport Security+ course lesson 2.
- TODO Apply semi supervised method from Gp + Yaounde to SOFTunis Answers.
- TODO JAWS upgrade.

* DAR <2017-04-06 Thu>
**  Goals for Thursday set Wednesday:
- TODO Skillport Security+ course lesson 2.
I got stuck in the pretest.
There are 3 questions that require matching items from 2 lists. 
I do not know how to do this with JAWS.
I called the help desk.
Keith Garris was very helpful.
He was returning a call I had made last week?
I had already resolved that problem.
When he asked me if there was anything else he could help with, I asked him if he could help with the list matching in skillport.
He remoted into my computer and tried to help figure out how I could do the task with JAWS.
He had a little exposure to JAWS in the past.
Anyway ...
It's time to get an upgrade for JAWS.

- TODO Work on LM for SOFTunis system.
- DONE Extract dictionary specific for SOFTunis lexicon from qcri lexicon.
This took more work than expected.
There are 1483 entries in the softunisia dictionary as it stands now.

- TODO Figure out why results are bad for SOFTunis kaldi build.

** Goals for Friday:
- TODO Attend Voxtex presentation.
- TODO JAWS upgrade
- TODO Try to get help to move forward with skillport Security+ course.

* DAR <2017-04-05 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Retake measure up test.
I failed with 32 out of 50 questions correct.
The Security+ course has been postponed until late June according to Mike Landes.
I've decided to work on the skillport Security+ course which is also a requirement.
Today I think I got through lesson 1.

- DONE Read Chapters 11 and 12 of Comptia Security + Study Guide
- TODO Work on LM for SOFTunis system.
- TODO Extract dictionary specific for SOFTunis lexicon from qcri lexicon.
- DONE Send minimal English Tagalog model set to transapps team.
- DONE Add Carmit to my benefits.
This was a lot easier than I expected.
- TODO Figure out why results are bad for SOFTunis kaldi build.

** Goals for Thursday:
- TODO Skillport Security+ course lesson 2.

- TODO Work on LM for SOFTunis system.
- TODO Extract dictionary specific for SOFTunis lexicon from qcri lexicon.
- TODO Figure out why results are bad for SOFTunis kaldi build.

* DAR <2017-04-04 Tue>
**  Goals for Tuesday set Monday:
- DONE Write Accomplishments for Reggie
I wrote a list of accomplishments.
- TODO Retake measure up test.
- TODO Read Chapter 11 of Comptia Security + Study Guide
- TODO Work on LM for SOFTunis system.
- TODO Extract dictionary specific for SOFTunis lexicon from qcri lexicon.

I ran through the kaldi training steps for monophones through tri3b. 
I decoded the training set ( I haven't separated the corpus into training dev and test folds yet).
The results seem pretty bad, which means something might be wron.
The best wer is 27.46 and it was obtained with the tri1 models.
I did not get results for tri2a.

- Joshua models for Transapps:
I fired up the English -> Tagalog joshua pipeline script that had on the GPU machine.
It seems to be running.
At least it is running the tuning step, which means thrax and hadoop are running.
They should, since nothing really has been changed since we installed hadoop, thrax, joshua, etc.
I'm going to make a really small English -> Tagalog model set (2500 bisegments) for a minimal set to send to the transapp team.
As I'm getting ready to leave, tuning has completed 3 iterations on the minimal training and tuning set system.

** Goals for Wednesday:
- TODO Retake measure up test.
- TODO Read Chapters 11 and 12 of Comptia Security + Study Guide
- TODO Work on LM for SOFTunis system.
- TODO Extract dictionary specific for SOFTunis lexicon from qcri lexicon.
- TODO Send minimal English Tagalog model set to transapps team.
- TODO Add Carmit to my benefits.
- TODO Figure out why results are bad for SOFTunis kaldi build.

* DAR <2017-04-03 Mon>
**  Goals for Thursday set Wednesday:
- DONE Read Comptia Security+ guide.

- TODO Retake measure up test.
- TODO Write Mid term accomplishments for Reggie.

** Goals for Tuesday:
- TODO Write Accomplishments for Reggie
- TODO Retake measure up test.
- TODO Read Chapter 11 of Comptia Security + Study Guide
- TODO Work on LM for SOFTunis system.
- TODO Extract dictionary specific for SOFTunis lexicon from qcri lexicon.

* DAR <2017-03-29 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Read Security+ Study Guide.
I'm reading the chapter on Cryptology.
- TODO Retake the Security+ measure up test.
- DONE Get new batch of OOV pronunciations to Zack.
I think Zack wants to go ahead on his own with the remaining 89 OOVS.

** Goals for Thursday:
- TODO Read Comptia Security+ guide.
- TODO Retake measure up test.
- TODO Write Mid term accomplishments for Reggie.

* DAR <2017-03-28 Tue>
I spend the day reading the Security+ study guide.
I also started the process of generating the next batch of candidate OOV pronunciations for Zack.

** Goals for Wednesday:
- TODO Read Security+ Study Guide.
- TODO Retake the Security+ measureup test.
- TODO Get new batch of OOV pronunciations to Zack.

* DAR <2017-03-27 Mon>
**  Goals for Monday set Friday:
- TODO TRANSTAC data preparation (first steps)
- TODO Read Network+ Study guide.
- TODO Read Security+ Study Guide.
- TODO Get pronunciations for SOFTunisia OOVs from Zack.
- TODO Investigate Multilingual model method.

* DAR <2017-03-24 Fri>
**  Goals for Friday set Thursday:
- TODO Read Security+ and Network+ Study guides, forget A+.
Security+ assumes you know IP subnetting from Network+.
I'm going back to read about subnetting in the Network+ book.

- TODO Language model for SofTunisia.
Instead of softunisia, I trained the lm for French for the transapps team.

- TRANSTAC
Justin put the TRANSTAC corpus on /mnt/corpora.
I am trying to figure it out.
I'd like to separate out the Iraqi Arabic.
I can find the audio files, but I am having trouble associating the .wav files with transcription files.

** Goals for Monday:
- TODO TRANSTAC data preparation (first steps)
- TODO Read Network+ Study guide.
- TODO Read Security+ Study Guide.
- TODO Get pronunciations for SOFTunisia OOVs from Zack.
- TODO Investigate Multilingual model method.

* DAR <2017-03-23 Thu>
**  Goals for Thursday set Wednesday:
- TODO Get close pronunciations for OOVs to Zack.
There are 120 OOVs in this batch.
The first run yielded 1818 words from the qcri dictionary.
I had not incorporated Zack's corrections, so I am running the levenshtein script over again.
I removed the exclamation mark in the recordings prompts.
The second run yielded 2230 words from the qcri dictionary.
They only cover 31 OOVs.
That leaves 88 left to do.

- TODO Minimal French English Joshua example.
- TODO Read A+, Security+ and Network+ study guides.
I read chapter 5 of the Network+ Study guide.

** Goals for Friday:
- TODO Read Security+ and Network+ Study guides, forget A+.
- TODO Language model for SofTunisia.

* DAR <2017-03-22 Wed>
**  goals for Wednesday set Tuesday:
- TODO minimal joshua example

- OOVs 
Zack got his edits back to me.
I am running them through my scripts to git the levenshtein-close pronunciations.
The way I am doing this involves the following steps:
1. Append Zack's new dictionary entries to the qcri dictionary.
Note that the qcri dictionary has been converted to utf8.
2. Run the first couple of data prep steps for the kaldi build.
This means getting to the step that creates the data/lang/words.txt file and the data/training/text file.
3. Run the kaldi script check_oov.pl with the data/lang/words.txt and data/training/text files as input.
This produces the OOVS.
4. Run the script that prints words and their pronunciations that are within a threshold levenshtein distance of the OOVs.

** Goals for Thursday:
- TODO Get close pronunciations for OOVs to Zack.
- TODO Minimal French English Joshua example.
- TODO Read A+, Security+ and Network+ study guides.

* DAR <2017-03-21 Tue>
** Goals for Tuesday set Monday:
- TODO French English minimal joshua example  
This is really giving me a hard time.
Joshua used to be easy to run, not any more.

Joshua is broken!
The problem is getting thrax to run.
I am trying to run the example on the GPU machine.
It looks like one problem is that thrax was not compiled.

** goals for Wednesday:
- TODO minimal joshua example

* DAR <2017-03-20 Mon>
**  Goals for Monday set Friday:
- TODO Read chapters 3  and 4 of  Comptia Security+ Study Guide.
- DONE Read chapter 1 of  Comptia Network+ Study Guide.
I read Chapters 1-3 over the week-end.
- TODO Finish IDP goals.
- TODO Secure the Arabic text data from the field manuals and Ranger handbook.
- TODO Convert the GALE Arabic transcripts from Buckwalter to UTF8.
- TODO Process the ksu data including text for the SOFTunis lm.
- TODO Process the Transtac Arabic data.

* Goals for Tuesday:
- TODO French English minimal joshua example  

* DAR <2017-03-17 Fri>
** Goals for Friday set Thursday:
- TODO LM for SOFTunis ASR build.
I finally got through the GALE Arabic data preparation steps.
Justin helped me get the directories better organized under /mnt/corpora.
The transcripts that are used to train the lm are in Buckwalter.
This is not a big deal, but at some point I'll have to convert them to utf8.

I have not succeeded in getting the Arabic from the field manuals.
- TODO Read chapters 3  and 4 of  Comptia Security+ Study Guide.
- TODO Read chapter 1 of  Comptia Network+ Study Guide.
- TODO Finish IDP goals.
Where do I do this?

** Goals for Monday:
- TODO Read chapters 3  and 4 of  Comptia Security+ Study Guide.
- TODO Read chapter 1 of  Comptia Network+ Study Guide.
- TODO Finish IDP goals.
- TODO Secure the Arabic text data from the field manuals and Ranger handbook.
- TODO Convert the GALE Arabic transcripts from Buckwalter to UTF8.
- TODO Process the ksu data including text for the SOFTunis lm.
- TODO Process the Transtac Arabic data.

* DAR <2017-03-16 Thu>
**  Goals for Thursday set Wednesday:
- DONE Read chapters 2 and 3  of  Comptia Security+ Study Guide.
Comptia assumes you've taken the Network+ exams for the Security+ exam.
I'm going to start reading the Network+ study guide too.
It's actually interesting and useful.
- DONE Get close pronunciations for OOVs to Zack.
I ended up with a total of 5349 words from the qcri dictionary that were within Levenshtein distance 3 of the OOVs.
- TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- DONE Convert the rest of the work repos on github  to repos on the GPU machine.
The repositories remaining were not done for ARL, so I'm leaving them.
- TODO Review form266 pdf file on ARL Open Source Guidelines and Instruction. 
- TODO Build an lm for SOFTunisia.
I'm collecting text data.
Justin put the Arabic fm texts that were on the L drive on the GPU machine.
- TODO Finish IDP goals.

** Goals for Friday:
- TODO LM for SOFTunis ASR build.
- TODO Read chapters 3  and 4 of  Comptia Security+ Study Guide.
- TODO Read chapter 1 of  Comptia Network+ Study Guide.
- TODO Finish IDP goals.

* DAR <2017-03-15 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Read chapter 1 of  Comptia Security+ Study Guide.
I feel pretty good about this after answering the practice questions at the end of the chapter.
- TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- DONE Find pronunciations for OOVs   from the SOFTunis transcripts in qcri dictionary.
I think I have the Levenshtein distance script working.
1. load the qcri dictionary in a hash.
2. for each o in OOVs:  Loop over the 228 OOVs.
3. For each w in qcri: Loop over the qcri dictionary entries.
4l(o,w): . Compute the Levenshtein distance between the oov and the qcri dictionary entry.
5. If l(w,o) < 3 write w and its pronunciation.

When I only wrote if the Levenshtein distance was less than 2, I did not get any words after a long time.
I'm setting the threshold to 3.

- TODO Build an lm for SOFTunisia.
- DONE Work on ARL Open Source Guidelines and Instruction with Cem.
I downloaded the acrotex and conv-xkv packages from https://www.ctan.org/.
wget http://mirrors.ctan.org/macros/latex/contrib/conv-xkv.zip
wget http://mirrors.ctan.org/macros/latex/contrib/conv-xkv.zip
I unzipped the archives.
I ran latex acrotex.ins and latex conv-xkb.ins
I copied the directories to /usr/share/texlive/texmf-dist/tex/latex
I ran texhash.
Cem's make now runs.

So far, the pdf form looks pretty good.
- DONE Convert 5 github repos to bare repos on the GPU machine.
I'm almost done removing work repos from github.
** Goals for Thursday:
- TODO Read chapters 2 and 3  of  Comptia Security+ Study Guide.
- TODO Get close pronunciations for OOVs to Zack.
- TODO - TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- TODO Convert the rest of the work repos on github  to repos on the GPU machine.
- TODO Review form266 pdf file on ARL Open Source Guidelines and Instruction. 
- TODO Build an lm for SOFTunisia.
- TODO Finish IDP goals.

* DAR <2017-03-14 Tue>
** Goals for Tuesday set Monday:
- TODO Read chapter 1 of  Comptia Security+ Study Guide.
- TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- DONE Find OOVs for SOFTunis transcripts in qcri dictionary.
I found 227 OOVs.

I spent most of the day working with dictionaries.
I'm trying to get words  in the QCRI dictionary that are close (in the levenshtein distance sense) to the 227 OOV words.
The problem is making this tractable.
- TODO Build an lm for SOFTunisia.
- TODO Work on ARL Open Source Guidelines and Instruction with Cem.
- DONE Convert github repos to bare repos on the GPU machine.
I converted several repos this morning.
I still have 28 to go.

** Goals for Wednesday:
- TODO Read chapter 1 of  Comptia Security+ Study Guide.
- TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- TODO Find pronunciations for OOVs   from the SOFTunis transcripts in qcri dictionary.
- TODO Build an lm for SOFTunisia.
- TODO Work on ARL Open Source Guidelines and Instruction with Cem.
- TODO Convert 5 github repos to bare repos on the GPU machine.

* DAR <2017-03-13 Mon>
**  Goals for Monday set Friday:
- DONE Buckwalter to Unicode on QCRI dictionary.
I converted the qcri dictionary types from Buckwalter to utf8 using the Encode::Arabic::Buckwalter perl module.
I think this worked.
I am not sure how many OOVs we have now.
- DONE SOFTunisia build.
It is running on the SOFTunis corpus.
I am going to run through tri3b models and decode on the training data.
- TODO Read chapters 14 and 15 of Comptia A+ Complete Study Guide.
Over the week-end I read  chapters 18, 19, 20 and 21.
I skipped for now chapter 14, 15, 16 and 17. 
They all deal with windows.
I need to spend more time on windows.
I'm going to try to work on my windows notebook at home where I have admin privileges.
I also downloaded the Comptia Security+ Study Guide and started reading it.

** Goals for Tuesday:
- TODO Read chapter 1 of  Comptia Security+ Study Guide.
- TODO Read chapters  14 and 15 of Comptia A+ Complete Study Guide.
- TODO Find OOVs for SOFTunis transcripts in qcri dictionary.
- TODO Build an lm for SOFTunisia.
- TODO Work on ARL Open Source Guidelines and Instruction with Cem.
- TODO Convert github repos to bare repos on the GPU machine.

* DAR <2017-03-10 Fri>
**  Goals for Friday set Thursday:
- TODO Map small Santiago Arabic dictionary to Transtac phone  set for SOF Tunis ASR system build with Kaldi.
This is not worth doing.
- TODO Read chapters 13 and 14 of Comptia A+ Complete Study Guide.
- TODO Check on status of Librispeech.

- QCRI dictionary:
I found a perl module for buckwalter.
Encode::Arabic::Buckwalter

** Goals for Monday:
- TODO Buckwalter to Unicode on QCRI dictionary
- TODO SOFTunisia build.
Read chapters 14 and 15 of Comptia A+ Complete Study Guide.
* DAR <2017-03-09 Thu>
** Goals for Thursday set Wednesday:
- DONE Minimal decoding with gp nnet2 online models
I put these models under:
/home/data/ateam/gp_fr_nnet2_online_components
The README.txt file has a minimal command line.

- DONE Incorporate Transtac Iraqi Arabic Dictionary into SofTunis ASR system build.
I got this to work.
I deleted the # sign from the new dictionary -- it is not allowed in kaldi.

- TODO Read chapters 12 and 13 of Comptia A+ Complete Study Guide.
Chapter 12 is on troubleshooting. It is very long. I read the section on troubleshooting Printers. Next is the  section on networking.
I finally finished chapter 12 today.
That covers the first 220-901 part of the A+ exams.
Chapter 13 is on Operating Systems in general.
- DONE IDp in ACT
I got this started:
Steve told me to go to:
https://actnow.army.mil/
I put Reggie's name and series 1550 and cp16.
I hit submit and accept.
I'm not sure if this worked or not.
Reggie got it.
I probably need to fill out my goals.

- Women's History Month:
I went to Melissa Flag's talk.

** Goals for Friday:
- TODO Map small Santiago Arabic dictionary to Transtac phone  set for SOF Tunis ASR system build with Kaldi.
- TODO Read chapters 13 and 14 of Comptia A+ Complete Study Guide.
- TODO Check on status of Librispeech.

** DAR <2017-03-08 Wed>
** Goals for Wednesday set Tuesday:
- TODO SOFTunis data/lang directory.
- TODO Convert gale arabic dictionary from Buckwalter to utf8.
- DONE GP online models.
The neural network training is running.
It will run for 384 passes.

- TODO Librispeech. 
- TODO Read chapters 11 and 12 of Comptia A+ Complete Study Guide.
At  COB today I am in the middle of chapter 12 Hardware and Network Troubleshooting, Troubleshooting Storage Device problems, Identifying Optical Drive Issues.

** Goals for Thursday:
- TODO Minimal decoding with gp nnet2 online models
- TODO Incorporate Transtac Iraqi Arabic Dictionary into SofTunis ASR system build.
- TODO Read chapters 12 and 13 of Comptia A+ Complete Study Guide.
- TODO IDp in ACT
 DAR <2017-03-07 Tue>
** Goals for Tuesday set Monday:
- DONE Read chapters 9 and 10 from Comptia A+ Complete Study Guide.
At COB today I am in the middle of the Laser Printers section  in chapter 11 on printers.

- TODO Train Online models for gp.
This is running on the GPU.
I had started much earlier on my laptop, but because the GPU is much faster than the CPU, the training on the GPU is on pass 10 while the training on the CPU is only on pass 8.

- TODO Librispeech 
This is also running on the GPU.
It is surprising that 2 jobs can be using the GPU.
- TODO convert github repos to bare repos.
I converted the gp repo to a bare repo.
- TODO SofTunis lexicon and lm.
I am stuck on the step that makes the data/lang directory.
The script that does this is choking on the <unk> symbol.
For now I want to get a minimal example working with our SANTIAGO dictionary.

** Goals for Wednesday:
- TODO SOFTunis data/lang directory.
- TODO Convert gale arabic dictionary from Buckwalter to utf8.
- TODO GP online models.
- TODO Librispeech. 
- TODO Read chapters 11 and 12 of Comptia A+ Complete Study Guide.

* DAR <2017-03-06 Mon>
**  Goals for Monday set Friday:
- TODO Incorporate LDC2017L01 dictionary into the SOFTunis kaldi system build.
- DONE Read chapters 8 and 9 of Comptia A+ Complete Study Guide.
At COB today I'm reading the section Display Port and Thunderbolt under The next Generation of laptop expansion cards under Understanding Laptop Architecture in Chapter 9.

- TODO librispeech tdnn on 960 hours of speech.
The first thing the script does seems to be extracting high resolution mfcc features?
I hope so, since these get used in training the online models.
- DONE Prepare models for delivery to Transapps team.
I prepared a minimal example with gp monophones.

- github repositories
I spoke with Chem Friday about git and github repositories.
I asked him what ARL's policy is concerning github repositories.
He said that a repository on github  that has code created at work should be made into an open source project.
For my purposes though he said what I really want is to create a bare repository.
I think I figured out how to make a bare repository today.
I used the following command on the GPU machine under /home/john:
git init --bare --shared=all softunisia.git
This creates a bare repository with name softunisia.git.
The next question was how to I convert the softunisia repo that I already had to use the new bare repository.
I went into the .git/config file and changed the url variable to point to the new bare repository on the GPU machine.
I set url to john@131.218.130.12:/home/john/softunisia.git
This seems to have done the job.
I pushed a commit.
I was prompted for my password.
Now I can delete the softunisia repo from github and I will ot be going outside our rednet.
I do this same procedure for all my repos.

- Online models:
The Transapps team is requesting the ASR components for decoding.
Now I have to put my money where my mouth is.
I setup a minimal example for the gp corpus system.
The decoder command is at:
/home/data/ateam/gp_mono_components/README.txt.
It only requires 2 files:
1. hclg.fst
2. final.mdl
There is another file:
words.txt
but I don't think it it required.
This is a very minimal example.
I copied the data/eval directory to the /home/data/ateam/gp_mono_components directory so the command could actually run.
The data/eval directory avs the files needed to point the decoder to the data files to be decoded.
I think we'll have to train online models to avoid using the files under data/eval in this way.

** Goals for Tuesday:
- TODO Read chapters 9 and 10 from Comptia A+ Complete Study Guide.
- TODO Train Online models for gp.
- TODO Librispeech 
- TODO convert github repos to bare repos.
- TODO SofTunis lexicon and lm.
* <2017-03-03 Fri>
**  Goals for Friday set Thursday:
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
Interesting fact from chapter 6.
The structure of TCP/IP is based on DoD the model.
The DoD model maps to the OSI model layers.
4 DoD layers map to 7 OSI layers.
Process or Application -> Presentation  + Application + Session
Host to Host -> Transport 
Internet -> Network
Network Access -> Datalink + Physical.

At COB, I've read chapters 6 and 7. 
In chapter 8, I am reading the section on wireless encryption.
- TODO Prepare softunisia data, including lexicon, lm and mfcc features.
Steve found a recent LDC delivery LDC2017L01 that contains a pronouncing dictionary for Arabic.

- TODO librispeech tdnn on 960 hours of speech.
The clean up is still running.
It does a lot of things.

- DONE Conference call with JHU and TransApps
Yenda was very helpful and cooperative.
** Goals for Monday:
- TODO Incorporate LDC2017L01 dictionary into the SOFTunis kaldi system build.
- TODO Read chapters 8 and 9 of Comptia A+ Complete Study Guide.
- TODO librispeech tdnn on 960 hours of speech.
- TODO Prepare models for delivery to Transapps team.

* DAR <2017-03-02 Thu>
**  Goals for Thursday set Wednesday:
- DONE Prepare transcripts for SOFTunis data.
I made the wav.scp, utt2spk, text and spk2utt files for the Recordings_Arabic data.

- TODO Rerun librispeech clean up on 960 hour corpus.
It is still running.
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
I'm not finished with chapter 6 yet.

** Goals for Friday:
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
- TODO Prepare softunisia data, including lexicon, lm and mfcc features.
- TODO librispeech tdnn on 960 hours of speech.

* DAR <2017-03-01 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Read chapters 5 and 6 of Comptia A+ Complete Study Guide.
I read chapter 5.
I am in the middle of reading chapter 6 on networking.
I am reading the section called "Explaining Ethernet Naming Standards"

- TODO Chain models on librispeech.
There was a problem.
It looks like the clean up process failed yesterday or today with the brown out.
The GPU is reporting about 30% utilization, but I don't see any processes running.
I asked Justin to reboot the GPU machine.

- DONE Data prep for SOFTunis
I downsample the Tunisia data to 16000 HZ.

** Goals for Thursday:
- TODO Prepare transcripts for SOFTunis data.
- TODO Rerun librispeech clean up on 960 hour corpus.
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

* DAR <2017-02-28 Tue>
** Goals for Tuesday set Monday:
- DONE Read chapters 3 and 4 of Comptia A+ Complete Study Guide.
- TODO chain models on Librispeech.
I am running the clean up script.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

- SOFTunisia:
We found a copy of the SOFTunis Arabic speech corpus.
I made a new repository for the scripts to process this corpus.

** Goals for Wednesday:
- TODO Read chapters 5 and 6 of Comptia A+ Complete Study Guide.
- TODO Chain models on librispeech.
- TODO Data prep for SOFTunis

* DAR <2017-02-27 Mon>
**  Goals for Monday set Friday:
- TODO Read chapter 2 of Comptia A+ Complete Review Guide.
I'm actually reading the Comptia A+ Complete Study Guide.
Today, I finished reading Chapters 1, 2 and part of chapter 3.
1. Motherboards, Processors and Memory.
2. Storage Devices and Power Supply.
3. Peripherals  and Expansion.

I'm on the paragraph on input devices, specifically, keyboards.

- TODO Librispeech build.
I am running the quick SAT training with 960 hours of speech (tri6b).

** Goals for Tuesday:
- TODO Read chapters 3 and 4 of Comptia A+ Complete Study Guide.
- TODO chain models on Librispeech.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.
- TODO Chain model training for librispeech.

* DAR <2017-02-24 Fri>
**  Goals for Friday set Thursday:
- TODO Read Chapter 1 of Comptia A+ Complete Review Guide.

I'm not finished yet at COB today.

** Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

** Goals for Monday:
- TODO Read chapter 2 of Comptia A+ Complete Review Guide.

* DAR <2017-02-23 Thu>
**  Goals for Thursday set Wednesday:
- DONE Read lesson 16 from Cyber Security Fundamentals.
I'm ready to take the test.
- DONE Take the cyber security fundamentals test (Can I do this?)
I had to go to the test link on the opening page.
I passed with a 75.

- TODO Linux+ on skillport.
I looked into this.
There does not seem to be anything for LX0-101 and LX0-102.
Justin says it is not a matter of knockingout a couple of test.
The Linux+ is a real Comptia Certification.
Maybe Reggie had it wrong?

** Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

** Goals for Friday:
- TODO Read Chapter 1 of Comptia A+ Complete Review Guide.

* DAR <2017-02-22 Wed>
**  Goals for Wednesday set Tuesday:
- Todo Lessons 8-16 of Cyber Security Fundamentals.

** Certification todo list:
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. TODO Sign PLA. 
4. TODO Take Cyber security Fundamentals Course 
a. DONE Lesson 1: Army Information Assurance Program.
b. DONE Lesson 2: Federal Laws, DoD Regulations and Policies.
c. DONE Lesson 3: Army Regulations and Policies.
d. DONE Lesson 4: Army Information Information Assurance Training Program
e. DONE Lesson 5: Network and Hacker Threats.
f. DONE Lesson 6. Malware
g. DONE Lesson 7: Physical Security
8. DONE Lesson 8: Risk Assessment and Management.
9. DONE Lesson 9: Security and Incident Response  Planning
10 DONE Lesson 10: Continuity Of Operations (COOP)
11. DONE Lesson 11: Department of Defense Information Assurance Certification and Accreditation Process (DIACAP).
12. DONE Lesson 12: Wireless Security (802.11)
13. DONE Intrusion Detection Systems (IDS) and Auditing.
14. DONE Lesson 14 Firewalss and Perimeter Defense.
15. DONE Lesson 15: Encryption and Common Access Cards.
16. TODO Lesson 16: Legal.



5. TODO Pass Cyber security Fundamentals Course:
6. 

I spent the whole day reading the Cyber Security Fundamentals lessons 8-16. 
I  should read lesson 16 again since I only skimmed through it this afternoon.

** Goals for Thursday:
- TODO Read lesson 16 from Cyber Security Fundamentals.
- TODO Take the cyber security fundamentals test (Can I do this?)
- TODO Linux+ on skillport.

* DAR <2017-02-21 Tue>
** Certification todo list:
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders:
3. TODO Sign PLA 
4. TODO Take Cyber security Fundamentals Course 
5. TODO Pass Cyber security Fundamentals Course:
 The lessons for this course are on the web and luckily they seem to be very accessible.
Today I've gone through:
a. Lesson 1: Army Information Assurance Program.
b. Lesson 2: Federal Laws, DoD Regulations and Policies.
c. Lesson 3: Army Regulations and Policies.
d. Lesson 4: Army Information Information Assurance Training Program
e. Lesson 5: Network and Hacker Threats.
f. Lesson 6. Malware
g. Lesson 7: Physical Security

.
6. 
** Goals for Wednesday:
- Todo Lessons 8-16 of Cyber Security Fundamentals.
* DAR <2017-02-14 Tue>
**  Goals for Tuesday set Monday:
- DONE Gp + Gabon Read + Gabon CONV evaluation for mono, tri1, tri2a, tri3b, tri4b and tri5b .
I have to spend some time on writing a script to get the results from the files written by kaldi.
- TODO Build nnet2, sgmm and sgmm2 models on gp+gabon read+gabon conv.
This is running today.
The nnet2 script is using the GPU.
- TODO librispeech nnet5a on {GPU.
This is also using the GPU.
I was surprised to find that you could run 2 scripts that both use the GPU. 
-TODO Demo
 I read some of the kaldi tutorial.
Specifically, about ark and scp files.
This helped me get a better understanding of how to compute mfcc features.
There is a kaldi program called compute-mfcc-feats that extracts mfcc features from .wav files and stores them in a file.
It takes 2 arguments:
1. a wav_rspecifier 
2. a feats_wspecifier

The 2 arguments involve ark and scp files.

- gp + gabon read + gabon conv  nnet2 models on the GPU:
When I left yesterday, I started a script to run the nnet2 p-norm training on the gp + gabon read + gabon conv data.
I was not sure if the script would run.
This morning it has gone through 267 iterations and the GPU is running at 99%.
It will go through 480 iterations, so it might be done this afternoon.

** Goals for Wednesday:
- TODO Kaldi Workshop

* DAR <2017-02-13 Mon>
**  Goals for Monday set Friday:
- TODO Librispeech
I started the training for the tri5b models.
tri5b is built by running more sat on top of tri4b with 460 hours of speech.
Recall that tri4b was SAT on 100 hours of speech.
The tri5b training finished.
I started the evaluation on tri5b.
I also started the nnet5a training.
I killed it because I saw that it is not using the GPU.
It is set to use 16 threads instead.
At COB today, I am restarting the nnet5a training with number of threads set to 1.
This should trigger the GPU.
 
- DONE build gp + gabon read.
Putting everything in one run.sh script was not working, so I've gone back to my method of putting steps in their own scripts that have names beginning with numbers.
The numbers make it easy to reproduce my steps.
I have mllt lda sat tri3b models built on gp.
I took the following steps: 
1. I extracted mfcc features from the gabon read data.
2. I consolidated the gp and gabon read data directories.
This wrote new spk2utt, text, utt2spk and wav.scp files for the consolidated gp and gabon read corpora.
3. I  aligned the gp gabon read data with the tri3b models.
4.  I  ran sat  training on the gp gabon read data to get tri4b models.
This tri4b training step finished.
I also did the following steps:
1. I extracted mfcc features from the gabon conv data.
2. I consolidated the gp, gabon read and gp conv data directories.
This wrote new spk2utt, text, utt2spk and wav.scp files for the consolidated gp,  gabon read and gp conv corpora.
3. I  aligned the consolidated gp, gabon read and gabon conv data with the tri4b models.
4.  I  ran sat  training on the gp, gabon read and gabon conv data to get tri5b models.

At COB today, I am running the decoding scripts for the model sets I've built so far.
- TODO Demo
   I put a little work into this today.
I am trying to get a minimal example for the mfcc extraction step.
The script that wants to split the data into chunks to run in parallel is getting in the way.
The utt2spk and spk2utt files are also being used and I'm not sure they are required.

** Goals for Tuesday:
- TODO Gp + Gabon Read + Gabon CONV evaluation for mono, tri1, tri2a, tri3b, tri4b and tri5b .
- TODO Build nnet2, sgmm and sgmm2 models on gp+gabon read+gabon conv.
- TODO librispeech nnet5a on {GPU.
-TODO Demo
 
* DAR <2017-02-10 Fri>
**  Goals for Friday set Thursday:
- TODO Librispeech 
At COB today I am aligning the 460 hour data set with the tri4b models.
tri4b is a speaker adapted system trained on 100 hours.
- DONE Gp build for gp + gabon 
I am starting to prepare the gabon read to incorporate it into the gp+gabon system.
The test set makes this a little tricky.
Our test set has speaker directory names that are different from the speaker directory names in the training set.
I parsed the speaker number and recording number out of the SRI transcribed test data files. 
I did the same for our test set data files and matched them.
- Caveat: this did not work.
The  recording names do not match.
The speakers can easily be mapped, but the recording names have to be aligned one by one.
We now have 2 test sets from the Gabon data collection.
1. Steve's 515 selected utterances from 12 speakers.
2. The 375 utterances that overlap between the SRI transcripbed read data and Steve's test set.
 
** Goals for Monday:
- TODO Librispeech
- TODO build gp + gabon read.
- TODO Demo

* DAR <2017-02-09 Thu>
**  Goals for Thursday set Wednesday:
- TODO Set up and build a gp + gabon system with the gabon data we sent to Voxtek.
As part of this goal, I made scripts to process the gabon_conv data and put them in the gabon-conversational repo.
At COB today, I've processed the gp data, the dictionary and the language model.
I plan on building sgmm models with the gabon data on top of the gp data.
I'll build the tri3b models only with gp
- DONE Train tri1 librispeech models.
At COB today I am training SAT models tri4b on 100 hours of speech.

- TODO Accent id
- TODO Demo
- DONE LM for gp + gabon
I included:
1. GP training prompts
2. Yaounde training (red) prompts.
3. BIC
4. Gabon read training prompts.
5. Gabon conv training transcripts.
6. subs

** Goals for Friday:
- TODO Librispeech 
- TODO Gp build for gp + gabon 

* DAR <2017-02-08 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Train English monophones on librispeech,
I ran the monophone training.
I started making the fst and decoding with the mono models.
I also started the alignment with the mono models.
These initial steps  are done with a relatively small amount of data.

- TODO Make a separate repo for the conversational gabon/sri/central accord speech transcripts processing scripts,
I made  gabon-read and gabon-conv repos.
I am writing processing scripts.
- TODO Incorporate the above scripts in the accent id repo,
- TODO Drill further down into gp demo.

** Goals for Thursday:
- TODO Set up and build a gp + gabon system with the gabon data we sent to Voxtek.
- TODO Train tri1 librispeech models.
- TODO Accent id
- TODO Demo
- TODO LM for gp + gabon

* DAR <2017-02-07 Tue>
**  Goals for Tuesday set Monday:
- TODO Niger data prep scripts.
- TODO Consolidate Niger scripts into accent id scripts.
- TODO Ditto for Central Accord/sri_gabon conversational speech scripts.
I did not get to this, but instead I put the gp processing scripts into the accent id repo.
Now we have scripts for 3 data sets under the accent id repo:
1. yaounde
2. niger
3. gp
- TODO Librispeech build.
I did not do a lot on this today, but I did run the following:
1. data preparation,
2. downloaded the pre-built language models,
3. Dictionary preparation,
4. lang directory preparation (this is mostly lexicon preparation),
5. format the language models (arpa to fst  and const arpa format),  
6. mfcc feature computation,
7. 

Maybe tomorrow I'll be able to start monophone training.
The first next step is to make data subsets.

- Demo project:
I took a first step at building a  speech to speech demo.
I setup a script to run a single wav file through the kaldi decoder.
I did this for the gp system.
I have  this tiny demo running on hclg fst files for  the following models:
1. mono
2. tri1

I'm in the process of getting it to work for:
3. tri2a
4. tri2b
5. tri3b
6. sgmm

** Goals for Wednesday:
- TODO Train English monophones on librispeech,
- TODO Make a separate repo for the conversational gabon/sri/central accord speech transcripts processing scripts,
- TODO Incorporate the above scripts in the accent id repo,
- TODO Drill further down into gp demo.

* <2017-02-06 Mon>
**  Goals for Monday set Friday:
- DONE Do another pass on the conversational speech transcripts conditioning.
Steve took a look at the results and it looks good.
- TODO  Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.
Today I put the scripts to process the yaounde read transcripts in the accent-id repo.
I'm currently working on the niger scripts in its own repo.
As I am getting ready to leave, the next step to take tomorrow is the script that gets the Niger speaker names. 
- librispeech:
librispeech is a free corpus of English read speech.
I had Justin put it on /mnt/corpora.
I can build an ASR system with it and use it to make an i-vector extractor.

** Goals for Tuesday:
- TODO Niger data prep scripts.
- TODO Consolidate Niger scripts into accent id scripts.
- TODO Ditto for Central Accord/sri_gabon conversational speech scripts.
- TODO Librispeech build.

* DAR <2017-02-03 Fri>
** Goals for Friday set Thursday:
- TODO Process the SRI provided transcripts for the CA16 data. Specifically, the conv tdf files and the read speech transcripts that Steve gave me today.
I worked all day on the conversational speech transcripts.
I removed pronunciation comments.
I processed comments in parens and double parens.
I may have deleted more text than I should have.
I'll have to go back and check what is done to the text in parens.

- DONE Make a repo for the Niger corpus processing scripts. Specifically, we want a place for the official transcripts.
I made  the repo and put the niger transcripts there.

- TODO consolidate the central accord test data and the sri_gabon directory on /mnt/corpora.
This will not happen for a while.

** WAR:
John Morgan conditioned a set of transcripts for the conversational part of the African  French speech corpus that was collected in Libreville, Gabon in 2016. 
The transcripts are intended to be used for adaptation of European French acoustic models to African accented French. 
Adaptted acoustic models will be used to improve speech-to-speech applications on hand-held devices.

** Goals for Monday:
- TODO Do another pass on the conversational speech transcripts conditioning.
- TODO  Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

* DAR <2017-02-02 Thu>
**  Goals for Thursday set Wednesday:
- TODO Investigate deeper the LID recipe in kaldi to see if we can use if for AID.
It would really help if we could get the LDC callfriend corpora, but not absolutely necessary.
We may already have them, but they are not on the /mnt/corpora disk.

- TODO Process the transcripts made by SRI for the CA16 corpus.
I worked on this today.
This will be the priority for the next couple of days.

- TODO Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

** Goals for Friday:
- TODO Process the SRI provided transcripts for the CA16 data. Specifically, the conv tdf files and the read speech transcripts that Steve gave me today.
- TODO Make a repo for the Niger corpus processing scripts. Specifically, we want a place for the official transcripts.
- TODO consolidate the central accord test data and the sri_gabon directory on /mnt/corpora.

* DAR <2017-02-01 Wed>
** Goals for Wednesday set Tuesday:
- TODO Build SGMMs for gp.
training finished over night on my laptop.
I started alignment this morning.

- TODO Build dnn i-vector extractor.
This would be cool, but I'm not sure now if it is necessary.

- Accent ID plan:
Omit seemed to suggest that we could follow the recipe that was used for LID to do Accent ID.
I would like to look into this.
My understanding of LID is that given some speech as input, LID outputs the best guess at the language spoken by the speaker.
The pyspeech tutorial is trained with 57 languages. 
I think we can start with a system that when given some french speech as input, outputs a guess that the speech is from either a speaker with a European accent or an African accent.

** Goals for Thursday:
- TODO Investigate deeper the LID recipe in kaldi to see if we can use if for AID.
- TODO Process the transcripts made by SRI for the CA16 corpus.
- TODO Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

* DAR <2017-01-31 Tue>
** Goals for Tuesday set Monday:
- TODO visit NIST
We had a Very good visit.

- LRE
Omid mentioned the LRE recipe as a model for accent id.
I am looking at the kaldi recipes for LRE.
It looks like all the steps he outlined are in the kaldi LRE recipes.
Specifically, there is a way to use a dnn instead of a gmm to build a ubm.
The ubm is then used to train an i-vector extractor.
The script to train the dnn i-vector extractor requires an sgmm.
The recipe I'm using requires a ubm to build an sgmm.
The ubm here is used to initialize the sgmm.
The first ubm is trained using a gmm.

As I'm getting ready to leave, I'm training SGMMs on gp.
A ubm was previously trained to initialize the sgmm.
I am considering  building a dnn ubm to train the i-vector extractor by following the scripts in the lre recipes.
This will probably require more data than the gp corpus to beat the gmm-based ubm.
I'd eventually like to follow the LRE scripts to build an accent id similarly to the way they built a lid.

** Goals for Wednesday:
- TODO Build SGMMs for gp.  
- TODO Build dnn i-vector extractor.

* DAR <2017-01-30 Mon>
**  Goals for Monday set Friday:
- TODO Finish the gp with simple lm build and consolidate all the steps in 1 run.sh script in the repo.
- TODO clean uup the gp repo.
- TODO Make a yaounde build and put all the scripts in the yaounde repo (wait for Steve's test set fold).
- TODO Work on the other builds and scripts in their own repos.
- TODo Consider a repo for gp + yaounde chain model scripts.

** Goals for Tuesday:
- TODO visit NIST

* DAR <2017-01-27 Fri>
** Goals for Friday set Thursday:
- TODO Clean up scripts in yaounde, gp+yaounde, etc. in their own repos.
- TODO Write results for gp with simple lm.

- Experiment:
My goal was to build a minimal gp chain model system for use as a baseline.
The previous system  I was using started the chain model build from the tri3b system.
How is the tri3b system built?
1. Train mono
2. Get alignments from mono
3. Train tri1 with mono alignments.
4. Train tri2b with mllt, lda and tri1 alignments.
5. Get SI alignments with tri2b (I think the SI stands for speaker independent).
6. Train tri3b with sat and tri2b alignments.
7. Get fmllr alignments with tri3b (this get used later).
8. Generate phone-level alignment lattices containing alternative pronunciations using fmllr alignment with tri3b models.
9. Build a tree using tri3b alignments made in step 7.
10.  Train a ubm for online i-vector extraction. This step uses the tri3b models to transform features (this is still a mystery).
11. Train tdnn. this step uses both the alignment lattices generated in step 8 and the tree built in step 9. Note that the tree built in step 9 uses the allignments from step 8. So, this tdnn training step uses both alignments generated using the tri3b models.
 
In attempting to build a minimal gp chain model system, I skipped the tri3b model and used the tri2b model instead. 
This means I skipped sat training.
The results were slightly worse for the system that only used tri2b.

I'm going to make the minimal gp chain model include the tri3b step.
I did this and the results wer as expected: wer: 49.59.

** Goals for Monday:
- TODO Finish the gp with simple lm build and consolidate all the steps in 1 run.sh script in the repo.
- TODO clean uup the gp repo.
- TODO Make a yaounde build and put all the scripts in the yaounde repo (wait for Steve's test set fold).
- TODO Work on the other builds and scripts in their own repos.
- TODo Consider a repo for gp + yaounde chain model scripts.

* DAR <2017-01-26 Thu>
**  Goals for Thursday set Wednesday:
- TODO Build gp system
I'm on the denominator lattice generation step for the sgmm system.
This takes forever.
- DONE Build gp chain model system
I found a problem with the way I was trying to build the gp-chain system.
I wanted to build the ubm directly from the tri1 models.
When I ran the script to train the diab ubm, I got and error saying that the splice options were not available.
I found that these splice options are associated with the triphone models that are trained with mllt and lda.
So, I am taking a step back to train triphones with mllt and lda.
As I'm getting ready to leave, the gp chain system has trained and even tested.
The scoring scripts were not accessible, so I'm running the decoding step over again.
  
- TODO Incorporate open subs into fr-lm training scripts.


- I spent more time cleaning up my repos.
 I put the eesen scripts in their own repos.
I worked on getting the yaounde scripts cleaned up in their own repo.

** Goals for Friday:
- TODO Clean up scripts in yaounde, gp+yaounde, etc. in their own repos.
- TODO Write results for gp with simple lm.
 
* DAR <2017-01-25 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Meet with Jacquin.
Jacq gave us models and results.
- TODO Train GP chain model system with only gp trainin prompts in lm. 
I created a new git repo called gp-chain for the chain models on the gp corpus scripts.
I have scripts to do the following:
1. build up to tri1.
2. Make alignment lattices.
3. Make the template model with a special topology.
4. Build a tree.
5. Train a diagonal UBM.
6. Train an i-vector extractor.
7. Do something with speakers. (not sure, but it looks like only 2 utterences from each speaker are put aside).
8. Extract i-vectors (online).
9. Train a tdnn.
10. Make a decoding fst graph.
11. Extract test i-vectors (online) for dev and eval data.
12. Decode dev and eval.

- TODO Incorporate open subs into fr-lm training scripts.
Did not get to this.

** Goals for Thursday:
- TODO Build gp system
- TODO Build gp chain model system
- TODO Incorporate open subs into fr-lm training scripts.

* DAR <2017-01-24 Tue>
** Goals for Tuesday set Monday:
- DONE Get more data to build LM and write scripts to process this data.
I processed the data from gp,train, dev and test,  yaounde, questions and prompts, sri gabon conversational tdf and bic

- gp training
I went ahead and started training a gp system only using the gp training prompts in the lm training.
Again, the mono WER scores are bad: 56 and 58.

** Goals for Wednesday:
- TODO Meet with Jacquin.
- TODO Train GP chain model system with only gp trainin prompts in lm. 
- TODO Incorporate open subs into fr-lm training scripts.
 
* DAR <2017-01-23 Mon>
** Goals for Monday set Thursday:
- TODO Build our own lm.
I created a new git repo for fr-lm.
I'm going to put the scripts and files for building French LMs here.
I have scripts for gettting the gp prompts.
I have the Yaounde prompts and questions.
I am in the middle of collecting the transcripts from SRI.
There are 4 files for the Canada and Gabon transcripts.
2 for conversational speech and 2 for read speech.
There are also transcripts under the tdf directory.
The files under the tdf directory contain more information than just the transcripts.
I'm not sure if the above 4 files overlap the tdf files.
The conversation transcripts contain words in parens, double parens, some asterisks and some comments in braces.
I am in the process of removing these by hand in the 4 above files.
Should I have done this in scripts?

I sent a message to the kaldi help mailing list asking Bogdan about what lm and lexicon he used to get his results.
 
- TODO Continue building system with lm trained on gp training prompts.

** Goals for Tuesday:
- TODO Get more data to build LM and write scripts to process this data.

* DAR <2017-01-19 Thu>
**  Goals for Thursday set Wednesday:
- Todo Reproduce the    gp results for models beyond monophones from the kaldi repo.
The best on tri1 dev:
%WER 36.70 [ 8182 / 22297, 2382 ins, 478 del, 5322 sub ]


- Taking a step back.
Steve found language models for global phone.
Justin put the Bremen 3gram lm for French under:
/mnt/corpora/Globalphone/gp/LMs/FR.3gram.lm.gz
I rebuilt the  lang fst with this lm and restarted the building steps.
- Mono results:
dev:
%WER 58.61 [ 13069 / 22297, 1181 ins, 2271 del, 9617 sub ]
eval:
%WER 56.44 [ 12247 / 21698, 1154 ins, 1917 del, 9176 sub ]

So forget it.
- Take another step back.
I'm going to move forward with the gp training prompts as the data for training the lm.

** Goals for Monday:
- TODO Build our own lm.
- TODO Continue building system with lm trained on gp training prompts.

* DAR <2017-01-18 Wed>
** Goals for Wednesday set Tuesday:
- DONE Meet with Jacquin to work on ASR for the voxtec device.
We got the test set and transcripts to Jacq
- TODO Reproduce the rest of the gp results from the kaldi repo.
There were problems building the LMs.
The srilm version seems to work on the gpu machine, but not on my laptop.
I ran the test using the standard lm and the enhanced sri lm.
the enhanced sri lm version gives slightly better results.
41.16 versus 40.98
- TODO Contact Bogdan Vlasenko (the person who got the kaldi repo results)

** Goals for Thursday:
- Todo Reproduce the    gp results for models beyond monophones from the kaldi repo.
* DAR <2017-01-17 Tue>
**  Goals for Tuesday set Friday:
- DONE Reproduce the results in the kaldi gp repo.

Why are my WER results for mono so bad compared to the results published in the kaldi gp repo?
They report 41.80 my best so far is 49.05.
I'm going back to the lm preparation scripts.

I made 3 changes:
1. I added the dev and test prompts to the lm training corpus. I doo not really want to do this since it feels like cheating.
2. I used the utils/format_lm_sri.sh  script to convert the lm to an fst.
3.  I also used the irstlm pruning commands.  

OK: I achieved some lower WERs.
Eval low: 
%WER 41.14 [ 8927 / 21698, 1206 ins, 1305 del, 6416 sub ] 
Dev low:
43.88
these are lower than the 41.80 and 45.69 that in the gp repo results.
I'll try again without the test and dev prompts in the  lm
Here is the srilm command to build the lm:
ngram-count \
    -order 3 \
    -kndiscount \
    -interpolate \
    -unk \
    -map-unk "<UNK>" \
    -limit-vocab \
    -text $corpus \
    -lm $lmthreegram || exit 1;

Then this command gets run:
utils/format_lm_sri.sh \
    --srilm-opts "-subset -prune-lowprobs -unk -tolower" \
    data/lang \
    language_models/lm_threegram.arpa.gz \
    data/local/dict/lexicon.txt \
    data/lang_sri

Then the following irstlm command is run on the original lm data, not the results of the previous command.
prune-lm \
    --threshold=1e-7 \
    language_models/lm_threegram.arpa.gz \
    /dev/stdout | \
    gzip -c > \
	 data/local/lm/lm_threegram.arpa.gz

Finally, the following irstlm command is run on what looks like the output from both the previous commands:
utils/format_lm.sh \
    data/lang_sri \
    data/local/lm/lm_threegram.arpa.gz \
    data/local/dict/lexicon.txt \
    data/lang_test

In the training I am using the data/lang_test directory.

Do I really need the 2 irstlm commands to get the low WERs?

- The dev and test prompts really make a difference.
The best for eval without the dev and test prompts is 56.07
The best for dev without the dev and test prompts is 58.54

I think they used the dev and test prompts in their lm.
 
** Goals for Wednesday:
- TODO Meet with Jacquin to work on ASR for the voxtec device.
- TODO Reproduce the rest of the gp results from the kaldi repo.
- TODO Contact Bogdan Vlasenko (the person who got the kaldi repo results)

* DAR <2017-01-13 Fri>
** Goals for Friday set Thursday:
- DONE Train the tdnn for the chain models on the gp corpus.
The 73 iterations of training finished after setting mini-batch to 128 instead of 512.

I am now running the mkgraph script to make the decoding fst.
it took several hours for the mkgraph program to finish.
I decoded the eval data set.
The best WER was 42.29
Something must be wrong.
The results from the gp kaldi directory gives WERs in the low 20s.
My best was a 38.78 for the sgmm system.



I'm going to try to get their results for the mono system before moving on.

** Goals for Tuesday:
- TODO Reproduce the results in the kaldi gp repo.

* DAR <2017-01-12 Thu>
**  Goals 
- TODO build a chain model on top of the gp cd gmm hmm.
I'm starting to work on this.
I have so far built the cd gmm hmm on the gp data.
I'm starting the process of building a chain model system.
1. lattice alignments
2. Generate topology 
From the script:
Generate a topology file.  
This allows control of the number of states in the non-silence HMMs, and in the silence HMMs.  
This is a modified version of 'utils/gen_topo.pl' that generates a different type of topology, one that we believe should be useful in the 'chain' model.  
Note: right now it doesn't have any real options, and it treats silence and nonsilence the same.  
The intention is that you write different versions of this script, or add options, if you experiment with it.

3. Build a tree
This script builds a tree for use in the 'chain' systems (although the script itself is pretty generic and doesn't use any 'chain' binaries).  
This is just like the first stages of a standard system, like 'train_sat.sh', except it does 'convert-ali' to convert alignments to a monophone topology just created from the 'lang' directory (in case the topology is different from where you got the system's alignments from), and it stops after the tree-building and model-initialization stage, without re-estimating the Gaussians or training the transitions.

4. Train a diagonal ubm.
5. Train an i-vector extractor.
6. Speaker Info
7. Extract I-vectors
8. train tdnn  on gpu.
This was failing on iteration 10.
I removed all the non-default options and it passed iteration 10.
It got to iteration 19 and crashed.
It looks like the GPU ran out of memory.
I am trying again with mini-batch set to 128 instead of 512.
** Goals for Friday:
- TODO Train the tdnn for the chain models on the gp corpus.


* DAR <2016-12-09 Fri>
**  Goals for Friday set Thursday:
- TODO Restart the gp cd gmm hmm build,
Steps done:
4. dict prep,
5. lang prep,
6. convert lm to fst,
7.  prune lm,
8. convert pruned lm to fst,
9.   extract MFCC features,
10. Train monophones,
exp/mono: nj=8 align prob=-101.34 over 22.74h [retry=0.5%, fail=0.0%] states=118 gauss=992
Does this mean 22.74 hours of training data?
14. align with mono,
15. train tri1,
-Good News:
The tri1 training is not showing the warnings about phones not being associated with data.
exp/tri1: nj=8 align prob=-96.77 over 22.73h [retry=0.5%, fail=0.0%] states=2998 gauss=50126 tree-impr=5.11
16. align with tri1,
20. train tri2a,
exp/tri2a: nj=5 align prob=-97.60 over 22.73h [retry=0.6%, fail=0.0%] states=1000 gauss=20047 tree-impr=4.88
24. Train tri2b,
tri2b also uses alignments from tri1.
tri2b does lda and mllt training
# LDA+MLLT refers to the way we transform the features after computing
# the MFCCs: we splice across several frames, reduce the dimension (to 40
# by default) using Linear Discriminant Analysis), and then later estimate,
# over multiple iterations, a diagonalizing transform known as MLLT or CTC.
# See http://kaldi-asr.org/doc/transform.html for more explanation.
exp/tri2b: nj=5 align prob=-49.03 over 22.73h [retry=0.5%, fail=0.1%] states=4972 gauss=75122 tree-impr=5.88 lda-sum=26.25 mllt:impr,logdet=1.09,1.82
28. align with tri2b,
29. train tri3b
# This does Speaker Adapted Training (SAT), i.e. train on
# fMLLR-adapted features.  It can be done on top of either LDA+MLLT, or
# delta and delta-delta features.  If there are no transforms supplied
# in the alignment directory, it will estimate transforms itself before
# building the tree (and in any case, it estimates transforms a number
# of times during training).
exp/tri3b: nj=5 align prob=-48.63 over 22.72h [retry=0.5%, fail=0.1%] states=5994 gauss=75100 fmllr-impr=2.85 over 19.88h tree-impr=8.68
33. align with tri3b
34. train ubm4,
# This trains a UBM (i.e. a mixture of Gaussians), by clustering
# the Gaussians from a trained HMM/GMM system and then doing a few
# iterations of UBM training.
# We mostly use this for SGMM systems.
35. train sgmm2b
# SGMM training, with speaker vectors.  This script would normally be called on
# top of fMLLR features obtained from a conventional system, but it also works
# on top of any type of speaker-independent features (based on
# deltas+delta-deltas or LDA+MLLT).  For more info on SGMMs, see the paper "The
# subspace Gaussian mixture model--A structured model for speech recognition".
# (Computer Speech and Language, 2011).
39. align with sgmm2_4a,
40. Make sgmm2 denominator lattices,

- TODO Write a good set of goals for when I return from leave.

As I'm getting ready to leave, the script to make sgmm2 denominator lattices is running.

For my goal of building chain models with the gp corpus, I'm basically done with this part of the project.
The chain model recipe builds on the cd gmm hmm.

** Goals for Next Year
- TODO build a chain model on top of the gp cd gmm hmm.
 
* DAR <2016-12-08 Thu>
**  Goals for Thursday set Wednesday:
- ToDO Restart the gp+yaounde with aligned speakers.
- TODO After finishing the gp cd gmm hmm build, start the chain model build on top of it.
 I am splitting up the gp kaldi recipe into 1 step per command.
Steps done:
1. Get tools (sox, probably not needed),
2. Prompts prep,
3. data prep,
4. dict prep,
5. lang prep,
6. convert lm to fst,
7.  prune lm,
8. convert pruned lm to fst,
9.   extract MFCC features,
10. Train monophones,
11. make decoding graph for mono system,
12. decode dev with mono,
13. decode eval with mono system,
14. align with mono,
15. train tri1,
16. align with tri1,
17. train tri2a,
18. 

- Problem:
I see warnings in step 15 about phones not being associated with data.
I found the source of the problem.
The gp scripts suck.
They do not handle utf8 encoding well.
The script that normalizes the dictionary is to blame.
I fixed the problem.
I wrote new scripts that write the lexicon with good utf8.
I guess this was hard when this recipe was first written.
My solution is to use the Encode module and the decode_utf8 function.
The decode_utf8 function is a one line solution.
  
I have to start the training all over again.

** Goals for Friday:
- TODO Restart the gp cd gmm hmm build,
- TODO Write a good set of goals for when I return from leave.

* DAR <2016-12-07 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Make sure the lang_test directory is correct in the gp cd gmm hmm system.
It looks like there was a bug in an older version of the kaldi format_lm_sh script.
I I was using the newer one on my laptop.
It copies the topo file over to the lang_test directory.

- DONE Restart the gp cd gmm hmm system.
I am running into a strange sorting problem.
The following 2 line are out of order:
ctell1-01-001	ctell1-01
ctell1-01-0010 ctell1-01
Ah! it's the space between the fields. One is a tab and the other is a space.
I am also running the decoding on the answers again. 
For some reason the scoring failed before.
To do the semi supervised training, I use the transcripts from the decoding with the boosted mmi trained sgmms.
The transcripts that were available were done with the old naming.
Actually, here is where the difference really should happen.
The answers and read parts of the yaounde corpus have their speakers are now aligned.
The effect of this should appear in the semi supervised training.

- DONE Run the semi supervised gp+yaounde system with aligned speakers.
I had to rebuild the data/train_semi_supervised directory.
I also need to extract features from the sri_gabon_read data.

** Goals for Thursday:
- ToDO Restart the gp+yaounde with aligned speakers.
- TODO After finishing the gp cd gmm hmm build, start the chain model build on top of it.
 
* DAR <2016-12-06 Tue>
**  Goals for Tuesday set Monday:
- DONE Get the gp+yaounde cd gmm hmm system running again with the new speaker aligned directories.
I worked a lot on writing the yaound answers and sri gabon read data preparation scripts.
As I'm getting ready to leave, I started the script to run the semi supervised part of this system build.
- TODO Get the gp cd gmm hmm system running again.
I'm separating the run.sh script into scripts for each command.
install tools
get prompts
prepare data
prepare dictionary
prepare lang
convert lm to fst
create pruned lm ( this is done with irstlm)
convert pruned lm to fst
extract mfcc features
train cd gmm hmm

I thought I had all of this running today.
As I'm getting ready to leave I found a bug.
The topo file under the lang_test directory is missing.
I'm not sure why.
The script utils/format_lm.sh has a for loop that copies this file over to from the lang directory to the lang_test directory.
Why did the topo file not get copied over?
This file is used to initialize the monophone system.

** Goals for Wednesday:
- TODO Make sure the lang_test directory is correct in the gp cd gmm hmm system.
- TODO Restart the gp cd gmm hmm system.
- TODO Run the semi supervised gp+yaounde system with aligned speakers.
 
* <2016-12-05 Mon>
**  Goals for Monday set Friday:
- TODO Run standard kaldi gp recipe.
The run.sh script fails when invoking the gp_format_lm.sh script.
This script converts the arpa lm into an fst.
The gp_format_lm.sh script runs srilm via a script called utils/format_lm_sri.sh.
That script says that you can convert an arpa lm to an fst with another script called format_lm.sh.
I'm running that script now.
The srilm script apparently restricts the vocabulary to the intersection of ...
the words in the training data for the lm and the words in the pronouncing dictionary.
This actually might be a cool thing, if it worked.
Srilm is failing for some reason.
When I use the script that does not use srilm, it succeeds in formatting the lm as an fst.

- TODO Run kaldi chain model on gp.
- DONE Fix answers data and continue building gp+yaounde cd gmm hmm system.
I wrote scripts specific for yaounde answers data.
The run.sh script is not crashing where it crashed last week.

I got stuck again on the answers data.
This time it was the text file.
It's a nightmare. 
The file was being appended to each time I ran the utt2text writer.
As a work around, I am deleteing any file with prefix yaounde_answers_ in the data/local/tmp directory.

** WAR:
John Morgan continued working on a project to adapt French Speech Recognition models to African accented speech.
He is taking a step back and training a baseline context dependent gaussian mixture model hidden markov model (CD GMM HMM) system only on a reference corpus of European French. 
The models in his previous systems were trained on data that included African Accented speech. 
After completing the baseline system, he will train chain models on top of the cd gmm hmm and perform adaptation experiments with i-vectors. 

** Goals for Tuesday:
- TODO Get the gp+yaounde cd gmm hmm system running again with the new speaker aligned directories.
- TODO Get the gp cd gmm hmm system running again.

* DAR <2016-12-02 Fri>
** Goals for Friday set Thursday:
- DONE Setup dictionary preparation for the standard kaldi gp fr recipe. 
- DONE Setup lang preparation for the standard kaldi gp fr recipe. 

I ran out of space on the GPU machine.
I deleted a lot of directories to free up some space.

- gp+yaound cd gmm hmm rerun with aligned speaker directories.
The script crashes when trying to decode the yaounde answers with boosted mmi sgmm.
I have not figured out why this is happening.
I see that decoding with mono, tri1, tri2, tri3, tri4, tri5, sgmm and sgmm fmllr all succeed.
Why is decoding the yaounde answers failing with boosted mmi trained sgmm?
The data/answers/text file is empty.

** Goals for Monday:
- TODO Run standard kaldi gp recipe.
- TODO Run kaldi chain model on gp.
- TODO Fix answers data and continue building gp+yaounde cd gmm hmm system.
 
* DAR <2016-12-01 Thu>
**  Goals for Thursday set Wednesday:
- DONE Make prompt lists for dev, eval, and train folds of gp data.
- DONE Run norm script on fold prompt lists

I spent the whole day working on the data preparation steps for the standard gp fr recipe.
I think I'm finished with the data preparation step.
I mostly went back to my gp+yaounde scripts, although I had to modify them to fit the dev, eval and train folds for the kaldi gp recipe.
I now have wav.scp, text, utt2spk and spk2utt files.
The kaldi gp recipe assumed these files already existed.
Writing these files is a large part of the work involved in building a kaldi system. 

As I'm getting ready to leave, I'm working on the gp_dict_prep.sh script
** Goals for Friday:
- TODO Setup dictionary preparation for the standard kaldi gp fr recipe. 
- TODO Setup lang preparation for the standard kaldi gp fr recipe. 

* DAR <2016-12-02 Fri>
* DAR <2016-11-30 Wed>
**  Goals for Wednesday:
- TODO Setup and build the standard gp cd gmm hmm system with the kaldi recipe
- Data preparation
I have lists of the dev, eval, and train speaker directories.
Next step:
get prompt lists for each of these folds.
run the normalization script from the kaldi gp egs recipe scripts on each fold list.
hopefully the rest of the recipe will work from that point on.
- TODO build chain models for the gp system

- gp+yaounde cd gmm hmm withnew naming
The recipe script crashed when trying to make the the decoding graph.
The g.fst was missing.
Sure enough, the commands for making the g.fst are missing from the run.sh script.
This involves the lm.
Replacing epsilon and <s> and </s> symbols
Finding  and removing oovs.
Compiling the fst into binary format
Sorting the fst
I think I have the script running now.

** Goals for Thursday:
- TODO Make prompt lists for dev, eval, and train folds of gp data.
- TODO Run norm script on fold prompt lists

* DAR <2016-11-29 Tue>
** Goals for Tuesday set Monday:
- TODO rebuild the gp+yaounde sgmm system with the new yaounde naming.
I saw these stats on the training trace after monophone training finished:
states=115 gauss=988
tri1:
states=821 gauss=10030 tree-impr=4.18
tri2:
states=831 gauss=20038 tree-impr=4.50
tri3:
states=4891 gauss=75119 tree-impr=5.16
tri4:
states=5079 gauss=75101 tree-impr=6.19 lda-sum=28.81 mllt:impr,logdet=1.05,1.77
tri5:
states=5120 gauss=75078 fmllr-impr=2.92 over 28.77h tree-impr=8.88

The system build had failed at the denominator lattice training.
I saw that the graph construction had been skipped because exp/sgmm5_denlats/dengraph/HCLG.fst already existed.
I put in a command to delete exp/sgmm5_denlats/dengraph/HCLG.fst and rerun.
It seems to have gotten passed the problem for now.

- TODO build the an4 chain model toy system.
Steps:
1. Data prep including utt2spk and spk2utt files 
2. Feature extraction MFCCs in this case
3. lang preparation
4. train context dependent gaussian mixture model hidden markov model (cd gmm hmm)
I'm going to use the term cd gmm hmm from now on to refer  to this kind of system.
5. build a tree. This is standard, but I do not know enough about it to explain it. Is it different from the fst decoding graph? 
6. train a universal background model (ubm).
7. train an i-vector extractor.
8. extract i-vectors for training data.
It looks like an i-vector is extracted for each recording file.
9. Train a chain model temporal delay neural network (tdnn)
10. Extract test i-vectors.
11. Decode.
I have succeeded in all these steps on my laptop, but the training step is failing on the GPU machine.
The WER is 20% compared to 6% for the cd gmm hmm system.

Do I move on to setting up chain models for a real corpus like gp and  gp+yaounde  or do I fix the problem on the GPU?

- DONE write a toy cfg example with nltk to demo for Luis

- Jamal's data
I asked Justin to put it under /mnt/corpora
I'll process it after I've built the new models.

As I'm getting ready to leave the gp+yaounde cd gmm hmm system build with the new naming is doing boosted mmi train of sgmm models.

** Goals for Wednesday:
- TODO Setup and build the standard gp cd gmm hmm system with the kaldi recipe
- TODO build chain models for the gp system

* DAR <2016-11-28 Mon>
** Goals for Wednesday set Tuesday:
- TODO Run gp+yaounde semi supervised sgmm system build and test with new file naming.
The recipe script was getting stuck on the lang directory building.
I removed the lang directory and it seems to be running now.

- TODO Take a pass on TR.
- TODO Decode Jamal's  recordings.
- TODO Read adaptation and iVector papers. 

I only came in to work for a short while. 
I had 2 health related appointments.
My upper GI imaging gave me some good news. 
As expected, I have a hernia, but it is very small, only the size of an almond.

- Incorporating Steve's corrected Niger transcripts into gp+yaounde cd gmm hmm system.

- an4 chain models
I want to investigate i-vectors.
The chain model recipes use i-vectors.
I am starting first with a toy system.
I am starting with the an4 recipe.
This is a relatively new recipe in the kaldi toolkit.
The data is free and there is a script to download it.
The recipe is very basic.
I am running the chain model recipe from the rm system  with the an4 data.
It would be nice to have the rm corpus and run the rm chain model recipe on the rm corpus.

** WAR
John Morgan is exploring the use of i-vectors as a method for adapting French speech recognition models  to accents from regions of Africa.
He is building a toy chain model system that uses a hybrid of a Deep Neural Network and a GMM HMM system together with i-vectors. 

** Goals for Tuesday:
- TODO rebuild the gp+yaounde sgmm system with the new yaounde naming.
- TODO build the an4 chain model toy system.
- TODO write a toy cfg example with nltk to demo for Luis
* DAR <2016-11-22 Tue>
**  Goals for Tuesday set Monday:
- TODO Setup semi supervised data directory with new renamed corpus.
I had a weird experience again today with the scripts.
For some reason that I still have not figured out, a tab separated values files is appearing with a regular white space instead of the tab character.
I finally just had the script split either on tab or white space and it is now running, but it took me most of the day to get to this point.

- DONE Run semi supervised speaker training experiment with eesen yaounde + gp phone and speaker split answers data.
I now have the eesen training running with semi supervised training data.
As I'm getting ready to leave the training is on epoch 8.
- TODO Run experiment with larger test set.
- TODO Take a pass on tr.

** Goals for Wednesday:
- TODO Run gp+yaounde semi supervised sgmm system build and test with new file naming.
- TODO Take a pass on TR.
- TODO Decode Jamal's  recordings.
- TODO Read adaptation and iVector papers. 
* DAR <2016-11-21 Mon>
**  Goals for Monday set Friday:
- DONE Fix the directory and file naming of the yaounde data to make the speakers in the read and answers parts of the corpus coincide.
I have not run the training scripts with the new naming, so I am not sure I did this correctly yet.
I gave the read and answers parts  of the yaounde corpus the same names for the same speakers.
There is a possible issue here with kaldi.
Kaldi uses the basename of the file as the utterance name.
The danger here  is that the speaker name is the first part of the basename of each file.
The rest of the utterance name is the recording number.

I don't think this will cause a collision because  I allocated 4 digits for the read recording names and only 3 digits for the answers recordings names.
I padded with leading zeroes  in both cases.
For example the file with name 1.wav was given the recording name 001.wav if it was an answer and 0001.wav if it was a read recording.
I ran into problems when I started the semi supervised training.
Here's one of the problems.
When I make the spk2utt file I have to consolidate all the utterances corresponding to one speaker.
This is not as easy as jut concatenating the spk2utt files for the read training data and the answers data.
The utterances come from the same speakers.
It took me a long time to figure out that this was a problem.
Another problem is the sorting issue.
The utterances do not appear in the correct order unless I explicitly sort them.
Anyway, I spent the afternoon working on this problem and I'm still not finished.
I'm pretty sure there are similar issues with the utt2spkand other files.  
- DONE Follow Steve's comments on the Niger corpus data to remove bad recordings.
I sent Justin a message requesting him to remove files.
- DONE Incorporate Steve's transcription of the Niger corpus into system build recipes.
I have not run scripts yet to validate the new transcripts.
- TODO Setup the Speaker test experiment.
Justin replaced the Yaounde corpus with the new corpus with renamed directories and files under /mnt/corpora/Yaounde.

** Goals for Tuesday:
- TODO Setup semi supervised data directory with new renamed corpus.
- TODO Run semi supervised speaker training experiment with eesen yaounde + gp phone and speaker split answers data.
- TODO Run experiment with larger test set.
- TODO Take a pass on tr.

* DAR <2016-11-18 Fri>
**  Goals for Friday set Thursday:
- DONE Doctors appointment at 9:30
Complete waste of time.
- TODO map yaounde answers directory names to match read directory names.
This is very tedious.
- TODO Setup new experiment.
I need to align the speakers first.
- TODO Take a pass on the tr.

** Goals for Monday:
- TODO Fix the directory and file naming of the yaounde data to make the speakers in the read and answers parts of the corpus coincide.
- TODO Follow Steve's comments on the Niger corpus data to remove bad recordings.
- TODO Incorporate Steve's transcription of the Niger corpus into system build recipes.
- TODO Setup the Speaker test experiment.
* DAR <2016-11-17 Thu>
** Goals for Thursday set Wednesday:
- TODO Setup new experiment for TR
I realized when working on this that I was very sloppy about naming the directories for the yaounde answers.
In order to perform the experiment, I have to align the speakers in the read data and the answers data.
My current naming of the speaker directories does not do this.
ctell1 speakers 1 through 17 should be good.
Things go haywire after that.
I am going back to the original data collection directories and aligning the read answers speaker directories.
I am writing a list of the renaming that needs to be preformed.
Here are the first couple of directory renames.
ctell2_17 -> ctell2_18
ctell1_18 -> ctell2_19
ctell2_01 -> ctell2_20
ctell2_02 -> ctell2_21

I think this pattern will hold for ctell2
- TODO Decode with eesen gp+younde phone models  (assuming it finishes tonight)
When I arrived in the morning epoch 17 was running. 
EPOCH 16 RUNNING ... ENDS [2016-Nov-17 01:50:13]: lrate 3.125e-07, TRAIN ACCURACY 95.4533%, VALID ACCURACY 86.4480%
The training finished:
EPOCH 18 RUNNING ... ENDS [2016-Nov-17 06:34:24]: lrate 7.8125e-08, TRAIN ACCURACY 95.4285%, VALID ACCURACY 86.6688%
finished, too small rel. improvement .0511
Training succeeded. The final model exp/train_phn_l4_c320/final.nnet
Removing features tmpdir /tmp/tmp.vZV7b1RFLz @ A-TEAM19054
cv.ark
train.ark

I had to do some digging to figure out how scoring works in the eesen scripts.
The WER ended up as 44.13 for 3gram and 43.48 for 4gram.
I'm not sure yet if this is good.
This is a system trained on yaounde read and gp.
No semi supervised training.


- TODO Doctor's appointment in the morning.
Today's appointment is actually at 11:30.
Tomorrow's appointment is at 9:30
- TODO Take a pass on the tr.

** Goals for Friday:
- TODO Doctors appointment at 9:30
- TODO map yaounde answers directory names to match read directory names.
- TODO Setup new experiment.
- TODO Take a pass on the tr.

* DAR <2016-11-16 Wed>
** Goals for Wednesday set Tuesday:
- DONE Get the device hypothesis file for only the files in our reference set (currently 532 utterances)
I wrote a script to do this. I called it niger_select_files.pl.
I end up with 531 utterances instead of 532.
I had deleted one line. I restore it and we now have 532 uttereances.

- DONE Run the scoring again for the device output of the 532 reference utterances.
The WER is 61.89.
- TODO Train the eesen gp+yaounde phone system. 
I had started the training with eesen yesterday before leaving and the process is still running.
As I am getting ready to leave It is on epoch 11.
I think it runs for no more than 25 epochs.
So it might still be running tomorrow.

- TODO possibly ask Justin to install eesen system wide.
- TODO Take a pass on the tr.

- Test gp+yaounde tri5-sgmm semi supervised systems on new Niger 532 data set:
I'm running into a problem.
The WER is over 200.
There seems to be a problem with matching the output with the reference transcript during scoring.
I'm trying to run the test on my laptop.
I am copying the minimal components to run the decoder from the GPU machine to  my laptop.
The HCLG.fst file is big.
final.mdl
tree/
graph/words.txt
graph/phones/silence.csl
Forget it ... this bogged down my laptop
I think I solved the problem I described above. I think there were old output files hanging around. 
The test results are slightly worse for the Niger 532 data set than for our previous central accord test set.
The WER for the same system that got 21.75 is:
%WER 22.77 [ 920 / 4041, 110 ins, 158 del, 652 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it4/wer_15_0.0
The semi supervised sgmm models are yielding WERs around 19.
The best WER was:
%WER 18.73 [ 757 / 4041, 85 ins, 133 del, 539 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it1/wer_11_0.0

- Experiment for tr
I am setting up an experiment that I think is missing for our tr.
Split the younde  data into 2 parts A and B. 42 speakers for each part.
1. Method A (our method):
Supervised Training on gp + yaounde read part A  and semi supervised  training on yaounde answers part A.
2. Method b:
Supervised training on gp + younde read part A and semi supervised training on yaounde part b.
So both systems get supervised training on the same corpus, but system A gets semi supervised training on yaounde answers part A while system B get semi supervised training on yaounde answers part B.
The idea is to test the claim that the overlap of speakers gives a boost to the accuracy of the models.  
** Goals for Thursday:
- TODO Setup new experiment for TR
- TODO Decode with eesen gp+younde phone models  (assuming it finishes tonight)
- TODO Doctor's appointment in the morning.
- TODO Take a pass on the tr.

* DAR <2016-11-15 Tue>
**  Goals for Tuesday set Monday:
- TODO Run the gp+yaounde system tests on the new 545 utterances Niger test set.
I started this, but the test set has changed in the mean time.
We now have a niger test consisting of 532 utterances.

- TODO Get the eesen gp+younde phone system setup. Compile FSTs.
I found a bug. My script uses a tab to separate the utterence id from the transcript. An eesen python script that makes numerized label files uses a plain space. I fixed the script and I am using my own version of this script.
I started working on the training.
The training script uses a program called net-initialize.
Apparently, this is a new program in eesen.
I have to recompile the eesen programs.
Maybe I'll ask Justin to put it on the GPU machine under a directory like /home/tools or /usr/local/share.
The GPU was dead.
Justin rebooted the GPU machine for me. 
- TODO Take a pass on the tr.
- DONE package the Niger test set for Jacquin.
I copied the data from /mnt/corpora to /home/data/ateam/

- Scoring the Voxtec device output
I reran the test with  an updated test set (not the latest yet) and the WER was 62.47.

** Goals for Wednesday:
- TODO Get the device hypothesis file for only the files in our reference set (currently 532 utterances)
- TODO Run the scoring again for the device output of the 532 reference utterances.
- TODO Train the eesen gp+yaounde phone system. 
- TODO possibly ask Justin to install eesen system wide.
- TODO Take a pass on the tr.

* DAR <2016-11-14 Mon>
** Goals for Monday set Thursday:
- TODO Prepare niger test for packaging and sending to Jack.
Steve did a pass on the niger recordings.
He got through system 2 speaker 5.
We now have 545 utterances from the Niger corpus with human validation.

- DONE Run our own test on the niger test set.
I ran the test on the current niger data set which consists of 5 speakers.
%WER 14.43 [ 1165 / 8074, 23 ins, 523 del, 619 sub ] exp/sgmm5_semi_supervised_3/decode_test_niger/wer_11_0.0
%WER 18.22 [ 1471 / 8074, 56 ins, 590 del, 825 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it1/wer_11_0.0
%WER 19.22 [ 1552 / 8074, 65 ins, 613 del, 874 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it2/wer_11_0.0
%WER 20.60 [ 1663 / 8074, 52 ins, 688 del, 923 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it3/wer_11_0.5
%WER 21.23 [ 1714 / 8074, 94 ins, 633 del, 987 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it4/wer_11_0.0
%WER 23.09 [ 1864 / 8074, 95 ins, 688 del, 1081 sub ] exp/sgmm5_semi_supervised_2/decode_test_niger/wer_11_0.5
%WER 23.51 [ 1898 / 8074, 120 ins, 662 del, 1116 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it1/wer_11_0.0
%WER 23.94 [ 1933 / 8074, 141 ins, 626 del, 1166 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it2/wer_10_0.0
%WER 24.37 [ 1968 / 8074, 100 ins, 724 del, 1144 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it3/wer_12_0.5
%WER 24.61 [ 1987 / 8074, 133 ins, 698 del, 1156 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it4/wer_13_0.0
%WER 26.73 [ 2158 / 8074, 155 ins, 686 del, 1317 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it1/wer_14_0.0
%WER 26.75 [ 2160 / 8074, 202 ins, 630 del, 1328 sub ] exp/sgmm5_semi_supervised/decode_test_niger/wer_12_0.0
%WER 27.37 [ 2210 / 8074, 170 ins, 701 del, 1339 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it2/wer_14_0.0
%WER 27.45 [ 2216 / 8074, 174 ins, 684 del, 1358 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it3/wer_13_0.0
%WER 27.73 [ 2239 / 8074, 134 ins, 758 del, 1347 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it4/wer_14_0.5
%WER 28.07 [ 2266 / 8074, 188 ins, 715 del, 1363 sub ] exp/tri5_semi_supervised_3/decode_test_niger/wer_15_0.0
%WER 29.29 [ 2365 / 8074, 190 ins, 717 del, 1458 sub ] exp/sgmm5/decode_fmllr_test_niger/wer_13_0.5
%WER 29.32 [ 2367 / 8074, 190 ins, 719 del, 1458 sub ] exp/sgmm5/decode_test_niger/wer_13_0.5
%WER 29.33 [ 2368 / 8074, 196 ins, 719 del, 1453 sub ] exp/tri5_semi_supervised_2/decode_test_niger/wer_18_0.0
%WER 29.60 [ 2390 / 8074, 220 ins, 700 del, 1470 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it1/wer_13_0.0
%WER 30.02 [ 2424 / 8074, 209 ins, 704 del, 1511 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it2/wer_13_0.0
%WER 30.18 [ 2437 / 8074, 198 ins, 731 del, 1508 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it3/wer_14_0.0
%WER 30.29 [ 2446 / 8074, 182 ins, 774 del, 1490 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it4/wer_16_0.0
%WER 34.30 [ 2769 / 8074, 260 ins, 836 del, 1673 sub ] exp/tri5_semi_supervised/decode_test_niger/wer_20_0.0
%WER 37.22 [ 3005 / 8074, 303 ins, 843 del, 1859 sub ] exp/tri5/decode_test_niger/wer_20_0.0
%WER 40.10 [ 3238 / 8074, 233 ins, 910 del, 2095 sub ] exp/tri5_semi_supervised_3/decode_test_niger.si/wer_14_1.0
%WER 42.40 [ 3423 / 8074, 291 ins, 898 del, 2234 sub ] exp/tri5_semi_supervised_2/decode_test_niger.si/wer_14_0.5
%WER 49.00 [ 3956 / 8074, 280 ins, 1243 del, 2433 sub ] exp/tri5_semi_supervised/decode_test_niger.si/wer_14_1.0
%WER 52.07 [ 4204 / 8074, 311 ins, 1328 del, 2565 sub ] exp/tri5/decode_test_niger.si/wer_18_1.0

- TODO Debug eesen gp+yaounde char system.
- TODO Setup eesen gp+yaounde phone system.
- TODO Take a pass on tr.

- Score niger test device hyps
I ran the compute-wer program with the test reference transcripts I got last week from Steve against the device hypotheses.
The WER was 78.10.
This is on the 162 transcripts Steve wrote for speakers 1 through 5 on system 1.
I ran it again this evening on the larger 545 utterance Niger test set and the WER went down to 71.83.

- Eesen gp+younde phone system.
I spent a lot of time debugging my scripts for preparing the dictionary, lexicon, and lm for compiling into FSTs.
I'm not sure I have it working yet.
There was a problem that an empty line was creeping into my dictionary after some processing.
 
** Goals for Tuesday:
- TODO Run the gp+yaounde system tests on the new 545 utterances Niger test set.
- TODO Get the eesen gp+younde phone system setup. Compile FSTs.
- TODO Take a pass on the tr.
- TODO package the Niger test set for Jack.

* DAR <2016-11-10 Thu>
** Goals for Thursday set Wednesday:
- DONE Package the niger and central accord test sets if Steve wants them.
- TODO Take a pass on the TR.
- TODO Debug gp+younde eesen char system 
- DONE Attend the Voxtec meeting.

- niger test set
I spent most of the day working on the niger test set.
This was pretty tedious. 
I had Justin delete the files that Steve commented on. 
As I'm getting ready to leave, I'm only working with 5 speakers from system1.
Steve's comments end in the middle of speaker 6.
My scripts depend on retrieving the wav files from the directories.
I have a directory for each speaker.
My scripts would pick up all the files in directory 6 and 7, so I'm going to wait until I know what files will finally be in those directories before processing with them.

** Goals for Monday:
- TODO Prepare niger test for packaging and sending to Jack.
- TODO Run our own test on the niger test set.
- TODO Debug eesen gp+yaounde char system.
- TODO Setup eesen gp+yaounde phone system.
- TODO Take a pass on tr.

* DAR <2016-11-09 Wed>
** Goals for Wednesday set Tuesday:
- TODO Train eesen gp+yaounde system (work on 1lang.sh and python scripts).
I got the python scripts to run.
They make the label files.
I updated the command lines from the latest eesen in the repo.
Training fails. I'm not sure why.
- TODO another pass on the tr. 
- Speech package for voxtec
I spent the day putting together the wav data and labels for the Yaound Answers and Read dataset and the Central Accord data.
The package is at:
/home/data/ateam/African_speech
on the GPU machine.

I  included neither the central accord test set nor the niger test.
I could do those tomorrow morning if needed.
I probably want to delete the 12 files Steve commented on.
I think we should delete them from the /mnt/corpora disk.

** Goals for Thursday:
- TODO Package the niger and central accord test sets if Steve wants them.
- TODO Take a pass on the TR.
- TODO Debug gp+younde eesen char system 
- TODO Attend the Voxtec meeting.

* DAR <2016-11-08 Tue>
** oals for Tuesday set Monday:
- DONE Vote
- DONE Run yaounde + gp systems tests only on niger corpus.
It turns out I don't have to do this.
I don't have reference transcripts yet, so I can't get real scores.
Below I included the WER scores taken between the output of our system and the device system.
I'm running the tests again anyway with only the niger data to check if the reference labels line up.
- TODO Train the yaound+gp eesen char system with the lm and lexicon I prepared today.
I made some progress on this.
As I'm getting ready to leave, I am trying to get the lang preparation script to run.
I have to get 2 python scripts working in a script I have called 1lang.sh.
The next step is training.
- DONE take another pass on the tr.
Steve wrote more on the lm section.
I made some changes in the intro and discussion sections.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.

- Yaound + gp systems test on central accord + niger test set:
%WER 48.29 [ 5740 / 11887, 415 ins, 1516 del, 3809 sub ] exp/sgmm5_semi_supervised_3/decode_test_central_accord+niger/wer_11_0.0
%WER 48.57 [ 5773 / 11887, 407 ins, 1528 del, 3838 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it1/wer_10_0.0
%WER 48.61 [ 5778 / 11887, 433 ins, 1501 del, 3844 sub ] exp/sgmm5_semi_supervised_2/decode_test_central_accord+niger/wer_10_0.0
%WER 48.80 [ 5801 / 11887, 388 ins, 1587 del, 3826 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it2/wer_11_0.0
%WER 48.80 [ 5801 / 11887, 413 ins, 1535 del, 3853 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it1/wer_10_0.0
%WER 48.99 [ 5824 / 11887, 370 ins, 1652 del, 3802 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it2/wer_14_0.0
%WER 49.02 [ 5827 / 11887, 398 ins, 1602 del, 3827 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it3/wer_11_0.0
%WER 49.03 [ 5828 / 11887, 399 ins, 1592 del, 3837 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it4/wer_11_0.0
%WER 49.07 [ 5833 / 11887, 380 ins, 1620 del, 3833 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it3/wer_10_0.5
%WER 49.10 [ 5836 / 11887, 335 ins, 1722 del, 3779 sub ] exp/tri5_semi_supervised_3/decode_test_central_accord+niger/wer_16_1.0
%WER 49.13 [ 5840 / 11887, 375 ins, 1659 del, 3806 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it4/wer_14_0.0
%WER 49.50 [ 5884 / 11887, 408 ins, 1592 del, 3884 sub ] exp/tri5_semi_supervised_2/decode_test_central_accord+niger/wer_18_0.0
%WER 49.64 [ 5901 / 11887, 354 ins, 1644 del, 3903 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it2/wer_15_0.5
%WER 49.64 [ 5901 / 11887, 359 ins, 1645 del, 3897 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it1/wer_15_0.5
%WER 49.73 [ 5911 / 11887, 389 ins, 1601 del, 3921 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it3/wer_13_0.5
%WER 49.83 [ 5923 / 11887, 363 ins, 1647 del, 3913 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it4/wer_15_0.5
%WER 49.89 [ 5930 / 11887, 363 ins, 1653 del, 3914 sub ] exp/sgmm5_semi_supervised/decode_test_central_accord+niger/wer_15_0.5
%WER 51.79 [ 6156 / 11887, 450 ins, 1651 del, 4055 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it2/wer_12_0.5
%WER 51.84 [ 6162 / 11887, 447 ins, 1655 del, 4060 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it1/wer_12_0.5
%WER 51.87 [ 6166 / 11887, 502 ins, 1598 del, 4066 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it3/wer_13_0.0
%WER 52.03 [ 6185 / 11887, 482 ins, 1635 del, 4068 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it4/wer_14_0.0
%WER 52.19 [ 6204 / 11887, 537 ins, 1580 del, 4087 sub ] exp/sgmm5/decode_fmllr_test_central_accord+niger/wer_14_0.0
%WER 52.23 [ 6209 / 11887, 541 ins, 1542 del, 4126 sub ] exp/sgmm5/decode_test_central_accord+niger/wer_13_0.0
%WER 53.25 [ 6330 / 11887, 454 ins, 1771 del, 4105 sub ] exp/tri5_semi_supervised/decode_test_central_accord+niger/wer_18_0.5
%WER 54.05 [ 6425 / 11887, 365 ins, 1826 del, 4234 sub ] exp/tri5_semi_supervised_3/decode_test_central_accord+niger.si/wer_15_1.0
%WER 54.58 [ 6488 / 11887, 367 ins, 1926 del, 4195 sub ] exp/tri5_semi_supervised_2/decode_test_central_accord+niger.si/wer_19_1.0
%WER 54.95 [ 6532 / 11887, 414 ins, 1906 del, 4212 sub ] exp/tri5/decode_test_central_accord+niger/wer_18_1.0
%WER 62.02 [ 7372 / 11887, 571 ins, 2010 del, 4791 sub ] exp/tri5_semi_supervised/decode_test_central_accord+niger.si/wer_13_0.5
%WER 63.31 [ 7526 / 11887, 570 ins, 2077 del, 4879 sub ] exp/tri5/decode_test_central_accord+niger.si/wer_13_1.0

The WER scores range from 63.31 to 48.29

** Goals for Wednesday:
- TODO Train eesen gp+yaounde system (work on 1lang.sh and python scripts).
- TODO another pass on the tr. 
* DAR <2016-11-07 Mon>
**  Goals for Monday set Friday:
- TODO Take another pass on the tr
- TODO Run gp+yaounde on niger corpus and compare it to the s2s device transcripts.
This is taking longer than I thought.
I spent most of the day writing the data prep script for the central_accord + niger test data set.
One problem was that the prompts files that I write are concatenated, so I have to delete them before I write them.
As I'm getting ready to leave, it looks like I have this script running.
I'm repeating the test  I ran before with the gp+yaounde system on the concatenation of the central accord and niger test data sets.
I realize now that what I really want is to only test on the niger data.
I'll let this run and I'll change it tomorrow.
- TODO Setup eesen on gp + yaounde training set.
I spent time working on training the lm
I had a bug that I just finally fixed, I was missing the final e on the word yaounde in one of my scripts.
I'm only doing the between 6 and 20 tokens restriction for this lm. 
I also worked on the lexicon for eesen char. 
I think this takes all the words in the training corpus-- which includes subs -- and makes a word to character map for each.
It looks like this ran successfully.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.


I got a flu shot today.
I also got some blood extracted.
I guess they'll test it for cholesterol and sugar. 
My blood pressure was 125 over 87 ( a little high).
It was 147 over 90 the first time they tried.
** Goals for Tuesday:
- TODO Vote
- TODO Run yaounde + gp systems tests only on niger corpus.
- TODO Train the yaound+gp eesen char system with the lm and lexicon I prepared today.
- TODO take another pass on the tr.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.
- TODO Another pass on the tr.

* DAR <2016-11-04 Fri>
** Goals for Friday set Thursday:
- DONE  Incorporate Steve's section on the Lexicon into my latex version of the tr.
There might be some problems with accents and apostrophes.
- TODO Take another pass on the tr.
- DONE Setup eesen scripts for gp+yaounde.
I got through the basic data preparation step, including the Niger corpus.
I even extracted the filterbank features.

** Goals for Monday:
- TODO Take another pass on the tr
- TODO Run gp+yaounde on niger corpus and compare it to the s2s device transcripts.
- TODO Setup eesen on gp + yaounde training set.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.

* DAR <2016-11-03 Thu>
**  Goals for Thursday set Wednesday:
- TODO Put the Niger corpus in kaldi format.
I spent the whole afternoon doing this.
I have listened a second time to all the recordings in the Niger West African corpus.
I had skipped some files yesterday.
There are a couple of recordings with a rooster in the background.
I've renamed the files and directories as I had planned yesterday.
I also put the device transcripts for each speaker in their directory.
It seems like there might be some directories where there are more transcripts than recordings.
But I feel pretty confident that there is some kind of transcript for each recording that I kept in the directories.
I separated out more noise files and files with Eddies voice.
There was one recording of only a cough.
There were a couple of recordings where the speaker spoke in English.
I separated out those too.


- TODO Setup eesen for gp+yaounde 
no work on this today.

- TODO Given Steve's identification of the speakers in the sri_gabon corpus that are in the test set, remove these speakers from the training data corpus.
I spent the morning working on this.
I had to do a search of my own.
The names we had given to the speakers in the test set did not align with the sri_gabon speakers names.
There was a case or two where the country was wrong.
There is a speaker that is not in the sri_gabon directory.
Justin moved the data with the new names to  the /mnt/corpora/central_accord.
He also moved the directories  containing the test data from the sri_gabon directory to a directory called held_out.

** Goals for Friday:
- TODO  Incorporate Steve's section on the Lexicon into my latex version of the tr.
- TODO Take another pass on the tr.
- TODO Setup eesen scripts for gp+yaounde.
** Goals for Monday:
- TODO Attend meeting with Voxtec.

* DAR <2016-11-02 Wed>
** Goals for Tuesday set Monday:
- DONE Go to hospital for endoscopy
Need to have hernia surgery
- TODO Data preparation for eesen gp+yaounde.
I got through the basic data preparation for gp and yaounde and the central accord test set. 

- I spent the whole day working with the Niger West African French Corpus.
The plan is to use this as test data.
I've listened at least once to the recordings.
There are transcripts generated by the voxtec device.
This is a special kind of data.
I'm renaming the files and directories to conform to the kaldi requirements.
I've tried to separate the recordings by speaker.
So far I have:
- 7 speakers on device sys1
- 9 speakers on device sys2
- 7 speakers on devide sys3
These are my best guesses right now.
There are several recordings with Eddie's voice that I've place in a separate directory.
Also a couple of files with only noise.

- kaldi format
niger_west_african_fr/sys{1|2|3}/sys{1|2|3}_{[1-9]}_[0123456][0-9][0-9].wav
At least this is my understanding of the kaldi format.
e.g. 
niger_west_african_fr/sys1/sys1_1_001.wav
So this would be the path to the file of the 001 recording for speaker 1 on device sys1.

I just saw Steve's message about the speakers in the test set.
I'll ask Justin to remove these speakers from the training corpous on /mnt/corpora.
I think it would not take much effort to extend the current test set to the entire set of recordings from these speakers.
Maybe Steve has a reason for not doing this.

** Goals for Thursday
- TODO Put the Niger corpus in kaldi format.
- TODO Setup eesen for gp+yaounde 
- TODO Given Steve's identification of the speakers in the sri_gabon corpus that are in the test set, remove these speakers from the training data corpus.

* Team DAR <2016-10-31 Mon>
** Goals for Monday set Friday:
- DONE Finish preparing recipe for gp+yaounde sgmm.
I think this is ready.
- TODO Get Steve up and running with recipe in his environment.
Steve is now a collaborator on my yaounde repo.
- TODO Make a pass on TR.
- TODO nnet2 dnn for gp+yaounde.


- dnn
I've built 2 dnn systems with different methods.
I'm not sure I've reached the best results with these systems.
1. dnn6 Pretrained dbn 
This is my understanding of how to explain the name.
dnn stands for deep neural network
The 6 is there because we build on the tri5 system.
dbn stands for deep belief network.
My understanding is that a deep belief network is another  name for a restricted boltzmann machine.
I think Hinton is to blame for the name.
Pretrain is there because the dbn is used for pretraining.
Here is the comments in the kaldi script: 
train a DNN on top of fMLLR features. 
The training is done in 3 stages,
1) RBM pre-training:
    unsupervised   train stack of RBMs, 
    starting point for frame cross-entropy trainig.
2) frame cross-entropy training:
objective:  classify frames to correct pdfs.
3) sequence-training optimizing sMBR: 
objective:  emphasize state-sequences with better 
frame accuracy w.r.t. reference alignment.

After steps 1 and 2, I decoded the test set and I got a WER of 22.53.
After step 3 I am getting really bad WER scores -- in the 90s.
Something must be wrong.
2. p-norm tri6 nnet2
Best WER: 22.88

3. online nnet2
The best I've done so far is a WER of 53.46 and this is not even real online yet. It apparently is only a simulation of online decoding.

I'm looking into eesen again.
** Goals for Tuesday:
- TODO Go to hospital for endoscopy
- TODO Data preparation for eesen gp+yaounde.

* Team DAR <2016-10-28 Fri>
**  Goals for Friday set Thursday:
- TODO Write tr
I'm ready to had it off to Steve.
- TODO Read more papers on dnn and sgmm accent adaptation.
- TODO Run smbr training on dnn models.
The denominator lattice generation script is still running.
I'm getting ready to leave and it is still running.

I had started the nnet2 training recipe.
I started it from the  tri5_semi_supervised directory so I could compare results with the sgmm system.
The training is on iteration 150 as I'm getting ready to leave.
It went all the way to iteration 159.
This seems like a lot.
And it looks like the training finished.
I'm not sure what is the next step.
** Goals for Monday:
- TODO Finish preparing recipe for gp+yaounde sgmm.
- TODO Get Steve up and running with recipe in his environment.
- TODO Make a pass on TR.
- TODO nnet2 dnn for gp+yaounde.

* Team DAR <2016-10-27 Thu>
** Goals for Thursday set Wednesday:
- TODO Write tr.
- TODO Chec if there is a problem with the gmm online decoding. Why is it so bad?
- TODO Take next step on dnn nnet system. I need to get a unigram lm somehow.
I run the script that makes the unigram lm.
There is a line that checks if the l times g fst is stochastic.
The answer is no.
The script says that this is an error.
I'm not sure this is a problem.
The script was not set to exit with an error status of 1, so I'm not sure if the fst is supposed to be stochastic or not.
Anyway, I'm moving on to make the fst graph.
The fst graph was made.
I am decoding the test set with the unigram lm.
The WER was 68.51.
I think this is run to get many alternative paths in the lattice of hypothesis.
When I ran the decoding yesterday before leaving with the pretrained dnn models the WER was 22.53
now I'm aligning with the pretrained dnn models.
Alignment finished. I see warnings in the log files.
I started denominator  lattice generation.
I think this will take a while.
- TODO Setup nnet2 online training
** Goals for Friday:
- TODO Write tr
- TODO Read more papers on dnn and sgmm accent adaptation.
- TODO Run smbr training on dnn models.
 Team DAR <2016-10-26 Wed>
** Goals for Wednesday set Tuesday:
- TODO Write tr.
- TODO Run hybrid dnn/hmm system with discriminative sequence training.
It is still running.

I read the paper for Karel Vesely's nnet setup.
I also started reading the paper for the nnet2 setup.
It has a section on online system.
- online decoding bad news
 I ran a script that does online decoding for the gmm system.
I started with the tri5_semi_supervised models.
It does some preparation then decodes.
The bad news is that the WER is 100 and 99 percent.
- nnet training
As I'm getting ready to leave the nnet system build is at iteration 14 of training.

** Goals for Thursday
- TODO Write tr.
- TODO Chec if there is a problem with the gmm online decoding. Why is it so bad?
- TODO Take next step on dnn nnet system. I need to get a unigram lm somehow.
- TODO Setup nnet2 online training

* Team DAR <2016-10-25 Tue>
** Goals for Tuesday set Monday:
- DONE Cyber Security Awareness Challenge mandatory training. Try to resume where I left off. I was working on the home security.
Finally got this done thanks to Steve.
- TODO Write TR.
- TODO Start investigating what step to take next: chain models, nnet2, nnet2 online, nnet3 ...
I found some scripts for nnet. 
This is a hybrid dnn/hmm system.
So the dnn replaces the gmm emission probability distribution. 
I've run this before.
Now I'm running it starting on the last tri5 models I produced. 
tri5 semi supervised 3.
Now that I write this, I realize that this is probably not a good idea, since these models have seen the test data.
I really probably should start at tri5 semi supervised.
Yeah ... I messed up.
I am using the semi_supervised_3 models with the original gp training data set.
Before I leave I'll restart the scripts using the correct data and models.

There's a paper by Karel Vesely about this system.
** Goals for Wednesday:
- TODO Write tr.
- TODO Run hybrid dnn/hmm system with discriminative sequence training.

* Team DAR <2016-10-24 Mon>
**  Goals for Monday set last Friday:
- TODO Cyber Challenge Mandatory Training
I've spent the afternoon trying to complete this training.
I am stuck again.

- TODO Read self training papers
- DON Finish work on gp+yaounde  system training and decoding.
The current project is done.
- TODO Show best transcripts to Steve for qualitative  evaluation.
- TODO Write tr

I spent most of the morning cleaning up the gp+yaounde directory.
I put all the scripts that run commands into one run.sh script.

** Goals for Tuesday:
- TODO Cyber Security Awareness Challenge mandatory training. Try to resume where I left off. I was working on the home security.
- TODO Write TR.
- TODO Start investigating what step to take next: chain models, nnet2, nnet2 online, nnet3 ...
* <2016-10-21 Fri>
** Goals for Friday set Thursday:
- DONE Doctor's appointment in the morning.
The doctor scheduled me for an endoscopy on November 1 at 7:30. 
He scheduled a follow up on Friday, November 18 at 9:15.
- TODO <2016-11-01 Tue 07:30> Endoscopy at Laurel 
- TODO <2016-11-18 Fri 09:15> Follow up with Dr. Lawrence
- TODO Cyber Challenge Training <2016-10-24 Mon>
- DONE Last Pass on objectives
- TODO Read self training papers.
- TODO Decode sri_gabon_conv datawith gp+yaounde system and show transcripts to Steve.
- TODO Write tr.
- TODO Get bibtex for tr citations.

** Goals for Monday:
- TODO Cyber Challenge Mandatory Training
- TODO Read self training papers
- TODO Finish work on gp+yaounde  system training and decoding.
- TODO Show best transcripts to Steve for qualitative  evaluation.
- TODO Write tr
* <2016-10-20 Thu>
** Goals for Thursday set Wednesday:
- DONE Take another pass on objectives
- TODO Cyber Challenge mandatory training.
Work on this Tomorrow with Steve
- DONE Writing pass on TR.
I'm getting references in bibtex .
I am finding several very relevant papers that were written a couple of years ago for the babel project.
They deal with semi supervised training a.k.a. self training.
I need to read these papers carefully.
 
- DONE Make decoding graph for gp sgmms.
- TODO Make decoding graphs for gp + yaounde stage 3 system.
I am making  two of them now as I am getting ready to leave.
The tri5 and sgmm ones.
- DONE Decode test data with gp sgmm boosted mmi system to get results to put in report.
Surprisingly, the boosted mmi training and lattice rescoring did not help.
The best WER was 32.84 by the sgmm system.
- TODO Decode sri_gabon_conv data with stage 3 gp + yaounde system.
As I am getting ready to leave, the training script is at the denominator lattice generation step.
Hopefully, this will finish by tomorrow.

I am done with the experiments  that I plan on reporting on with one possible exception.
I want to report speaker independent results.
I can do this for the tri5 systems,but I'm not sure I can do it for the sgmm systems.

After reading some of the semi supervised self training papers, I am wondering if I might change the training regime. 
those papers work with deep neural networks, so I'm eager to move on and consider this work as a baseline.
** Objectives Draft
*** 1. TECHNICAL COMPETENCE
**** ASR Adaptation:
It is not clear that the advances made last year can be implemented in applications that would directly benefit the Army. 
This year I propose to capitalize on last year's successes by investigating ASR models that have well defined pathways to implementation  in speech to speech devices. 
I plan on focusing on developing models that result in software that can be demoed with realtime interaction. 

**** kaldi:

The ASR systems I have built this year are based on HMMs and SGMMs. 
I will consider these systems as baselines for the work I will do using neural network models. 
I will continue developing with the Kaldi ASR toolkit. 
Specifically, I will implement systems with the following models:
Bottle Neck Features
Chain Models
nnet2
nnet3
TDNNs
RBMs
Eesen end to end rnn and lstm models.

i. European French to African accents
ii. Standard Arabic to Tunisian accent.
b. Language Modeling:
i. Dialogue modeling:
A. French
B. Arabic 
ii. Lexicon expansion
A. French
B. Arabic.
C. Dari
2. Machine Translation
a. Variable Computation Graphs

**** Research:
***** Variable Structured computational graphs.
Many models used in NLP applications have a network of connected nodes. 
Training these networks has been restricted to computing weights associated with the connections. 
The topology of the networks has largely remained fixed. 
Lately there have been attempts to develop training methods that change the network topology with each training example. 
I propose to learn to use a toolkit called DyNet (or one like it) that is designed to build systems with variable graph structures. 

I plan on using DyNet or a toolkit similar to it to build a Machine Translation System and to compare its performance with systems built with other reference toolkits like Joshua, Moses, Tensorflow, etc.  
*** 2. COOPERATION

Collaborate with colleagues to write papers that report on advances made in our projects. 
Collaborate with the Basic Research team by contributing speech recognition components to efforts such as the bot language project. 
*** 3. COMMUNICATIONS

Write weekly activity reports to team members to keep them up to date on my work. 
Read and comment on reports made by my team and branch mates.

*** 4. MGMT. OF TIME & RESOURCES

Set aside time during the day to practice some kind of  activity for physical fitness. 
Stay abreast of possible areas where hardware upgrades could improve work efficiency. 
*** 5. CUSTOMER RELATIONS

Establish relationships with MFLTS and CERDEC to remain aware of Army requirements.
Establish contacts with researchers in the ASR and NLP fields. 
Establish contacts with s2s device manufacturers.

*** 6. TECH TRANSITION

Contribute recipes for building ASR systems with our corpora to the MFLTS. 
Transition ASR components and our other products to USA Army Africa and MFLTS.  
*** 7. DIVERSITY: 
Support ARL's diversity initiatives by participating in locally-sponsored diversity training, broad outreach, and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees. 
*** 8. SHARP: 
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all. 
Demonstrate support for the SHARP program by actively participating in required training and other educational programs. 
Intervene and appropriately respond to any instances of sexual harassment or sexual assault and encourage others to do the same.

Third, Pls be sure to include the fixed values for "Wgt Assigned" for DB-3s to total 100: 
40 - 15 - 10 - 15 - 10 -10 
Also, check the box with an X for Tech Competence.

** Goals for Friday:
- TODO Doctor's appointment in the morning.
- TODO Cyber Challenge Training
- TODO Last Pass on objectives
- TODO Read self training papers.
- TODO Decode sri_gabon_conv datawith gp+yaounde system and show transcripts to Steve.
- TODO Write tr.
- TODO Get bibtex for tr citations.
* <2016-10-19 Wed>
** Goals for Wednesday set Tuesday:
- TODO Write objectives
- DONE Decode sri_gabon_conv data with sgmm boosted mmi models
- DONE Start stage 3 of gp+yaounde system
This took all morning.
I found some bugs in the scripts for the previous stage that caused problems in the current stage.
I had a bug in the way I named the directories and files.
When I was working only with the read files, theer was no problem with sorting.
When I added the conv files, the naming caused a problem with sorting. 
I finally got the acoustic model training to start after fixing the data prep scripts.
- DONE Start stage 2 of gp system to get semi supervised results
This was relatively easy.
- TODO Write TR
Made some progress. I wrote a first pass on the abstract.
I filled out some of the arlticle form. It is starting to look like an ARL report.

As I'm getting ready to leave, there are 3 jobs running on the gpu machine:
- stage 3 of semi supervised training on the gp + yaounde system.
I am using the transcripts of the sri_gabon_conv obtained from models trained in stage 2 to train new models that will be used to get another (hopefully better) transcription of the sri_gabon_conv data.
Once this step is finished, I will stop working on this project and move to neural network methods.
- semi supervised training of the gp system.
I am using the answers transcripts obtained in the first stage of training the gp system.
I am only doing this for completeness. 
We need the results to show that ...
Well ...
that collecting the read   part of the corpus makes a difference in WER scores.
- Tri5 Decoding graph for gp system.
The gp system training is passed the tri5 stage, so I can start generating the decoding graph for the tri5 models.
I'll need this graph for decoding with the tri5 models and the sgmm models. 
** Goals for Thursday:
- TODO Take another pass on objectives
- TODO Cyber Challenge mandatory training.
- TODO Writing pass on TR.
- TODO Make decoding graph for gp sgmms.
- TODO Make decoding graphs for gp + yaounde stage 3 system.
- TODO Decode test data with gp sgmm boosted mmi system to get results to put in report.
- TODO Decode sri_gabon_conv data with stage 3 gp + yaounde system.
* <2016-10-18 Tue>
** Goals for Tuesday set Monday:
- TODO Write Objectives
I've been procrastinating on this.
- DONE Finish training gp only system.
The first stage is done training.
I have decoded the test data with the tri5 models:
WER: 48.55 for speaker dependent models
WER: 62.04 for speaker independent models
I have made the decoding graph for the sgmm models and I am currently decoding with them.

- TODO Write tr.
I spent a lot of time on this today.
In summary, I installed the arlticle document class and typeset the current draft of the tr with it.
I'll include it below
- TODO Run stage 3 of the gp+yaounde system which uses the sri_gabon_read transcripts from stage 2.
I'm working on this.
The training goes fast, but making the decoding graphs and actually decoding all the data takes a while.
As I am getting ready to leave, I am decoding the sri_gabon_conv data with the sgmm models.
After this I'll have to decode with the boosted mmi trained sgmm models.

** TR draft
ARL-IR-0000 •JAN 2015
US Army Research Laboratory
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
by John J Morgan, and Stephen A LaRocca
Approved for public release; distribution is unlimited.
NOTICES
Disclaimers
The findings in this report are not to be construed as an official Department of the
Army position unless so designated by other authorized documents.
Citation of manufacturer’s or trade names does not constitute an official endorse-
ment or approval of the use thereof.
Destroy this report when it is no longer needed. Do not return it to the originator.
ARL-IR-0000 •JAN 2015
US Army Research Laboratory
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
by John J Morgan
Computational and Information Sciences Directorate, ARL
Stephen A LaRocca
Computational and Information Sciences Directorate, ARL
Approved for public release; distribution is unlimited.
REPORT DOCUMENTATION PAGE 
Form Approved 
OMB No. 0704‐0188 
Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the 
data needed, and completing and reviewing the collection information.  Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing 
the burden, to Department of Defense, Washington Headquarters Services, Directorate for Information Operations and Reports (0704‐0188), 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA 22202‐
4302.  Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to any penalty for failing to comply with a collection of information if it does not display a 
currently valid OMB control number. 
PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS. 
1. REPORT DATE (DD‐MM‐YYYY) 
 
2. REPORT TYPE 
 
3. DATES COVERED (From ‐ To)
4. TITLE AND SUBTITLE 
 
5a. CONTRACT NUMBER 
5b. GRANT NUMBER 
5c. PROGRAM ELEMENT NUMBER
6. AUTHOR(S) 
 
5d. PROJECT NUMBER 
5e. TASK NUMBER 
5f. WORK UNIT NUMBER 
	
7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)
 
8. PERFORMING ORGANIZATION REPORT 
NUMBER 
9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES)
 
10. SPONSOR/MONITOR’S ACRONYM(S)
11. SPONSOR/MONITOR'S REPORT NUMBER(S)
12. DISTRIBUTION/AVAILABILITY STATEMENT 
13. SUPPLEMENTARY NOTES 
14. ABSTRACT 
15. SUBJECT TERMS 
16. SECURITY CLASSIFICATION OF:   
17. LIMITATION
       OF  
       ABSTRACT 
18. NUMBER
       OF  
        PAGES 
	
19a. NAME OF RESPONSIBLE PERSON
a. REPORT 
 
b. ABSTRACT 
 
c. THIS PAGE 
 
19b. TELEPHONE NUMBER (Include area code)
  Standard Form 298 (Rev. 8/98) 
  Prescribed by ANSI Std. Z39.18
January 2015 Internal Report
Bootstrapping A Question Answering Speech Recognizer With Read Speech
John J Morgan, and Stephen A LaRocca
ARL-IR-0000
Approved for public release; distribution is unlimited.
October 2014-November 2014
AH80
US Army Research Laboratory
ATTN: RDRL-CII-T
Adelphi Laboratory Center, MD 20783-1138
primary author’s email: <john.j.morgan50.civ@mail.mil>.
This report is about Automatic Speech Recognition.
document style, arlticle, revision, sans serif, L
A
TEX
16
John J Morgan
301-394-1902
Unclassified Unclassified Unclassified UU
ii
Approved for public release; distribution is unlimited.
Contents
List of Tables iv
Acknowledgments v
1. Abstract 1
2. Introduction 1
3. Methods 3
3.1 Data 3
3.2 Acoustic Model Training 3
3.3 Language Model Training 4
4. Results 4
5. Discussion 5
6. References 6
Distribution List 7
iii
Approved for public release; distribution is unlimited.
List of Tables
Table 1 WER scores for models and training sets. .....................................5
iv
Approved for public release; distribution is unlimited.
Acknowledgments
John Morgan wishes to sincerely thank his co-author, Dr. Stephen LaRocca.
v
Approved for public release; distribution is unlimited.
INTENTIONALLY LEFT BLANK.
vi
Approved for public release; distribution is unlimited.
1. Abstract
A recommended method for data collection that enables automatic rough draft tran-
scription after semi supervised adaptation of acoustic models.
2. Introduction
Speech to speech (S2S) devices enable dialogues between people who speak dif-
ferent languages. S2S devices for communicating between languages L1 and L2
consist of three major components: two Automatic Speech Recognizers, ASR1 and
ASR2 for languages L1 and L2 respectively, also known as Speech to text; one Ma-
chine Translation (MT) system; and two Speech Synthecizers, T2S1 and T2S2 also
known as text to speech. Speaker 1 speaks sentence s1 in language l1. ASR system
asr1 converts s1 into text t1 in language l1. t1 is translated into t2 in language l2 by
machine translation system mt. Text t2 is converted into the spoken sentence s2 in
language l2 by T2S2.
The U.S. Army is interested in using high quality S2S technology to help com-
municate with soldiers in allied military units during training missions. Frequently,
these soldiers speak an accented version of a world language like French or Arabic.
S2S devices are trained on the speech data that is most widely available, which is
most often the standard version of the language. Accented speech can be different
enough from the standard speech to make the ASR component of an S2S device
fail. Adaptation techniques have been used to remedy this problem.
Large amounts of recorded speech is used to train the acoustic models for ASR
systems. ASR systemss for S2S devices are ideally trained on speech that is similar
to the task for which the device will be used. Collecting this ideal kind of dialogue
data is expensive. In order for the data to be used as training data for an ASR system
it must be transcribed at the word-level. This transcription task is a major part of
the reason why the data collection is expensive. A way to cut back on this cost
is to obtain an automatically generated rough draft of the dialogue type of speech
collected then to have a human correct the rough draft.
If the data being collected comes from a language that lacks a corpus of speech data
or if the data comes from a highly accented flavor of a well-resourced language,
automatic transcriptions of the data that are useful for humans to correct will not be
possible. One way to solve this problem is to collect a small corpus of recitations
1
Approved for public release; distribution is unlimited.
by each speaker as part of the data collection. We will refer to this as the read part
of the corpus. The other part will be refered to as the conversational part. Note that
each informant contributes both a read and conversational part to the corpus. The
small read corpus will not be sufficient to serve as a training set for an ASR system
to be used in an S2S device. However, it can serve as a corpus to train an ASR
system that can be used to obtain rough draft transcriptions of the conversational
speech part. one reason this is possible is because as noted above the speakers in
the read part are the same speakers that are in the conversational part.
For scientific evaluation, for any ASR task, the speakers in the test set and training
set are kept disjoint. The ASR taskk becomes much easier when the speakers in the
training and test sets are the same.
The cost of building an ASR system with read speech is much lower than building
one with conversational speech. A pronouncing dictionary is the most expensive
component of a phone-based ASR system. For a system built with read speech
there is no cost involved with transcribing the data. The transcriptions are given by
the prompts. The dictionary can be used to obtain a phone-level transcription from
a word-level transcription.
Previous work has shown that ASR for accented speech can benefit from the use
of subspace gaussian mixture models (SGMM)s instead of triphone models.1
One
of our contributions in this paper is a improvement to the above work. We describe
a two step semi supervised process for building an ASR system that can be used
effectively to get a rough transcription of the conversational part of a corpus. The
first step uses the read part of the corpus to train acoustic models which are used
to get a rough transcription of the conversational part of the corpus. The second
step trains new models by adding to the training data the conversational part of the
corpus with its automatically transcribed labels. We will refer to this as quasi semi
supervised training (not quite semi supervised), since the speakers in the unlabeled
training set are the same as those in the supervised read part of the training corpus.
There are two benefits to this kind of data collection. First, we show that the quasi
semi supervised training results in lower WER scors. Second, the automatic label-
ing results in a transcription of the conversational part of the corpus that can lower
costs for human in the loop labeling. The results of this two stage approach to sys-
tem building also supports our recommendation that both read and conversational
2
Approved for public release; distribution is unlimited.
speech be collected in data collection of accented speech.
3. Methods
3.1 Data
Three speech corpora were used in this project.
The Yaounde corpus: collected in Yaounde ,the capital city of Cameroon. It has
two parts: the read part which consists of recitations fof prompts and the
conversational part which consists of answers to questions.
The French part of the Globalphone corpus: This corpus consists of 100 native
French speakers. They recorded a total of 10478 utterences.
The Central Accord Corpus: Collected in Gabon from speakers from four Cen-
tral African countries. A small part of the read part of this corpus was used as
test data.
3.2 Acoustic Model Training
All the experiments performed in this project used the kaldi toolkit. The standard
kaldi recipe framework was used.
As recommended by the Babel project, we trained models on plp and pitch fea-
tures. The following model building sequence was followed: We tried to follow the
naming conventions used in the recipes for the babel project.
Monophones (mono) Flat start and 40 iterations of monophone training, with delta-
delta features. Per speaker cepstral mean normalization was applied.
Triphones (tri1)
Triphones (tri2)
Triphones (tri3)
Triphones (tri4) Trained with lda and mllt transforms.
Triphones (tri5)
3
Approved for public release; distribution is unlimited.
Supspace Gaussian Mixture Models (sgmm)
SGMMs with boosted mmi (sgmmb)
Two configurations of the training folds of the data were compared.
GP Consisting of the Globalphone prompts.
GP + Yaounde Consisting of both the Yaounde and Globalphone prompts.
The set of unlabeled data consists of answers to questions in the Yaounde corpus.
The answers were given by the same speakers who made the recitations in the read
part of the Yaounde corpus.
After training Boosted MMI SGMM models on the supervised training sets, QUASI
SEMI SUPERVISED transcriptions were obtained for the Answers by decoding
with the resulting ASR system. decoding was done with lattice rescoring, where
lattices were generated from a previous SGMM system. Speaker vectors and MLLR
transforms were also used. The Yaounde Answers data together with their quasi
semi supervised labels were Then appended to the training set and the same training
regime was run again.
3.3 Language Model Training
A three gram statistical language model was trained with srilm on the following text
data sets:
• Subtitles
• GP transcripts
•
4. Results
sgmm boosted mmi
4
Approved for public release; distribution is unlimited.
Table 1 WER scores for models and training sets.
models trainingspeaker supervisionspeaker GP GP + Yaounde
tri5 dependent full 48.55 34.78
tri5 independent full 62.04 44.02
tri5 dependent semi 29.87
tri5 independent semi 46.94
sgmm dependent semi 21.25
sgmm dependent full 38.28 25.85
sgmm independent full
sgmm independent semi
5. Discussion
The Semi supervised method yields gains when speaker dependent models are
trained. In this case the WER goes down from 25.85 to 21.25. However, when
speaker independent model training methods were used, we saw the WER go up. In
the triphone case, the scores went up from 44.02 to 46.94.
Unfortunately, this observation implies that our method will not be useful for our
target S2S device application where speaker dependent models are not practical.
In future work, we plan on exploring neural network models and deep learning
techniques to extend our ideas to the online decoding scenario.
5
Approved for public release; distribution is unlimited.
6. References
1. Motlicek P, Garner PN, Kim N, Cho J. Accent adaptation using subspace
gaussian mixture models. In: The 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP); 2013 May; Vancouver, BC, Canada.
(38; no. Idiap-RR-38-2013) Rue Marconi 19, Martigny, Switzerland: p. 7170–
7174.
6
Approved for public release; distribution is unlimited.
1
(PDF)
DEFENSE TECHNICAL
INFORMATION CTR
DTIC OCA
2
(PDF)
DIRECTOR
US ARMY RESEARCH LAB
RDRL CIO L
IMAL HRA MAIL & RECORDS MGMT
1
(PDF)
GOVT PRINTG OFC
A MALHOTRA
7
Approved for public release; distribution is unlimited.
INTENTIONALLY LEFT BLANK.
8

** Goals for Wednesday:
- TODO Write objectives
- TODO Decode sri_gabon_conv data with sgmm boosted mmi models
- TODO Start stage 3 of gp+yaounde system
- TODO Start stage 2 of gp system to get semi supervised results
- TODO Write TR
* <2016-10-06 Thu>
** Goals for Thursday set Wednesday:
- DONE Write a first pass on objectives
- DONE Wrap up yaounde answers semi supervised training 
- DONE Start on sri_gabon_read semi supervised training.

The sgmm5 denominator lattices generation had finished this morning.
I started the boosted mutual maximum information (mmi) sgmm training.
There might be something wrong.
I'm getting the following warning:
Frame-counts disagree 10969869 versus 9789113
This might have something to do with the problem I had yesterday when I reran the data prep and feature extraction scripts.

The decoding of the test set yesterday with sgmm5 models gave a best wer:
WER: 22.03
The similar decoding of the answers also succeeded:
WER: 18.29
This is testing on a lot of the training set.
The similar decoding  on the sri_gabon_reaed data failed.
This is because I had not decoded the sri_gabon_read  data with the tri5 models to get  transforms.
Recall that these are speaker dependent models.
I'm running the decoding of the sri_gabon_read data now with tri5 models.

The boosted sgmm5 mmi training finished.
There is the warning about different numbers of frames still.
I am running the rescore decoding with the boosted mmi trained sgmm models.
On the test set:
Best WER: 21.25

The decoding of the sri_gabon_read set with the tri5  yaounde answers semi supervised models finished.
So now I have automatically generated transcripts of the sri_gabon_read data (modulo the problem yesterday).
Now I need to decode with the sgmm5 semi supervised models.
All done for this stage.

I'm going to use these transcripts as supervision in the next stage of training.

The next stage will use both the answer and sri_gabon_read automatically generated transcripts as training labels.
Can I delete the data/sri_gabon_read directory and regenerate it?

I ran the feature extractor for the train_semi_supervised_2 data set.

I fired up a script that is supposed to run the steps  for the second stage of mono to sgmm semi supervised training.

**  all the scores in my experiments
Below are all the wer scores from the experiments I've run in the past few weeks.
I'm surprised I got this far without deleteing my working directory.
I sorted them in reverse numerical order.

 
%WER 99.51 [ 14247 / 14317, 2614 ins, 3802 del, 7831 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it3/wer_20_1.0
%WER 99.33 [ 14221 / 14317, 2579 ins, 3812 del, 7830 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it4/wer_20_1.0
%WER 99.25 [ 14210 / 14317, 1461 ins, 5427 del, 7322 sub ] exp/tri1_semi_supervision_2/decode_answers/wer_20_1.0
%WER 99.23 [ 14207 / 14317, 2579 ins, 3794 del, 7834 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it1/wer_19_1.0
%WER 99.15 [ 14195 / 14317, 2568 ins, 3812 del, 7815 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it2/wer_20_1.0
%WER 99.09 [ 14187 / 14317, 2541 ins, 3819 del, 7827 sub ] exp/sgmm5/decode_answers/wer_20_1.0
%WER 99.02 [ 14176 / 14317, 2539 ins, 3793 del, 7844 sub ] exp/sgmm5/decode_fmllr_answers/wer_20_1.0
%WER 98.84 [ 14088 / 14253, 1186 ins, 6097 del, 6805 sub ] exp/mono_semi_supervision/decode_answers/wer_17_1.0
%WER 98.74 [ 14073 / 14253, 1318 ins, 5990 del, 6765 sub ] exp/tri1_semi_supervision/decode_answers/wer_19_1.0
%WER 96.67 [ 13840 / 14317, 1504 ins, 5443 del, 6893 sub ] exp/tri2_semi_supervision_2/decode_answers/wer_17_1.0
%WER 96.05 [ 13751 / 14317, 1134 ins, 6449 del, 6168 sub ] exp/tri2_semi_supervision/decode_answers/wer_16_1.0
%WER 93.56 [ 13395 / 14317, 1157 ins, 6075 del, 6163 sub ] exp/sgmm5_semi_supervision/decode_answers_no_mllr/wer_20_1.0
%WER 93.01 [ 13316 / 14317, 2282 ins, 4169 del, 6865 sub ] exp/tri5_semi_supervision/decode_answers/wer_19_1.0
%WER 92.99 [ 13314 / 14317, 2173 ins, 4329 del, 6812 sub ] exp/tri5_semi_supervision_2/decode_answers.si/wer_17_1.0
%WER 92.39 [ 13227 / 14317, 2349 ins, 3957 del, 6921 sub ] exp/tri5_semi_supervision_2/decode_answers/wer_15_1.0
%WER 92.38 [ 13226 / 14317, 1802 ins, 5256 del, 6168 sub ] exp/tri5_semi_supervision/decode_answers.si/wer_19_1.0
%WER 89.05 [ 12749 / 14317, 1714 ins, 4596 del, 6439 sub ] exp/sgmm5_semi_supervision/decode_answers/wer_17_1.0
%WER 88.96 [ 12737 / 14317, 1615 ins, 5171 del, 5951 sub ] exp/tri3_semi_supervision_2/decode_answers/wer_17_1.0
%WER 88.75 [ 12707 / 14317, 1851 ins, 4299 del, 6557 sub ] exp/sgmm5_semi_supervision_2/decode_answers/wer_14_1.0
%WER 88.64 [ 12691 / 14317, 1156 ins, 6298 del, 5237 sub ] exp/tri3_semi_supervision/decode_answers/wer_16_1.0
%WER 88.52 [ 12674 / 14317, 1747 ins, 4781 del, 6146 sub ] exp/tri4_semi_supervision_2/decode_answers/wer_19_1.0
%WER 86.16 [ 12335 / 14317, 1917 ins, 4064 del, 6354 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it1/wer_11_1.0
%WER 86.15 [ 12334 / 14317, 1980 ins, 4056 del, 6298 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it1/wer_11_1.0
%WER 85.70 [ 12270 / 14317, 2001 ins, 4035 del, 6234 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 85.33 [ 12217 / 14317, 2044 ins, 4002 del, 6171 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it4/wer_11_1.0
%WER 85.24 [ 12204 / 14317, 1920 ins, 4038 del, 6246 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 85.23 [ 12203 / 14317, 2023 ins, 4008 del, 6172 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it3/wer_11_1.0
%WER 85.02 [ 12172 / 14317, 1991 ins, 3924 del, 6257 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it3/wer_10_1.0
%WER 84.77 [ 12137 / 14317, 1944 ins, 4011 del, 6182 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it4/wer_11_1.0
%WER 72.88 [ 9528 / 13073, 585 ins, 2216 del, 6727 sub ] exp/mono_semi_supervised/decode_answers/wer_9_0.5
%WER 62.89 [ 10499 / 16694, 666 ins, 4244 del, 5589 sub ] exp/tri1_semi_supervised/decode_answers/wer_12_0.5
%WER 62.86 [ 2006 / 3191, 67 ins, 772 del, 1167 sub ] exp/mono_semi_supervision_2/decode_test/wer_9_0.0
%WER 60.17 [ 1920 / 3191, 117 ins, 1025 del, 778 sub ] exp/tri5_semi_supervision_2/decode_test.si/wer_10_0.0
%WER 59.39 [ 1895 / 3191, 135 ins, 426 del, 1334 sub ] exp/mono_semi_supervision/decode_test/wer_13_0.0
%WER 58.95 [ 9841 / 16694, 725 ins, 4054 del, 5062 sub ] exp/tri2_semi_supervised/decode_answers/wer_14_0.0
%WER 57.98 [ 1850 / 3191, 111 ins, 475 del, 1264 sub ] exp/mono_semi_supervised/decode_test/wer_11_0.0
%WER 56.06 [ 1789 / 3191, 84 ins, 1075 del, 630 sub ] exp/tri4_semi_supervision_2/decode_test/wer_9_0.0
%WER 54.00 [ 1723 / 3191, 78 ins, 963 del, 682 sub ] exp/tri3_semi_supervision_2/decode_test/wer_10_0.0
%WER 49.67 [ 1585 / 3191, 168 ins, 423 del, 994 sub ] exp/tri5_semi_supervision/decode_test.si/wer_13_1.0
%WER 47.54 [ 1517 / 3191, 51 ins, 830 del, 636 sub ] exp/tri2_semi_supervision_2/decode_test/wer_11_0.0
%WER 47.51 [ 1516 / 3191, 96 ins, 613 del, 807 sub ] exp/tri1_semi_supervision_2/decode_test/wer_10_0.0
%WER 46.94 [ 1498 / 3191, 200 ins, 264 del, 1034 sub ] exp/tri5_semi_supervised/decode_test.si/wer_13_0.0
%WER 44.02 [ 1410 / 3203, 149 ins, 306 del, 955 sub ] exp/tri5/decode_test.si/wer_19_1.0
%WER 43.18 [ 1378 / 3191, 83 ins, 384 del, 911 sub ] exp/tri1_semi_supervision/decode_test/wer_12_1.0
%WER 42.78 [ 1365 / 3191, 108 ins, 458 del, 799 sub ] exp/tri3_semi_supervision/decode_test/wer_18_0.0
%WER 42.24 [ 1348 / 3191, 145 ins, 336 del, 867 sub ] exp/sgmm5_semi_supervision/decode_test_no_mllr/wer_10_0.0
%WER 41.52 [ 1325 / 3191, 137 ins, 268 del, 920 sub ] exp/tri1_semi_supervised/decode_test/wer_12_0.0
%WER 41.49 [ 1324 / 3191, 157 ins, 345 del, 822 sub ] exp/tri4_semi_supervision/decode_test/wer_14_0.0
%WER 41.18 [ 21 / 51, 5 ins, 0 del, 16 sub ] exp/tri5_semi_supervised/decode_answers.si/wer_16_0.0
%WER 39.05 [ 1246 / 3191, 95 ins, 332 del, 819 sub ] exp/tri2_semi_supervision/decode_test/wer_15_0.0
%WER 37.01 [ 1181 / 3191, 111 ins, 277 del, 793 sub ] exp/tri2_semi_supervised/decode_test/wer_12_0.5
%WER 34.78 [ 1114 / 3203, 168 ins, 225 del, 721 sub ] exp/tri5/decode_test/wer_19_1.0
%WER 34.66 [ 1106 / 3191, 141 ins, 350 del, 615 sub ] exp/tri5_semi_supervision_2/decode_test/wer_13_0.0
%WER 32.81 [ 1047 / 3191, 91 ins, 367 del, 589 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it4/wer_13_0.0
%WER 32.62 [ 1041 / 3191, 134 ins, 270 del, 637 sub ] exp/tri5_semi_supervision/decode_test/wer_15_0.0
%WER 31.96 [ 1020 / 3191, 109 ins, 313 del, 598 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it3/wer_10_0.0
%WER 31.12 [ 993 / 3191, 110 ins, 293 del, 590 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it2/wer_10_0.0
%WER 29.87 [ 953 / 3191, 142 ins, 197 del, 614 sub ] exp/tri5_semi_supervised/decode_test/wer_18_0.0
%WER 29.84 [ 3901 / 13073, 485 ins, 598 del, 2818 sub ] exp/tri5_semi_supervised/decode_answers/wer_16_0.5
%WER 29.52 [ 942 / 3191, 131 ins, 199 del, 612 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it1/wer_9_0.0
%WER 29.22 [ 936 / 3203, 162 ins, 135 del, 639 sub ] exp/dnn6_pretrain-dbn_dnn/decode_test/wer_10_1.0
%WER 28.61 [ 913 / 3191, 129 ins, 163 del, 621 sub ] exp/sgmm5_semi_supervision_2/decode_test/wer_9_0.0
%WER 27.72 [ 888 / 3203, 160 ins, 128 del, 600 sub ] exp/sgmm5/decode_test/wer_17_0.0
%WER 27.41 [ 878 / 3203, 110 ins, 183 del, 585 sub ] exp/sgmm5/decode_fmllr_test/wer_16_1.0
%WER 26.16 [ 838 / 3203, 145 ins, 131 del, 562 sub ] exp/sgmm5_mmi_b0.1/decode_test_it1/wer_18_0.0
%WER 25.98 [ 832 / 3203, 107 ins, 163 del, 562 sub ] exp/sgmm5_mmi_b0.1/decode_test_it3/wer_20_0.5
%WER 25.94 [ 831 / 3203, 107 ins, 169 del, 555 sub ] exp/sgmm5_mmi_b0.1/decode_test_it4/wer_20_0.5
%WER 25.85 [ 828 / 3203, 149 ins, 120 del, 559 sub ] exp/sgmm5_mmi_b0.1/decode_test_it2/wer_15_0.0
%WER 24.38 [ 778 / 3191, 99 ins, 201 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it4/wer_12_0.0
%WER 24.22 [ 773 / 3191, 95 ins, 200 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it3/wer_12_0.0
%WER 23.97 [ 765 / 3191, 118 ins, 148 del, 499 sub ] exp/sgmm5_semi_supervision/decode_test/wer_11_0.0
%WER 23.82 [ 760 / 3191, 94 ins, 188 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it2/wer_12_0.0
%WER 23.79 [ 759 / 3191, 97 ins, 169 del, 493 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it1/wer_12_0.0
%WER 22.38 [ 714 / 3191, 91 ins, 141 del, 482 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it4/wer_12_0.5
%WER 22.03 [ 703 / 3191, 92 ins, 148 del, 463 sub ] exp/sgmm5_semi_supervised/decode_test/wer_17_0.0
%WER 21.69 [ 692 / 3191, 116 ins, 111 del, 465 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it2/wer_11_0.0
%WER 21.62 [ 690 / 3191, 96 ins, 126 del, 468 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it3/wer_14_0.0
%WER 21.25 [ 678 / 3191, 93 ins, 126 del, 459 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it1/wer_14_0.0
%WER 20.78 [ 2693 / 12960, 283 ins, 516 del, 1894 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it4/wer_14_1.0
%WER 20.56 [ 2665 / 12960, 299 ins, 482 del, 1884 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it3/wer_12_1.0
%WER 19.91 [ 2580 / 12960, 290 ins, 466 del, 1824 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 19.31 [ 2502 / 12960, 309 ins, 419 del, 1774 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it1/wer_11_0.5
%WER 18.29 [ 2370 / 12960, 317 ins, 399 del, 1654 sub ] exp/sgmm5_semi_supervised/decode_answers/wer_11_0.5
%WER 129.10 [ 83893 / 64984, 18938 ins, 23081 del, 41874 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 129.08 [ 83884 / 64984, 18930 ins, 23127 del, 41827 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 129.02 [ 83845 / 64984, 18890 ins, 23122 del, 41833 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 129.02 [ 83842 / 64984, 18888 ins, 23086 del, 41868 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 128.74 [ 83663 / 64984, 20719 ins, 22972 del, 39972 sub ] exp/tri5_semi_supervised/decode_sri_gabon/wer_20_1.0
%WER 128.71 [ 83643 / 64984, 20832 ins, 23084 del, 39727 sub ] exp/tri5_semi_supervised/decode_sri_gabon.si/wer_20_1.0
%WER 127.02 [ 82542 / 64984, 19483 ins, 23425 del, 39634 sub ] exp/tri5/decode_sri_gabon/wer_20_1.0
%WER 126.76 [ 82376 / 64984, 19248 ins, 23295 del, 39833 sub ] exp/sgmm5/decode_fmllr_sri_gabon/wer_20_1.0
%WER 126.69 [ 82327 / 64984, 19196 ins, 23309 del, 39822 sub ] exp/sgmm5/decode_sri_gabon/wer_20_1.0
%WER 126.29 [ 82069 / 64984, 18948 ins, 23521 del, 39600 sub ] exp/tri5/decode_sri_gabon.si/wer_20_1.0
%WER 122.17 [ 79518 / 65089, 16340 ins, 27735 del, 35443 sub ] exp/tri5_semi_supervision/decode_sri_gabon.si/wer_20_1.0
%WER 121.86 [ 79320 / 65089, 16043 ins, 27404 del, 35873 sub ] exp/tri1_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 121.01 [ 78761 / 65089, 15386 ins, 27515 del, 35860 sub ] exp/tri5_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 119.70 [ 77787 / 64984, 14549 ins, 27388 del, 35850 sub ] exp/sgmm5_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 119.62 [ 77735 / 64984, 14497 ins, 27461 del, 35777 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 119.56 [ 77697 / 64984, 14442 ins, 27484 del, 35771 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 119.54 [ 77680 / 64984, 14435 ins, 27484 del, 35761 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 119.52 [ 77670 / 64984, 14419 ins, 27498 del, 35753 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 117.68 [ 76598 / 65089, 12998 ins, 29308 del, 34292 sub ] exp/mono_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 117.44 [ 76320 / 64984, 13110 ins, 29970 del, 33240 sub ] exp/mono_semi_supervised/decode_sri_gabon/wer_20_1.0
%WER 115.39 [ 74982 / 64984, 11584 ins, 30617 del, 32781 sub ] exp/tri5_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 114.57 [ 74453 / 64984, 11177 ins, 29937 del, 33339 sub ] exp/sgmm5_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 114.41 [ 74346 / 64984, 11038 ins, 30111 del, 33197 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 114.35 [ 74309 / 64984, 11030 ins, 30215 del, 33064 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 114.35 [ 74307 / 64984, 11026 ins, 30203 del, 33078 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 114.32 [ 74290 / 64984, 11028 ins, 30189 del, 33073 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 105.41 [ 15091 / 14317, 2817 ins, 3819 del, 8455 sub ] exp/tri5/decode_answers.si/wer_20_1.0
%WER 104.25 [ 67748 / 64984, 3837 ins, 45236 del, 18675 sub ] exp/tri5_semi_supervision_2/decode_sri_gabon.si/wer_20_1.0
%WER 104.13 [ 39389 / 37827, 2166 ins, 4183 del, 33040 sub ] exp/tri5_semi_supervised/decode_sri_gabon_read.si/wer_20_1.0
%WER 104.11 [ 67658 / 64984, 3680 ins, 44021 del, 19957 sub ] exp/tri1_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 103.37 [ 14799 / 14317, 2846 ins, 3682 del, 8271 sub ] exp/tri5/decode_answers/wer_20_1.0
%WER 103.14 [ 39015 / 37827, 1944 ins, 2591 del, 34480 sub ] exp/tri5_semi_supervised/decode_sri_gabon_read/wer_20_1.0
%WER 103.10 [ 66998 / 64984, 2974 ins, 45341 del, 18683 sub ] exp/mono_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 102.91 [ 66873 / 64984, 2721 ins, 47246 del, 16906 sub ] exp/tri2_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 102.08 [ 38613 / 37827, 1579 ins, 2191 del, 34843 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it4/wer_20_1.0
%WER 102.06 [ 38607 / 37827, 1584 ins, 2198 del, 34825 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it3/wer_20_1.0
%WER 102.05 [ 38602 / 37827, 1577 ins, 2216 del, 34809 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it2/wer_20_1.0
%WER 102.00 [ 38583 / 37827, 1561 ins, 2232 del, 34790 sub ] exp/sgmm5_semi_supervised/decode_sri_gabon_read/wer_20_1.0
%WER 101.98 [ 66270 / 64984, 2044 ins, 49862 del, 14364 sub ] exp/tri3_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 101.97 [ 38571 / 37827, 1549 ins, 2225 del, 34797 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it1/wer_20_1.0
%WER 101.89 [ 66212 / 64984, 1966 ins, 50793 del, 13453 sub ] exp/tri4_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 100.28 [ 14357 / 14317, 853 ins, 6401 del, 7103 sub ] exp/mono_semi_supervision_2/decode_answers/wer_19_1.0
john@A-TEAM19054:~/yaounde/kaldi-trunk/egs/gp+yaounde$ 

** WAR:
Since I'm going on leave tomorrow, I'm writing a WAR today.
John Morgan achieved a new best word error rate (WER) score for the speech recognizer he is building with the kaldi toolkit on African accented French. 
The new best WER is 21.25 down from the previous best of 23.79. 
The improvement was obtained by automatically cleaning the transcripts of the data that was transcribed by the recognizer in the previous supervised stage of training. 

** Performance Objectives
The form is a nightmare.
General Objectives:
ASR Adaptation:
What is practical?
What method works in an S2S device?
What kind of speaker adaptation can be done online?
The best results we are getting with kaldi are speaker dependent.
Can these models be used in a S2S device?
If not, what are the best models for an S2S device?
Latest Methods:
Variable computational graphs.
Learn pycnn.
How can these methods be used for Army needs?
kaldi:
Chain Models.
nnet2
nnet3

1. Speech Recognizer Adaptation:
a. Acoustic Modeling.
i. European French to African accents
ii. Standard Arabic to Tunisian accent.
iii. Neural Network Models:
A. RBM
B. TDNN
C. RNN/LSTM
D. Chain 
b. Language Modeling:
i. Dialogue modeling:
A. French
B. Arabic 
ii. Lexicon expansion
A. French
B. Arabic.
C. Dari
2. Machine Translation
a. Variable Computation Graphs

After I sent out this report to Steve, I continued working on the next stage of semisupervised training.
The acoustic models have trained through tri5.
I am making the fst decoding graph as I am laeaving.
This will also decode the test, answers  and sri_gabon_read sets.
I have also prepared the sri_gabon_conv data directory.
I have extracted plp  pitch features from it too.
The next step will be to decode the sri_gabon_conv set with the tri5 models to get transforms.

** Goals for When I come back from leave:
- TODO Write objectives and put them in the form (I'll need help with the form).
- TODO Finish second stage of semi supervised training.
- TODO Use best resulting models to transcribe sri_gabon_conv data.
- TODO Get qualitative evaluation of these transcripts from Steve.
- TODO Wrap up sgmm ASR system build recipes.
- TODO Start on neural network approaches to ASR
- TODO Compare neural network approaches to baseline sgmm approach (this is a long term goal. To be achieved by Xmas)  
* <2016-10-05 Wed>
** Goals for Wednesday set Tuesday:
- TODO finish writing the script to prepare the sri_gabon_read data
- TODO Wrap up my current run of the first stage of semi supervised training.


The mono2sgmm script failed at sgmm denominator lattice making.
Why did it fail?
The fst decoding graph already existed, so it did not remake it.
This could be the problem.
I deleted the directory where the work on the denominator lattices is done and I am rerunning the denlats making script.

I decode the test set with the tri5 semi supervised models.
WER: 29.87 for speaker dependent models
WER: 46.94 for speaker independent models.

I screwed up again.
I deleted the data/train_semi_supervised directory while working on the script to do the next stage.
So the sgmm denominator lattice making died.
I still might be able to run the test decoding with the regular sgmm models.
I reran the data prep script and the plp pitch extractor again.
I'm rerunning the denlats script just in case it works without having to start over.
Well ... it seems to be running.
It seems to have crashed and restarted?
it is still running.
It does decoding as part of the denlats making.
I think half of the jobs died.
There are only 5 directories and there should be 10.

The decoding graph script for sgmm5 semi supervised finally finished. I'm surprised it did not crash.
The decoding is running, also surprising.

The denlats generation is also running. 

** Goals for Thursday
- TODO Write a first pass on objectives
- TODO Wrap up yaounde answers semi supervised training 
- TODO Start on sri_gabon_read semi supervised traing.
* <2016-10-04 Tue>
** Goals for Tuesday set Monday:
- DONE Write script to remove asterisks from per_utt file.
I just had to debug the perl script I had written yesterday.
- TODO Rerun semi supervised training with the improved labels. 
I'm starting over again at the semi supervision step.
I'm extracting the plp and pitch features.
Can I skip to tri5?
tri5 requires the alignments from tri4.
The first step, the monophone training, uses a flat start.
I decode the answers data to get the transcripts.
As part of this decoding, alignment is performed (I think).
Maybe not by default.
I do not know how to jump to tri5 with the semi supervised data.

I am now training monophones with the semi supervised data.
The labels for the semi supervised data now does not contain the asterisks.
I killed the process I was running and I'm starting over again.
I am going  to use a script that runs all the steps from feature extraction through monophones to sgmm5.
I had such a script for the first supervised stage of training.
I had to modify it or the semi supervised stage.
I decoded the test set with semi supervised monophones. 
WER: 57.98
- TODO Separate out read sri_gabon data:

- lexicon work:
I'm removing the numbers in parens after some words in the lexicon.

As I'm getting ready to leave, I'm working on the script to prepare the sri_gabon_read data.
** Goals for Wednesday:
- TODO finish writing the script to prepare the sri_gabon_read data
- TODO Wrap up my current run of the first stage of semi supervised training.
* <2016-10-03 Mon>
** Goals For today Monday:
- TODO Rerun the semi supervised training experiment

I found the problem with the automatic transcripts I was using for semi supervised training.
I already made a mistake in stage 1 when I used the Answers transcripts for training.
The file I was using was actually the reference transcripts, which in the answers case were the questions.
I was considering extracting the recognizer output from the decoding logs, but the log files are different when you get up into the sgmm modeling.
I found another file that contains the hypotheses from the recognizer.
I have to be careful, because it also contains the reference and some other files used in scoring.
I am going back to the first semi supervised stage and using these new transcripts.
They contain symbols like "*" that worry me a little.

I have finished training monophone through sgmm acoustic models in supervised mode. 
I am trying to decode with the sgmm models.
First, I need to decode with the tri5 models to get transforms.
Yes. Now that I have the tri5 transforms, the decoding is going forward with sgmm models.
I made the mistake of firing up all the decoding steps at one.
This bogged the machine and some of the processes failed.
I'm going to go through them all 1 at a time.
First monophones

The monophones score lower 

I've been looking at the output from the decoding with semi supervised monophones on the sri_gabon data.
Look at speaker 048.
She seems to be reading the whole list of prompts in one recording.
There are a lot of very long utterences in several directories.
No wonder it takes so long to decode the sri_gabon data. this speaker 113 goes on and on and on ...

- problem:
The three asterisks that appear in the wer_details/per_utt file make the scoring fail when I use it as the reference.
I'll have to delete them before using it as a reference or as the source of my labels.
** Goals for Tuesday:
- TODO Write script to remove asterisks from per_utt file.
- TODO Rerun semi supervised training with the improved labels. 
* Goals for Friday set Thursday<2016-09-30 Fri>:
- TODO Finish second stage of semi supervised training with sri_gabon

The mmi training is still running on the gpu machine on the sri_gabon semi supervised stage.
Actually, it hasn't even got to training yet, it's still on denominator lattice generation.
- DONE Run model building scripts for gp only system on gpu machine ( maybe put several steps in 1 script)

I started a script that (if all goes well) will train the acoustic models tri2 through sgmm5.
The scripts I had run before got through tri1 training. So this one starts with alignment using tri1.

I started a similar script starting at monophones for yaounde+gp on my laptop.
The difference with these scripts is that I'm only concentrating on training the acoustic models. I don't make the decoding graphs and I do not decode. 
I'll make scripts for decoding graphs, decoding and the lm separately.
* Goals for Thursday set Wednesday: <2016-09-29 Thu>
- DONE GP model building on gpu machine

I'm starting to run the gp model building scripts on the gpu machine and I'm finding an interesting problem. 
The files don't seem to get sorted in the same wasy.
I think the   sorting problem  depends on environment variables
There might also be a file concatenation problem.
I am concatenating files somewhere and I am not deleting the old file before starting the concatenation. 
I had done a lot of these fixes under the yaounde+gp directory.
I am copying those fixes to the gp directory.
Problems remained.
There were concatenation problems at every step.
I spent most of the morning fixing these problems.
I think I'm good now on data prep.
I spoke a little too soon. I had do more fixes for the sorting.
Now I'm getting similar problems with the test data.
I was finally able to move on to the next steps.

- TODO Write TR
- TODO Finish gp + yaounde sgmm model building

It's still cooking.
- TODO Read papers

I've got several things running as I'm getting ready to leave.
- The second stage of the semi supervised training with sri_gabon is still making the denominator lattices.
- The model building for the gp only system is training monophones on the gpu machine
- sgmm models for the gp only system are being trained on my laptop
- Decoding of  sri_gabon  with sgmm5 modles
I'll only be here for half a day tomorrow, so I'm not expecting to much done.
The modle building steps from monophones to sgmm5 are pretty stable in my scripts. I might put them all in 1 script and run them on the gpu machine.
The step after the first sgmm5 step require an decoding fst from tri5, but this requires decoding with tri5 models. So, I'll do this step in a separate script before starting the next sgmm5 step.

** Goals for Friday:
- TODO Finish second stage of semi supervised training with sri_gabon
- TODO Run model building scripts for gp only system on gpu machine ( maybe put several steps in 1 script)

* Goals for Wednesday set Tuesday:
- TODO Another pass at big 6 accomplishments
** Big 6 accomplishments
Name: John Morgan
Office:
Team Leader (No):
Grade: DB03
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations – at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  Yes
Recognition (awards, letters of appreciation, etc.)
 List significant projects you are working on & your specific responsibilities:
Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.
Documentation of processes used in the project.
Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.
6 Most significant Actions / Impacts
1.  
Action: 1. Applied    algorithms from the reference kaldi ASR toolkit to in-house data sets for   speech recognition tasks of interest to the US ARMY.
Impact: An important outcome of this effort is a capability and expertise at using GPU-based technology in the MCAB branch. 
2. 
Action: 1. Applied  Deep Learning approaches with  toolkits to in-house data sets for machine translation  tasks of interest to the US ARMY.
Impact: As a requirement for this project I became proficient at python programming, which the branch can now count on as a capability. 
3. 
Action: 1. Research work with UMD professors on simultaneous translation. 
Impact: As a requirement for this project, I bcame familiar with the area of machine learning  called reinforcement learning, which should have an important impact on future applications of Deep Learning to NLP applications.
4. 
Action: 1. Coded Recurrent Neural Networks for prediction    in simultaneous translation. 
Impact: Understanding of the functioning of RNNs at an elementary level.
5.  
Action: 1. Investigated a potentially novel approach to adaptively training an ASR system to an accented version of a world Language.
Impact: Potential cost savings in transcription of collected speech data.
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY

** Accomplishments
Technical Competency:
Apply methods, theories, techniques, and skills learned in Computer Science Ph.D. corsework and research at UMD to projects of interest to ARL and the ARMY.

In this past year I took and passed 2 graduate courses, 1 in Scientific Computing and 1 in Database Management Systems.
The course in Scientific Computing covered the fundamental theory of optimization, which is  relevant to the computational solutions of many if not most problems in NLP and virtually all problems in Deep Learning. 
I was introduced to matlab and octabe in this course and I studied the implementations of important algorithms in the python modules numpy and scipy. 
Our branch has several of its own speech and text corpora that have not yet been curated. 
The DBMS course I took will help our branch utilize and share our data for the benefit of the Army.

As part of a research team at UMD, I developed software that uses Deep and Reinforcement Learning techniques to predict language behaviors of a simultaneous translator. 

Develop machine translation software using the theories and methods emerging from the field of Deep Learning.

I became familiar with several toolkits for developing neural networks including google's open source Tensorflow, theano and keras. 
I used recurrent neural networks (RNN)s with long short term memory and gated recurrent units to perform Machine Translation. 
I wrote code to implement an RNN that makes predictions about future words a simultaneous translators will choose to interpret. 

Use python programming skills to develop software in ARL projects.

Both the Tensorflow and Theano toolkits are implemented in python. 
I used my python programming skills to apply the tools in those toolkits to develop the MT systems I worked with.

Support the team project to implement an Automatic Speech Recognition system adapted to  speech as spoken in African countries. 

I made good progress on this objective. 
I dedicated a lot of hard work to preparing our inhouse speech corpora  for processing by ASR system development tools. 

I can now build subspace gaussian mixture model based ASR systems with the kaldi toolkit. 
The recipes I have developed to prepare and process the African accented speech can be applied to our other holdings.
I am investigating a semi supervised  acoustic model training strategy that could potentially cut the cost of transcribing collected speech data.

Cooperation:

I collaborated closely with Dr. Stephen LaRocca on his project to implement an algorithm for selecting data to be used to train statistical n-gram language models. 

Serve as a bridge between the multilingual computing branch and the academic community at UMD. 

Communication:

Publish a journal paper as first author on research on simultaneous translation.


Customer Relations:

Respond to requests from team customers for advice and technical support on
issues concerning machine translation and machine learning.

Technology Transition:

Transition simultaneous translation code developed for research into branch projects.


Diversity:
Support ARL's diversity initiatives by participating in locally sponsored training, outreach and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees.

I learned a lot about the sacrifices African American soldiers made during World War II for the U.S. Army by attending a film in the ALC auditorium.
I also served as a speaker on a panel for Disability Awareness Month.

SHARP:
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all.
Demonstrate support for the SHARP program by actively participating in required training and other educational programs.  
Intervene and appropriately respond to any instances of sexual harassment or sexual assault; encourage others to do the same.
End of Accomplishments

- TODO Finish sri_gabon semi supervised model building and decoding.

The WER results are really looking bad for the second stage of the semi supervised training strategy. 
The transcripts that we thought look really good actually are really bad. 
I'm not sure why?
The output looks very fluent, but they don't look like they are aligned to the speech. 
I think they are just bad. 

- TODO Write the tr.

The bad results on the sri_gabon actually supports the point I wanted to make in the tr. 
My hypothesis is that if you put just a little effort in collecting some read speech, it makes a big difference. 
Our results might show this.
The results on the yaounde answers are very good (maybe). 
This is the case where there is an overlap in the speakers, so it is quasi semi supervised.
When you don't have this overlap, that is, the semi supervised case, you get bad results.

- Experiment:
I think I need to run another experiment in order to support the point I'm making in the tr.
My point is that there is a large payoff to collecting some read data on a speech data collection   mission. 
- Strategy 1:
Do supervised training first with the read speech concatenated to the out of domain corpus (gp+yaounde read)
Automatically transcribe the unlabeled corpus yaounde answers.
Do semi-supervised training with  yaounde answers automatically generated transcriptions 
Use these models to decode yaounde answers
- Strategy 2:
Do supervised training with only the out of domain corpus.
Automatically transcribe the unlabeled corpus sri_gabon
Do semi-supervised training with  automatically generated transcriptions of sri_gabon
Use these models to transcribe the sri_gabon data.

Find the amount of read data required to get similar results.


- MAP:
This comment is in the steps/train_map.sh script in kaldi:
# Train a model on top of existing features (no feature-space learning of any
# kind is done).  This script does not re-train the tree, it just does one iteration
# of MAP adaptation to the model in the input alignment-directory.  It's useful for
# adapting a system to a specific gender, or new acoustic conditions.

# Note: what we implement here is not the MAP from the paper by Gauvain and Lee,
# it's the simpler (and, I believe, more widely used) so-called "relevance MAP",
# implemented in HTK, where we add a fixed count "tau" of fake Gaussian stats
# generated from the old model, to the new 'in-domain' stats from the features
# and alignments provided;  and we only update the mean.  So if the new count
# is zero it just gives you the Gaussian parameters from the old model, but as
# you get more than about tau counts, it approaches the in-domain stats.
# We use 'gmm-ismooth-stats' in the command line because the equations for this
# are the same as the equations for i-smoothing in discriminative training
# (for which, see my [Dan Povey's] PhD thesis).

There is also a script called steps/decode_with_map.sh in kaldi. 

- GP:
I've started the process of building models on the GP corpus alone.
I'm doing this on my laptop for now. 
** Goals for Thursday:
- TODO GP model building on gpu machine
- TODO Write TR
- TODO Finish gp + yaounde sgmm model building
- TODO Read papers
* Goals for Tuesday set Monday:
- DONE Continue with the sri_gabon part of semi supervised model ASR training.

I'm not sure why I did not start this before leaving yesterday. The decoding of the sri_gabon data by the sgmm5 mmi models had finished.
I wrote the scripts for all the steps needed to complete the second stage of semi supervised training. 
I have so far run the mono, tri1 and tri2 training and decoding scripts.
As soon as the training finishes, I start the alignment for the next model set. I don't have to wait for decoding to finish.

- TODO Another pass on accomplishments and top 6.
- TODO Write more for tr.
- DONE The IDP form
I took a first pass at filling out the IDP form. It may not be possible for me to do this with JAWS.

I've spent most of the day writing and running the scripts to do the sri_gabon stage of supervised training.
I've almost gone through the whole process of training. 
The decoding lags way behind.
Something weird that has me worried.
The tri3 model decoding yielded results before the tri2 model decoding.
I checked and the tri2 models is decoding, it has not died.
The tri3 models are currently decoding the sri_gabon data.

I also read the sgmm accent adaptation paper.

As I'm getting ready to leave, the sgmm5 model set is being trained with the answers and sri_gabon semi supervised data.
Hopefully, this will be finished tomorrow and I'll start the last steps of this model building with the denominator lattice generation, mmi training and decoding.
I also hope all the graph building and decoding up to tri5 and sgmm5 is finished. 
I want to start writing the tr more seriously. 
This will motivate me to wrap up this project before I go on leave.
I'll have to fill in the missing pieces.
I might want to do an experiment where  I use the sri_gabon data as unsupervised training data with gp.
 
** Goals for Wednesday:
- TODO Another pass at top 6 accomplishments
- TODO Finish sri_gabon semi supervised model building and decoding.
- TODO Write the tr.

 
* Goals for Monday set Friday:
- TODO Fix the mess I got myself into  for trying to get rid of mllr.

I reran the script that makes the decoding fst graph. This takes a long time.
After this finished I ran the script to decode the sri_gabon data.
It still fails.
My guess is there is a problem with the features. 
My guess is that there is a problem with the file containing the map between the file names containing the extracted eatures and the files actually containing the extracted features.
My guess is that this file got corrupted when I ran the program to generate the mfcc features.
I am now rerunning the plp pitch feature extractor to see if this works and if my guess is correct.
The decoding is now running, although I won't know for sure until it finishes if my guess was right.
This is a good lesson for anyone who wants to learn kaldi.
The files like utt2spk, spk2utt, feats.scp, wav.scp, ... are very important in kaldi.
You're not going to get very far if you don't copy the pattern in these files.
these files get created/modified when I run the plp and pitch feature extractor.
Scripts downstream will fail if you change these files upstream.
So, I probably didn't have to do all the script rerunning I did.
I probably just had to rerun the plp and pitch feature extractor.
The decoder has an argument pointing to the directory containing information about the test data or in my particular case right now the data/sri_gabon data directory.
That information was incorrect, because I had run the mfcc extractor over the sri_gabon and that information was written to the data/sri_gabon directory.
The models I had been building used the previously stored plp pitch feature vectors.
So when I went to decode the data the input vectors were mfccs which have their standard dimensions.
The models on the other hand had been trained with plp pitch vectors which have a different standard dimension.

 
- TODO If this gets fix, decode the sri_gabon data

It apparently got fixed and I am going through the process of decoding the sri_gabon data.
It is taking a long time to decode the sri_gabon data with the sgmm5 models.

- TODO Use the automatic transcripts of the sri_gabon as semi supervised training data (try to finish this before moving on to nnet2 stuff).
- TODO make another pass on top 6 list and accomplishments
** Top 6 accomplishments
Name: John Morgan
Office:
Team Leader (No):
Grade: 
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations – at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  No
Recognition (awards, letters of appreciation, etc.)
 List significant projects you are working on & your specific responsibilities:
Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.
Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.
6 Most significant Actions / Impacts
1.  
Action: 1. Applied state-of-art Deep Learning approaches with several toolkits to in-house data sets for machine translation and speech recognition tasks of interest to the US ARMY.
Impact: An important outcome of this effort is a capability and expertise at using GPU-based technology in the MLCAB branch. 
2. 
Action: 1. 
Impact: As a requirement for this project I became proficient at python programming, which the branch can now count on as a capability. 
3. 
Action: 1. Research work with UMD professors on simultaneous translation. 
Impact: Also as a requirement for this project, I bcame familiar with the area of machine learning  called reinforcement learning, which should have an important impact on future applications of Deep Learning to NLP applications.
4. 
Action: 1. 
Impact: 
5. 
Action: 1. 
Impact: 
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY


As I'm preparing to leave, the sri_gabon decoding with sgmm5 models step is still running. 
I'm not sure what step I'll take tomorrow when this decoding  finishes. 
I'll try to skip directly to decoding with the discriminative mmi models, but I suspect this will fail. 
IIn that case, I'll have to take the next steps, wich are to align, extract denominator lattices and mmi train.
After that I should finally be able to decode the sri_gabon data.

The decoding finally finished.
So now I have sgmm5 transcripts of the sri_gabon data.
I also started the decoding with the discriminative mmi trained sgmm5 models.
it looks like this is running and I don't have to run the alignment, denominator lattice generation and mmi training over again.
The only problem was with the feature maps that I explained above.
I was thinking of waiting for this decoding to finish because it is for some reason very fast decoding, but the corpus is pretty big 7400 utterences.

** Goals for Tuesday:
- TODO Continue with the sri_gabon part of semi supervised model ASR training.
- TODO Another pass on accomplishments and top 6.
- TODO Write more for tr.
- TODO The IDP form
* Goals for Friday set Thursday: <2016-09-23 Fri>
- TODO Write scripts to run semi supervised training with sri_gabon data in addition to the answers data.
- TODO Write accomplishments
- TODO Write top 6 accomplishments document.
- TODO Write more on tr.
- TODO Run scripts to train sgmms discriminatively with mmi.

The script to train the denominator lattices had finished when I came in this morning.
I started the script to do mmi training on the sgmms.
Training finishes relatively quickly (maybe 2 hours).
I looked at the script to decode with the mmi discriminatively trained models. 
I needed to add the command to decode the sri_gabon data. 
Transforms are required to run this decoding.
You get the transforms by decoding the test data with the tri5 data.
So there 's some cheating going on here.

- Data preparation for sri_gabon semi supervision:
When I wrote the script to prepare the answers semi supervision training, I wrote the output training files to a directory called data/train_semi_supervision.
I did this so as not to overwrite the training files from the supervised training stage.
Now when I add the sri_gabon data, I am going to write the training files to a directory called data/train_semi_supervision_2.
The best WER was 23.79 slightly better than the best so far.
I think we need another test set.
The test set we've been using will be the devtest.

- online nnet2 
I'm trying to start the process of building an online nnet2 system on our African Accented corpus.
I got the recipe scripts from kaldi/egs/wsj/s5/local/online
The first step is to extract MFCCs.
These recipes use a different config file for the mfcc extraction.
They extract higher resolution features.
I guess they work better with neural nets.
They add the suffix hires to some files and directories.
Then a diag ubm set is trained.
They need to run an fmllr alignment step to do this training.
Then the lda mllt training is run.
Then the ubm training is run on top of this.

I am running into trouble.
The hires mfcc features hozed my old mfcc features.
I don't  think that's a big deal, I just have to run the old script over again.
It copied the data/ directories, so it should not hurt them. Just in case, I'm rerunnig the prepare script to remake them.
I'm going to skip the hires features for now.
I'm going to continue the training from tri5.
This is what the rm recipe does.
The wsj recipe gets fancy with the new hires features and copying directoreis ect...
So, I definitely need to rerun the script that extracts the mfcc features.

As I'm getting ready to leave I'm trying to take the last step of the first stage of semi supervised training.
I stillhave to decode the sri_gabon data with the best sgmm models.
The problem was that in order to do this, I had to decode the sri_gabon data with the tri5 models in order to get the mllr transforms.
That is now done.
I am waiting to get the mfcc features back after having hozed them.
Then I'll run the sri_gabon data through the decoder with the sgmm models.
This will give me sri_gabon automatic transcripts.
then I can start the building process over again with both answers and sri_gabon as semi supervised data.
Oops, I was not using mfcc features for the model building, so I did not actually hoze anything.
So, what do I do now for the nnet2 recipe?
Can I use plp + pitch   features?
I'm having problems decoding  the sri_gabon data too.
It looks like I messed up things when I tried to get rid    of mllr.
I'm going back to the step where I tried to make that change.
I started doing it at the ubm training stage.
I'm running that training script over again.
I'm going back that far, because the next step fails.

I ran the ubm training again.
Now I'm running the sgmm training (step 57)

** Goals for Monday:
- TODO Fix the mess I got myself into  for trying to get rid of mllr.
- TODO If this gets fix, decode the sri_gabon data
- TODO Use the automatic transcripts of the sri_gabon as semi supervised training data (try to finish this before moving on to nnet2 stuff).
- TODO make another pass on top 6 list and accomplishments

* Goals for <2016-09-22 Thu>
- TODO run scripts to build sgmms.
- TODO Write accomplishments.

Technical Competency:
Apply methods, theories, techniques, and skills learned in Computer Science
Ph.D. corsework and research at UMD to projects of interest to ARL and the
ARMY.

In this past year I took and passed 2 graduate courses, 1 in Scientific Computing and 1 in Database Management Systems.
The course in Scientific Computing covered the fundamental theory of optimization, which is  relevant to the computational solutions of many if not most problems in NLP and virtually all problems in Deep Learning.
Our branch has several of its own speech and text corpora that have not yet been curated. The DBMS course I took will help our branch utilize and share our data for the benefit of the Army.

As part of a research team at UMD, I developed software that uses Deep and Reinforcement Learning techniques to predict 
language behaviors of a simultaneous translator. 

Develop machine translation software using the theories and methods emerging
from the field of Deep Learning.

I became familiar with several toolkits for developing neural networks including google's open source Tensorflow, theano and keras. I used recurrent neural networks with long short term memory and gated recurrent units to perform Machine Translation. 

Use python programming skills to develop software in ARL projects.

Both the Tensorflow and Theano toolkits are implemented in python. I used my python programming skills to apply the tools in those toolkits to develop the MT systems I worked with.

Support the team project to implement an Automatic Speech Recognition system
adapted to  speech as spoken in African countries. 

I made good progress on this objective. 
I dedicated a lot of hard work to preparing our inhouse speech corpora  for processing by ASR system development tools. 

I can now build subspace gaussian mixture model based ASR systems with the kaldi toolkit. 
The recipes I have developed to prepare and process the African accented speech can be applied to our other holdings.

Cooperation:

Serve as a bridge between the multilingual computing branch and the academic
community
at UMD. 

Communication:

Publish a journal paper as first author on research on simultaneous
translation.


Customer Relations:

Respond to requests from team customers for advice and technical support on
issues concerning machine translation and machine learning.

Technology Transition:

Transition simultaneous translation code developed for research into branch
projects.


Diversity:
Support ARL's diversity initiatives by participating in locally sponsored
training, outreach and/or special emphasis programs to increase personal
awareness and understanding of the various cultures that exist among
laboratory employees.


SHARP:
Support leadership's efforts to address and prevent sexual harassment and
sexual assault and ensure a respectful work environment for all.
Demonstrate support for the SHARP program by actively participating in
required training and other educational programs.  Intervene and
appropriately respond to any instances of sexual harassment or sexual
assault; encourage others to do the same.


** Top 6 accomplishments
Name: John Morgan

Office:

Team Leader (No):
Grade: 
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations – at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  No
Recognition (awards, letters of appreciation, etc.)

 List significant projects you are working on & your specific responsibilities:

Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.

Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.



6 Most significant Actions / Impacts
1. 
Action: 1. 
Impact: 
2. 
Action: 1. 
Impact: 
3. 
Action: 1. 
Impact: 
4. 
Action: 1. 
Impact: 
5. 
Action: 1. 
Impact: 
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY

-I'm running the alignment script that uses the first sgmms to align the data.
I'm also running the script to make the decoding graph for the first sgmms.
I decoded the test set with the sgmm5 models.
WER: 23.97

- SRI Gabon:
I plan to do the same for the sri_gabon data set that I did for the Answers.
At this point, if for no other reason than to got the steps of processing the data.   
I just realized that I have not been decoding the sri_gabon data as I've been taking the semi supervised stepsp.
I fired up a scrip to decode the sri_gabon data with monophones and triphones.
I don't really have to do this yet.
I only need to do it when I know which model set performed best.

As I'm getting ready to leave, the script that makes the denominator lattices is still running. I need them to do discriminative training. 
** Goals for Friday:
- TODO Write scripts to run semi supervised training with sri_gabon data in addition to the answers data.
- TODO Write accomplishments
- TODO Write top 6 accomplishments document.
- TODO Write more on tr.
- TODO Run scripts to train sgmms discriminatively with mmi.

* Goals for Wednesday set Tuesday:
- TODO Write more on accomplishments.
- TODO Run steps to build models from the Answers semi supervision .

The fst build for tri1 semi supervised had finished. I fired up decoding.
Tri1 semi supervised WER: 43.18
The previous fully supervised WER was 45.33. So we're still doing better.
I've moved on to training tri2 semi supervised models, which are the same as tri1 only that they use the tri1 alignments.

- TODO Investigate question: Is the new LM being used?
- TODO If unigram fst making continues, consider building a different kind
of dnn (nnet2, nnet3, chain models)

It stopped, but I don't think it finished successfully. Maybe it ran out of memory?

- Answers:
I am using the transcriptions from the best sgmm model system output as labels for training. I could also use them to get a better estimate of the WER after decoding.
I guess I don't want to do this because I don't want to overwrite the files I have there now that are working.
I've spent the day running the scripts to build the models 
. As I'm getting ready to leave the script that trains the first sgmm model is running.

This script takes a while.
I'll wait until tomorrow to run the script for the next step.
I've been running the scripts that make the decoding fst for the different models.

Here are the results I got today for the semi supervised yaounde + gp system:
model & WER
mono & 59.39
tri 1 & 43.18
tri2 & 39.05
tri3 & 43.78
tri4 & 41.49
tri5 & 32.62
tri5 ci & 49.67


The tri5 score of 32.62 is the best score I've gotten so far for 
triphone models without using sgmm or dnn techniques, so I'm still hopeful that I'll beat previous bests as I move to sgmms tomorrow.
however, the tri5 ci score of 49.67 is worrisome. It is quite a bit worse than its score of 44.02 for a previous run.
Why did this score come out so bad?
I'm also a little disappointed with the score I get for the answers  when I use the automatic transcripts as the reference.
tri3 & 88.64

I can't remember at which model I started using the automatic transcripts as refernces, but I'm pretty sure I did it for tri3.
I would have guessed this WER would be lower. I'm kind of testing on the training data. In fact, this is really disappointing.

- Dari
I got excited about working with Hazrat on automatically transcribing the data from the Afghan Military Academy. However, this data was already transcribed by transtac, so I'm standing down on that project.

** Goals for Thursday:
- TODO Write more accomplishments.
- TODO Run scripts to build semi supervised sgmm models.
* Goals for Tuesday set Monday: <2016-09-22 Thu>
- TODO Write accomplishments
- TODO Try to continue dnn training where we left off.

I started the mkgraph script yesterday evening. It is still running this morning. Top says it's running at 99% of a cpu.
Is something wrong?

- TODO Start over with Steve's new lm.
- TODO Write scripts to use the answers as semi supervision.

- Semi-supervision
I created a new directory data/train_semi_supervision
I concatenate and sort the 3 files spk2utt, utt2spk and wav.scp under the data/train directory with the same files under data/answers and write them to data/train_supervision.
I concatenate and sort the data/train/text file and the decoder output from the sgmm model run on the answers data and write this file to data/train_semi_supervision/text.

Then I run the plp pitch extractor script on the train_semi_suprevision directory.
I trained monophones using the data from the data/train_semi_supervision directory.
I'm going to try to decode the test set with this new mono_semi_supervision model set.
I'm going to use the same lm. 
I'll start using Steve's new lm soon.
I'm making the decoding graph (fst).
If the monophones don't do well, I don't think I'll continue with the supervision idea.
I decoded the test set with the new semi-supervised monophones.
WER: 59.39 versus 62.10 previously with only full supervision.
I think this means it's worth continuing down this road.
Aligning with semi-supervised monophones.
Naming the directories so that I don't hoze previous runs is getting harder.
I trained and formated the new lm.
Unfortunately, I overwrote the old lm and its fst.
I tried to avoid this, but I failed.
This is going to screw up my results if I don't go back to the old lm.
I'm not sure what to do at this point.
If I get better results, I might just go forward and redo all the steps with the new lm.
If the results are words, then I've got a problem.
I'll have to go back.

I've spent a couple of hours this afternoon writing the scripts to do the semi supervised build from monophones to sgmms.
This was mostly copying the supervised scripts and adding some words to directory names.
The mkgraph scripts take forever.

top reports numbers like 0.461t for virt and res and 82% for mem for the process that is making the fst graph for the dnn.
I am finished writing scripts to build the semi supervised sgmms.
I've written scripts to run decoding at each step.

I'd like to start the next step, but the scripts to run the graph making are still chugging away.

** Goals for Wednesday:
- TODO Write more on accomplishments.
- TODO Run steps to build models from the Answers semi supervision .
- TODO Investigate question: Is the new LM being used?
- TODO If unigram fst making continues, consider building a different kind of dnn (nnet2, nnet3, chain models).




* Monday, September 19, 2016 5:35 PM
To:	Larocca, Stephen A CIV USARMY RDECOM ARL (US); Vanni, Michelle T CIV 
USARMY RDECOM ARL (US); Hernandez, Luis CIV USARMY RDECOM ARL (US)
Subject:	Daily activities Report for Monday September 16 2016
Signed By:	john.j.morgan50.civ@mail.mil



** Goals for Monday set Friday:
- TODO Attempt to resume dnn training.
- TODO If that fails rebuild models?
- TODO Alternatively, transcribe answers with current sgmm models 
- TODO Do semisupervised training with answers (maybe sri_gabon too)
transcriptions.
- TODO Write accomplishments


When I left Friday evening, I tried to fire up monophone training after
rerunning the plp pitch extraction script. The monophone training script
gave me an error about feature dimension. The solution ended up being that I
had to delete the directories having to do with the training data. In the
monophone training I script I stole from the babel recipe, they split the
training data into sub corpora to do different kinds of training.
I think those sub corpora were linked to specific numbers associated with
the plp extraction. When I reran the plp extraction, those numbers got
changed. Right now, this is only a guess. But deleting all the directories
associated with plp and pitch extraction and training data subsetting seems
to have fixed the problem.

- Attempt to restart the dnn training.
The next step requires a unigram language model.
There are problems building this model and converting it into an fst.
The gp corpus still needs conditioning in order to get this to work.
I'm finding some problems with the gp corpus conditioning that might make me
want to restart the training from the beginning.
The s' was not separated from the rest of the word.
I thought this had already been done for the gp transcripts that come with
the corpus.
Apparently not.
This is bad. The tokenization is not consistent in the original corpus. Some
of the l' have been separated from the rest of the word. Bad Bad Bad.
I found 69 cases.
Do I have to do this by hand?
I wrote a script to reattach the apostrophes.
Problem coeur does not appear in the lexicon with the oe ligature spelling. 
Somehow the oe ligature spelling got into my transcripts.
Maybe when I converted from dos to unix and then to utf8?
I'm just going to stick with the oe spelling.
- Lexicon problems:
Since the d' are now separated, i'm finding words that start with d' that do
not have pronunciations in the lexicon.
I'm copying the d' entries to entries with the d' deleted and the initial dd
in the pronunciation removed.
Steve. I think I need your help here.
Now I'm getting problems with words that are lower cased.
I might be doing something wrong.

I'm finding problems in the lexicon.
The words pronounced au need to be fixed.
etat is in my transcriptions, but not in the lexicon.
I checked this.
One of the prompts is:
Jamais par le bras d'autrui, Grands Etat n'ont e"te" conquis.
Etat does not appear in the lexicon.
Is this a  mis spelling?
habituation was missing a space.

- roll back, revert, reset what ever it's called. I messed up the dictionary
and I wanted to get the version of the dictionary before I started changing
it.
I ran:
git checkout "REVISION_HASH~1" local/src/lexicon.txt
I wasn't sure this worked, so I got the old dictionary from the gp
directory.
I was still getting errors.
They were the errors that could be fixed with dos2unix.
So I'm back to a previous dictionary.

- I spent most of the day working on getting pronunciations for words in our
corpora.
Finally I got the fst to compile.
I hope I did not do anything stupid.
The script I'm running computes the cost of something ...
Then it creates FSTs.
It puts these under data/lang
Sorts the arcs.
Checks if it's stochastic.
Determinizes the FSTs.
.


I'm trying to restart the dnn training where I had left off.
I'm making a decoding graph.
I think this is the step that required the unigram fst.

** Goals for Tuesday:
- TODO Write accomplishments
- TODO Try to continue dnn training where we left off.
- TODO Start over with Steve's new lm.
- TODO Write scripts to use the answers as semi supervision.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 16, 2016 6:26 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 15 2016

** Goals for Friday:
- TODO Finish fixing problems with sri and gp data preparation.
- TODO Do top six accomplishments 
- TODO Try to recover dnn nnet training recipe 


I'm writing the scripts to prepare the sri_gabon data.
I have to avoid the conflict with the gp data.

OK it took me most of the day, but I think I finished data preparation  for
the sri_gabon corpus.
I still have to do the lm work and maybe dictionary work? Well ... I guess
I'm not finished yet.
I found more problems with the gp data prep scripts.
I'm training the lm with Steve's new selected corpus.
The data prep never ends.
I put the gp data in directories like gp_001.
Under each one of these directories the gp data has names like
fr001_001.wav.
kaldi does not like this.
It wants something like:
gp_001_FR001_001.wav
in the directory gp_001


I spent all day on data prep for sri_gabon and gp.

I had a problem getting the plp pitch extractioon to run.
It looks like I had to delete a previously created file.
cmvn

** Goals for Monday:
- TODO Attempt to resume dnn training.
- TODO If that fails rebuild models?
- TODO Alternatively, transcribe answers with current sgmm models 
- TODO Do semisupervised training with answers (maybe sri_gabon too)
transcriptions.
- TODO Write accomplishments

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 15, 2016 6:21 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 15 2016

** Goals for Thursday:
- TODO Write script to prepare sri_gabon data.
- DONE Assuming denominator lattice generation is finished, move on to
fdiscriminative training on the laptop so I can do the semisupervised
training with the answers.
- DONE If the dnn nnet pretraining finishes, move to dnn nnet cross entropy
training.
- TODO Incorporate Steve's sri_gabon prompt list into the lm.

The dnn nnet pretraining had finished, so I fired up the next step of cross
entropy training.
The denominator lattice generation for sgmm discriminative training has
finished, so I fired up the discriminative training script that uses mmi on
my laptop.

mmi training finished. I started the graph generation for decoding with the
mmi trained models on my laptop.

- sri_gabon data prep:
Steve gave me 3 files with potential prompts.
I've converted the 3 files to utf8.
I ran dos2unix to get rid of the cr.lf
I then ran iconv -f ISO_8859-1 -t UTF8 INFILE > OUTFILE
to put the file in utf8.


The script that runs the graph maker for the mmi trained sgmm decoding died.
It looks like it runs out of memory on my laptop.
I'm trying it again after rebooting.
It happened again:
The error message is:
std::bad_aloc

The dnn nnet training with cross entropy finished.
Decoding the test set:
WER: 29.22

Problem:
In trying to get the new sri_gabon data incorporated into my recipe I messed
up the old training data.
What should I do now.
I am going to stop the dnn nnet training until I finish preparing the
sri_gabon data.
The problem was with apostrophes.
Somehow, I was not conditioning completely the sri_gabon data.
Apostrophes were not being tokenized correctly.
So I was not able to make the unigram graph that is for some reason required
to do the next step in dnn nnet training.
When I was working on the sri_gabon scripts, I found some conflicts with the
gp corpus.
I made some modifications to the gp data preparation scripts and I did not
test them.
There were problems created.
I need to fix these problems.
To distinguish the gp from the sri_gabon data, I prepended a sri_gabon
prefix to directory names.
I haven't done this everywhere yet.
I really should do the same for the gp data.
Problem:
I am not deleting the data directory, because I was accidentally deleting
all my work.
There are some scripts that append lines to already existing files.
So these files are getting larger and larger each time I run the script.
I need to delete these files that get appended to.

** Goals for Friday:
- TODO Finish fixing problems with sri and gp data preparation.
- TODO Do top six accomplishments 
- TODO Try to recover dnn nnet training recipe 


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, September 14, 2016 6:31 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday September 14 2016


** Goals for Wednesday set Tuesday:
- TODO Figure out how babel does dnn systems differently. How do they do the
semi-supervision?
- TODO Read references.
- TODO Run script to  do semi-supervision with sgmm5 transcripts on the
laptop (let the gpu do the dnn build that I started today)
The dnn recipe run looks like it died. There seems to be no activity
registered since I left yesterday. Top reports that it is indeed running. I
don't understand what is going on. 
I think it could be an issue with the gpu.
The gpu is registering no activity.
Actually, I did find some activity in the log files.
Is it running on the cpu?
It turns out that it was running on the small gpu.
I deleted the process.
Justin reset the gpu.
I restarted the process.
It now reports to be running on the tesla gpu.

- sri_gabon corpus:
I should run our best system on the sri_gabon corpus.

I'm working through the scripts to prepare the sri_gabon data.
There are 5851 read speech files in the sri_gabon corpus.

There's a conflict between the gp and sri_gabon speaker directory names.
They both just number them.
I'll have to copy the files from /mnt/corpora  and rename them I do this for
gp already.
Steve will give me prompts for the sri_gabon data. They are not directly
associated with the file names. 
My plan is to randomly select sentences from Steve's list as transcriptions
for the sri_gabon data.
I really don't have to do this, but it might save time later.
I do want to include Steve's list in my language model.

This is going to take some dedicated work to get right.

The denominator lattice generation is still running on my laptop for the
sgmm discriminative training.
It should finish soo, but I need to leave.

** Goals for Thursday:
- TODO Write script to prepare sri_gabon data.
- TODO Assuming denominator lattice generation is finished, move on to
fdiscriminative training on the laptop so I can do the semisupervised
training with the answers.
- TODO If the dnn nnet pretraining finishes, move to dnn nnet cross entropy
training.
- TODO Incorporate Steve's sri_gabon prompt list into the lm.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, September 13, 2016 4:45 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday September 13 2016

** Goals for Tuesday set Monday:
- DONE Assuming denominator lattice generation finishes successfully,
continue with the mmi discriminative training of the yaounde + gp system.
- TODO Setup the scripts to use the automatic transcriptions of the Answers
data as training data.

- TODO Read reference papers for the babel system.


Yesterday before leaving and after I wrote my report I ran a decoding with
the yaounde + gp tri5 models.
The WER was 34.75.
I thought this was not good news. However, I looked again at the results
from my previous runs and the score for tri3b models was exactly the same
34.75.
The speaker independent version that alsow gets run at the same time gave a
WER of 4402 which is actually better than the previous run that gave 46.80.
I'm still anxious about the results for the sgmm5 models.
The sgmm5 WER came in at 27.72.
It looks like this is slightly worse than my previous results which were
27.47.
I'm not going to worry about such a small difference.
No. I'm actually slightly better at this point.
The previous WER was 28.38.
I'm still waiting for the results for decoding with fmllr.

I started working on a script to use the automatic answers transcriptions as
supervision for training with the answers data.

Discriminative training finished on the gpu machine.
When I run the mkgraph script it doesn't seem to work.

Maybe it was already done?
Yes. I think I already ran the script that makes the FSTs.

I need to write a script to decode the final sgmm5 models that were trained
discriminatively.
If I get good results, I'll be ready to move on.
The fmllr results are in, WER was 27.41, slightly better than before which
was 27.47.

OK, the decoding is done.
The results are slightly better.
The best wer is 25.85 which is down from the previous best of 25.98.

Time to move on.

I have to wait for the answers to get transcribed.

Change of plans:
I'm going to run the dnn recipe that I ran for the gp and yaounde builds.
I think it's the nnet recipe. I'm not sure if it's karel's recipe or Dan's
recipe. My best guess is that it's Karel's.
I'm running this because I had already run it for the other 2 builds and it
looks like I got improvements for those systems.
However, the babel recipes do things differently.

I'm going to run different recipes on the laptop and gpu machine.
On my laptop I'm going to run the semi-supervised training with the
transcripts produced by the sgmm5 system.
I won't have the sgmm5 complete on my laptop until some time tomorrow or
maybe even later.
Right now I'm generating denominator lattices on the laptop for the sgmm
discriminative training.


** Goals for Wednesday:
- TODO Figure out how babel does dnn systems differently. How do they do the
semi-supervision?
- TODO Read references.
- TODO Run script to  do semi-supervision with sgmm5 transcripts on the
laptop (let the gpu do the dnn build that I started today)


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, September 12, 2016 4:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday September 12 2016


** Goals for Monday set Friday:
- TODO Investigate how babel Cantonese builds deep neural network models
including bottle neck filters. How can I apply these methods to our yaounde
and other corpora?

I've decided to follow the babel naming conventions for my yaounde+gp build.
In babel (and other places) dnn recipes build off the tri5 models.
Babel calls it tri5, the previous recipie I was following (I think it was
rm) called it tri3b.
tri5 means there are 5 steps of triphone enhancements.. I could list them
...
tri1 builds off the monophone alignments.
tri2 adds delta features.
tri3 builds off the alignments from tri2.
tri4 uses lda and mllt
tri5 uses speaker adapted training with mllr.
In babel they don't even worrry about decoding test sets at all these lower
levels.
I wanted to read the paper on mllt, but I could not access it.
Apparently, mllt has to do with the covariance matrix. It's a way to use a
full covariance matrix instead of just a diagonal one.

After tri5 alignments are obtained recipes split off into sgmm training and
dnn hmm hybrid training.
 
- TODO Wrap up hmm builds for all 3 data set configurations.
I'm rerunning the yaounde+gp build through sgmm discriminative training
which is the only build yet to be completed.
I'm going to skip fmmi for now.

- TODO Ask Justin to put the babel Cantonese corpus under /mnt/corpora
I'm going to wait until the sgmm training I'm running now on the Cantonese
corpus to finish, otherwise I'm ready to move this corpus to /mnt/corpra.

- TODO Try using output transcripts for Answers as labels for training with
Answers.

- DONE symlink utils and steps directories.

The focus today is on finishing the pre dnn models for the yaounde+gp. This
means getting the sgmm models. Then training them discriminatively. I have
the basic sgmm models and the discriminative training is happening right
now. This is very slow, so it might not finish today.
I want to decode with the sgmm basic models.
When I look back at my previous recipe, I see that I used transforms from
the tri3b=tri5 models when decoding with the sgmm models.
I guess I have to do this again.
Yes, and to get those transforms I have to also run the mkgraph script for
the tri5 models.

I've been trying to  read the references in the  babel  summary, but the
journals make it way to hard to access their papers.

As I'm getting ready to leave I have a couple of processes running that I
should leave alone and come back tomorrow to check on.
I've got 1 process running on my laptop and several on the gpu machine.
On my laptop I'm running the basic training of the sgmm models.
On the gpu machine I have the script that runs the recipe for the Cantonese
babel recipe.
This is the first of many scripts.
It builds the tri5 and sgmm5 models, including the discriminatively trained
sgmm models.
Right now the script is making the denominator lattices for the
discriminative training.
I'm also running 2 processes for the yaounde + gp system.
I'm decoding with the sgmm5 models (not the discriminatively trained ones
yet).
Actually, I got mixed up. The denominator lattices are being generated for
the yaounde + gp system. The Cantonese system is doing the discriminative
training.

** Goals for Tuesday:
- TODO Assuming denominator lattice generation finishes successfully,
continue with the mmi discriminative training of the yaounde + gp system.
- TODO Setup the scripts to use the automatic transcriptions of the Answers
data as training data.
- TODO Read reference papers for the babel system.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 09, 2016 5:09 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 9 2016


** Goals for Friday set Thursday:
- DONE Copy the data from the Babel Cantonese dvd to the gpu machine ( I
can't believe how long this takes.)
- TODO Setup and run the Babel Cantonese recipe.
- TODO Finish the pre dnn part of the recipes for the 3 builds yaounde gp
and yaounde+gp.
The yaounde recipe run is still running. It is just getting to the fmmi
parts.

The gp recipe run had crashed this morning on the denominator lattices
creation for the sgmm2x_4a models.
There were some old graphs/FSTs hanging around. I deleted them and that
still did not fix the problem. There was an extra option setting the number
of jobs to split into. I removed this option and it seems to be running now.

The yaounde + gp build is still chugging away.


- TODO Read reference papers for kaldi and the techniques used in their
recipes.
- TODO Check on runs of yaounde dnn and yaounde+gp build from scratch.

- TODO Clarify the difference between karel's dnn recipe and the dnn hybrid
recipe that I've run on the gp and am running on the yaounde corpus build.



- Setting up babel Cantonese.
Modify the conf/lang -> lang.conf file for cantonese, this is going to
require several passes.
Lesson: The steps and utils directories are symlinked to the wsj versions of
these directories.
If babel can do this, I should do this too.
I  thought it was too good to be true when I was setting up my recipes.
From now on I'm going to symlink the steps and utils directories to our
installation of kaldi.
Specifically:
/home/tools/kaldi/egs/wsj/s5/{steps|utils}
This is going to save me a lot of space and effort copying scripts from thos
directories.
All the new scripts go under the local directory
- I was missing configuration files.
I went to the clisp cluster.
I found the place where the bable development was happening.
All the configuration files seem to be there and the paths coincide with the
ones in the configuration file.
I copied the config files to the gpu machine.
The first run script is running after some tweeks. 
I had to touch a dummy glm file.
The recipe seems to be using plp features with pitch instead of MFCCs.

Monophone training is happening now.

- yaounde:
The run is not finished yet as I'm getting ready to leave. However, it has
already done the decoding with sgmm 4a mmi b02 models. These were the best
models for the yaounde + gp data set build. They are not the best for the
yaounde data set build. They come in at 37.84. The sgmm with fmllr comes in
at 36.42, which is the best so far for yaounde.
The script still has to run the fmmi modeling. I don't think this will beat
the best, but who knows?

- gp:
The script I'm running for the gp data set configuration is doing lattice
generation. It will use these lattices to do discriminative training of sgmm
models.
Those are the sgmm mmi 4a b0 2 models that gave the best results on the
yaounde + gp data set configuration.
I'm actually running a dnn script on this data set configuration too.
I don't understand why I'm not seeing any activity on the gpu. I was seeing
a lot yesterday.

- yaounde + gp:
The script running on this data set configuration is at the stage where it
has built the tri3b models and generated lattices and discriminatively
trained with mmi and is now decoding.
After that it will move on to sgmm modeling.
So this won't finish until some time this week-end.


** Goals for Monday:
- TODO Investigate how babel Cantonese builds deep neural network models
including bottle neck filters. How can I apply these methods to our yaounde
and other corpora?
- TODO Wrap up hmm builds for all 3 data set configurations.
- TODO Ask Justin to put the babel Cantonese corpus under /mnt/corpora
- TODO Try using output transcripts for Answers as labels for training with
Answers.
- TODO symlink utils and steps directories.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 08, 2016 6:15 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday September 8 2016

** Goals for Thursday set Wednesday:
- DONE Check if the yaounde+gp run crashed.
It had crashed before I left. It was a chain models recipe. I think the
chain models recipe need a lot of work before they're ready for prime time. 
yBefore I left yesterday I started the gp build with Karel's dnn hybrid
recipe.
This recipe finished without errors
This morning I've started decoding the test set with the dnn hybrid models. 
The decoding of the test finished very quickly. 
wer 41.47. 
This  is the best so far for the gp builds.



- TODO Incorporate dnn recipes into my recipe.



I tried to incorporate Karel's dnn hmm hybrid models into my yaounde + gp
recipe. 
for some reason it started the whole recipe from the beginning. 
I'm not sure why this happened.
I'm starting from scratch with the yaounde + gp system.
There was a confusion. I'm running 2 different dnn builds. One is karel's
and the other is a dnn hybrid model.

I'm setting up the dnn recipe for the yaounde build.

I got the Cantonese babel corpus from Steve.
I'm putting it on the gpu machine.
Justin can put it on /mnt/corpora on Monday.
My plan is to run through the Cantonese babel recipe for dnn, bottle neck
features and if possible chain models.
My hope is that by observing a working example, I'll be able to replicate it
on my recipe for accented french.
It's taking for ever to retrieve the data from the Cantonese dvd.
OK, the copying finished. The second dvd did not take as long as the first.

I spent most of today getting the yaounde and gp builds to the point where I
can fill out the results table.
There are still sections I'd like to fill out.

** Goals for Friday:
- DONE Copy the data from the Babel Cantonese dvd to the gpu machine ( I
can't believe how long this takes.)
- TODO Setup and run the Babel Cantonese recipe.
- TODO Finish the pre dnn part of the recipes for the 3 builds yaounde gp
and yaounde+gp.
- TODO Read reference papers for kaldi and the techniques used in their
recipes.
- TODO Check on runs of yaounde dnn and yaounde+gp build from scratch.
- TODO Clarify the difference between karel's dnn recipe and the dnn hybrid
recipe that I've run on the gp and am running on the yaounde corpus build.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, September 07, 2016 5:31 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday September 7 2016



** Goals for Wednesday set Tuesday:
- DONE Check on processes on gpu.
They were all in different states of chrash.
I did get one good result from the yaounde + gp build. We broke the 26
percent WER barrier. We're at  25.98 with the sgmm2x_4a_mmi_b0.2 iteration
2.

- DONE  Get Justin to reboot gpu machine.
- DONE Run chain model recipes.
I started the chain model training run on the yaounde build. It is behaving
strangely. It seemed to be using the gpu at the beginning. Now it does not
seem to be using the gpu.
The script claims it fails then it continues. Something is not right.

- TODO Investigate bottle neck feature training.
- TODO Consider how to train on automatically transcribed Answers.
- TODO Figure out why the GP build is crashing on the Answers. This should
not be happening. It does not happen for the other 2 builds.

Justin is recompiling a new fresh version of kaldi for me. So I'm doing
clean up of directories and git repos.

The recompile is done.

I started the runs for the yaound+gp and gp builds.

I am going to stop running more than one build at a time.
I'm going to concentrate on the yaounde+gp for a while.
I killed the gp build.

The chain model yaounde+gp  build crashed

. There's a problem. Maybe it has to do with the parameters.
There are a lot of parameters and who knows what they should be set to.
The mini batch size for example.
I'm seeing problems with inverting matrices on the gpu. It  falls back to
processing on the cpu. This is strange.
This makes me think that there are problems with my parameter settings.

I started the dnn hybrid script. I think this is Karels recipe.

** Goals for tomorrow:
- TODO Check if the yaounde+gp run crashed.
- TODO Incorporate dnn recipes into my recipe.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, September 06, 2016 5:03 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday September 6 2016

** Goals for Tuesday set Friday:
- DONE What happened to the 3 runs?
I came in over the week-end to make tweeks and restart crashed runs.
I reordered the sgmm building before the nnet chain model building.
The fMMI recipe is not done yet.
- DONE Did tri3b decoding on Answers succeed for the 3 runs?
Yes. It looks like I can get transcripts for answers where ever I get
transcripts for test.
- TODO Did the runs reach the chain model step?
Only the yaounde build is getting there this morning.
- TODO Write on tr.
- TODO Get bottle neck model recipe from babel.
This will take some more work.


** Goal for today
- TODO Make the recipe scripts consistent for the 3 data sets.

The yaounde build is ahead of the other 2 builds because it is much smaller.
I've added the FMMI dubm parts to the yaounde recipe. I need to add these to
the 2 other scripts.

The chain part of the yaounde build is now at the point where it depends on
having the gpu available.

Something is wrong with the gp build. It keeps crashing on answers.


I've been looking into the automatic lexicon expansion described in babel.
The babel data came with syllable boundary marks. 
I found references to syllable taggers. One called EasyAlign uses praat and
htk.

I cannot log on to the gpu machine.
I guess it's bogged down with processes.

** Goals for Wednesday:
- TODO Check on processes on gpu.
- TODO Get Justin to reboot gpu machine.
- TODO Run chain model recipes.
- TODO Investigate bottle neck feature training.
- TODO Consider how to train on automatically transcribed Answers.
- TODO Figure out why the GP build is crashing on the Answers. This should
not be happening. It does not happen for the other 2 builds.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 02, 2016 4:22 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 2 2016

** Goalls for Friday set Thursday:
- TODO Check on the 3 recipe runs that I started this afternoon.
Specifically, did they crash on the answers? 

It looks like the answers are not crashing the run.sh recipe script now. The
WER is infinity.

Are there dummy reference transcriptions that are missing. I would be
surprised if there were none.

I don't know how to check this yet.

This was only working for the gp build this morning.

- TODO Did tri3b fmllr decoding work for the Answers? (important)

Bad news. The gpu machine is really bogged down with all the processes I'm
running. There were 96 processes running when I checked this morning. This
makes everythink slow.

The yaounde+gp is training the tri3b models this morning.

- TODO Mandatory Training
- TODO Write more on TR.
- TODO Finish the pass on the yaounde recipe run.sh script.
- TODO Read papers on kaldi.


I'm making a pass on the run.sh recipe script for the yaounde build.
The chain model  training command line has 28 options, including many
options for the jesus layer.
There are a lot of decodings happening in the sgmm model builds in the
recipe. 
I'm trying to do both the test and answers decoding for each kind of
decoding.
I'm not sure this is worth all the work.
I'd rather spend time on bottle neck feature (filter?) models.

- Good news:
it looks like the problem with decoding the Answers with tri3b models is
solved.
I think I had to go through the process of doing exactly what I had done for
the dev and test sets.
That was for the yaounde build.
The yaounde + gp is not ready yet.
Actually, it was the gp build that had the answers and questions correct.
I now updated the yaounde and yaounde+gp builds to include the answers
fixes.
I had made a modification to the script that puts the prompts in one file. I
assumed the numbers in the file names were separated by a dash. The names
for the answers files are separated by underscores.
That is fixed now.
This makes me feel  a lot better about letting these scripts run over the
week-end.
Weird: I guess it wasn't the gp build that had the answers fixed. Anyway, I
updated it too. Maybe I only had it running on my laptop?
As I'm getting ready to leave for the long week-end, I have all 3  builds
running from a fresh start.
I rewrote the order of the commands in the gp script. I do the sgmm before
the chain models. This makes more sense.
I'll rewrite the other scripts next week to put them in this order.
Unfortunately, they'll crash when they get to the chain models before they
get to the sgmm models, but I don't have the time today to fix this.



The output from the decoding should be labeled with the fold. Right now the
gp build on the tri3b mmi step labels the output of the test fold only as
decode. I really want this to be labeled decode_test. The answers fold
labeled as decode_answers and the dev fold labeled as decode_dev.
This is important because the output  later gets used for mllr transforms or
something like that.
I'll have to have all 3 scripts consistent on this point. 
116 processes are running on the gpu machine.

** Goals for Tuesday:
- TODO What happened to the 3 runs?
- TODO Did tri3b decoding on Answers succeed for the 3 runs?
- TODO Did the runs reach the chain model step?
- TODO Write on tr.
- TODO Get bottle neck model recipe from babel.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 01, 2016 4:20 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday September 1 2016

** Goals for tomorrow Thursday set Wednesday:
- TODO Figure out why tri3b decoding is not working for answers but is
working for tri3b dev and test.
- TODO  Check on chain models for yaounde+gp.


I checked the chain model build for the yaounde + gp data set.It died at the
first iteration of neural network training. The gpu was not detected.
This will be a show stopper until the machine gets rebooted.
In the meantime, I'll have to work on the other 2 scripts to get them to
this point. I'll also work on the issue of transcribing the Answers data
with the tri3b models.

I was surprised to find out that the chain models are dnn hmm hybrid models.

I'm going back to the data prep step to get the answers aligned with the
test and dev sets.
Instead of using transcriptions for each file as in the case for dev and
test, I'm going to use the questions that was asked to get the answer that
was spoken.

Decoding without mmlr works for answers.

I'm starting from the beginning with the gp data set build.
I'm hoping that having the (dummy) transcripts for the Answers will enable
fmllr decoding (tri3b) for the Answers.


I've done another pass on the gp and younde+gp run.sh recipe scripts.
I've started on the yaounde recipe script.
I've incorporated the questions to the answers in the scripts. The decode
scripts do not crash now when I decode the answers. The WER results are not
valid since the reference transcriptions wers are only dummy sentences (the
questions not the answers).

** Goalls for Friday:
- TODO Check on the 3 recipe runs that I started this afternoon.
Specifically, did they crash on the answers? Are there dummy reference
transcriptions that are missing. I would be surprised if there were none.
- TODO Did tri3b fmllr decoding work for the Answers? (important)
- TODO Mandatory Training
- TODO Write more on TR.
- TODO Finish the pass on the yaounde recipe run.sh script.
- TODO Read papers on kaldi.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 31, 2016 5:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 31 2016

** Goals for Wednesday set Tuesday:
- TODO Check how far my recipe scripts reached. As I'm preparing to leave
the gp and yaound+gp data set builds are doing the cleanup run. The Yaounde
data set build is training tri2b models.

I've spent the morning concentrating on the yaounde+gp run. I got some
really good news this morning. We broke the 30 WER barrier. The bad news is
that I haven't been able to apply these models to the Answers data. The test
results are worthless if I cannot apply the models to the Answers data.


- TODO write more of the tr.

The answers are not being transcribed with the tri3b models.
The test set gets transcribed. It seems to be using fmmlr transforms.
Somehow I have to get mllr transforms for the answers data.
What is the cleanup?
I guess it is a way to get feedback on the quality of the recordings?

I'm pushing forward on the yaounde + gp data set build.
I'm leaving the issue of decoding the answers for later. It'll take a lot of
concentration.

I'm working on the chain part of the recipe now.

python scripts are getting used now.
 There is a lot of information on chain models at the url below:

http://kaldi-asr.org/doc/chain.html



** Goals for tomorrow Thursday:
- TODO Figure out why tri3b decoding is not working for answers but is
working for tri3b dev and test.
- TODO  Check on chain models for yaounde+gp.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 30, 2016 5:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 30 2016



** Goals for Tuesday set Monday:
- TODO Check on status of run.sh recipe scripts ( they will probably have
failed).

I checked the yaounde + gp run. One thing I noticed is that the ansers
decoding fails, I knew this, but it has become an issue because it makes the
whole run.sh script fail when I have a || exit 1 phrase at the end of the
command.
I noticed that it fails because it does not find the data/answers/text file.
I need to incorporate a dummy file here so that the command does not fail.
My work around now is to remove the || exit 1 phrase.

Actually, now that I look at this closer, I think this is an important 
 file to have. The questions that were asked to the speakers. For each
answer, we should have the question.
I have a list of the scenarios and questions, but I do not have a list of
the question that was asked for each specific answer.

I had not included the options parsing script in the yaounde run.sh script.
This made the build start from the beginning each time I ran it even if I
specified the --stage  option. The --stage option does not get parsed unless
the options parsing script is invoked in the run.sh script.

- TODO Incorporate sgmm training into recipes.
- Todo Write more for tr
- TODO Do another mandatory training

- Questions to Answers:
I wrote a list of 300 questions for the Answers. I'm not sure they align
with the answers.

I'm incorporating the chains recipe into my recipe for the gp data set. I'll
do this for the other 2 recipes later.
There are references to jesus in this recipe.
I have no idea what this is about.

I'm incorporating a script to find bad utterances into my run.sh recipe
scripts.

Here's what I've written so far for the tr:
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
John Morgan
August 30, 2016
Abstract
1 Introduction
Speech to speech (S2S) devices convert speech input by a speaker in one
language
into speech in a different language. The automatic speech recognition (ASR)
system is a key component of a speech to speech device. ASR systemss for S2S
devices are ideally trained on speech that is similar to the task for which
the
device will be used. S2S devices are intended to be used to enable dialogues
between speakers of different languages. Collecting this ideal kind of
dialogue
data is expensive. In order for the data to be used as training data for an
ASR
system it must be transcribed at the word-level. This transcription task is
a
major part of the reason why the data collection is expensive. A way to cut
back
on this cost is to obtain an automatically generated rough draft of the
dialogue
type of speech collected. If the data being collected comes from a language
that
lacks a corpus of speech data or if the data comes from a highly accented
flavor
of a well-resourced language, automatic transcriptions of the data will not
be
possible. One way to solve this problem is to collect a small corpus of
recitations
by each speaker in the data collection. We will refer to this as the read
part of
the corpus. The other part will be refered to as the conversational part. An
ASR system built with the small read corpus will not serve as a training set
for an S2S device. However, it can be used to obtain rough draft
transcriptions
of the conversational speech part. one reason this is possible is because
the
speakers in the read part are the same speakers that are in the
conversational
part. For scientific evaluation, for any ASR task, the speakers in the test
set and
training set are kept disjoint. The taskk becomes much easier when the
speakers
in the training and test sets are the same. The cost of building an ASR
system
with read speech is much lower than building one with conversational speech.
A pronouncing dictionary is the most expensive component of a phone-based
ASR system. For a system built with read speech there is no cost involved
with
transcribing the data. The transcriptions are given by the prompts.
1

2 Methods
Three corpora were used in this project.
The Yaounde corpus: collected in Yaounde ,the capital city of Cameroon. It
consists of two
parts: the read part which consists of recitations fof prompts and the
conversational part which consists of answers to questions.
nch part of the Globalphone corpus:
The Central Accord Corpus: Collected in Gabon from speakers from four
Central African countries. A
small part of the read part of this corpus was used as test data.
All the experiments performed in this project used the kaldi toolkit. Most
of the standard kaldi recipes were used. Three training sets were compared.
Yaounde Consisting of only the prompts from the Yaounde corpus.
GP Consisting of the Globalphone prompts.
Yaounde + GP Consisting of both the Yaounde and Globalphone prompts.
3 Results
model Yaounde GP Yaounde+ GP
monophone 64.03 69.06 62.10
tri1 59.57 56.07 45.33
tri2a 60.83 55.04 44.99
tri2b 64.57 57.26 43.46
tri3b 47.17 34.75
tri3b si 61.74 56.85 46.80
tri 4 48.58
tri4 si 63.31
Table 1: WER scores for models and training sets.
2


** Goals for Tomorrow Wednesday:
- TODO Check how far my recipe scripts reached. As I'm preparing to leave
the gp and yaound+gp data set builds are doing the cleanup run. The Yaounde
data set build is training tri2b models.
- TODO write more of the tr.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 29, 2016 5:27 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 29 2016

There was a problem with the network on Friday August 26 2016.
The network was not available for the entire day.
This meant I could not log on to my enterprise machine (JAWS license?).
The network seems to be working fine today.


** Goals for tomorrow Friday August 26 2016 set Thursday:
- TODO Check if processes have finished.
- TODO Do another sanity check.
- TODO Write more for tr
- TODO Investigate triphone training steps (maybe Dan Povey's paper?)


I looked closer at the kaldi recipes and I decided to take yet another step
back.
Everything is good up through tri1 model training and decoding.
Then there is a branch off to tri2a models.
These models use another step of deltas on the features.
Apparently this is a dead end. 

Model building continues on another branch.
The first step on this branch builds tri2b models.
These models use lda and mllt features.

I had confused myself by ignoring the difference between tri2a and tri2b. I
had renamed tri2a tri2 and tri2b  to tri3. This led to confusion. I thought
one built on the other.

I wrote a run.sh script that includes the tri2a step for the record, but
continues building models on the tri2b branch.
The kaldi recipes put all the steps in a run.sh script.
I was putting the steps in separate files, but now I've moved to writing
everything in the run.sh script.

I wrote run.sh recipe scripts for each of the 3 data sets.
I launched them on the gpu machine.

** Goals for tomorrow:
- TODO Check on status of run.sh recipe scripts ( they will probably have
failed).
- TODO Incorporate sgmm training into recipes.
- Todo Write more for tr
- TODO Do another mandatory training

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 25, 2016 6:00 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 25 2016



** Goals for tomorrow Thursday set Wednesday:
- DONE Check that the yaounde graph making script finished.
- DONE Check that the scripts for the other 2 data sets finished. Some of
the decoding seems to be stuck.

Everything seems to have finished, but I think I've gotten out of sync.
- TODO Do a sanity check on the scripts. They seem to have become
unsynchronized.
- TODO Continue the Constitution training on page 37.
- TODO Start writing the TR



model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri3 & 63.85 & 47.17 & 45.15


- tri3 results: 
Why did the gp models do so much better. Am I really comparing the same kind
of models?

- Sanity checking:
It looks like I'm taking diffeerent steps for the different models.
1. Monophones look consistent for the 3 training data sets.
2. Tri1 ditto
3. tri2: this may be where the steps go along different paths.
Yes.
Yaounde: 
input alignments: tri1_ali
output directory: tri2a
step script: steps/train_deltas.sh

gp:
input alignments: tri1_ali
output directory: tri2
step script: steps/train_lda_mllt.sh

yaounde + gp:
input alignments: tri1_ali
output directory: tri2a
step script: steps/train_deltas.sh


I think the extra step I'm taking for the yaounde and yaounde+gp training
data sets is a waste of time.
I'll go back and skip it and follow the steps I'm taking for the gp training
data set.
The yaounde+gp training step takes noticeably longer than the yaounde
training set script.



- tri2:
I'm going to take a step or 2 back.
The tri1 results are great.
The tri2 results get worse.
I'm going to try 1 more time to fix this.
The alignment step after the tri1 models have been trained and used to
decode can take an option to use graphs.
I'm going to try this.



model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri2 use graph alignment & 64.57 & 57.26 & 43.93
tri3 & 63.85 & 47.17 & 45.15
tri3 using graph alignment & 64.91 & 56.85 & 44.65

When I added the --use-graph option for alignment using tri1 models
, the decoding using tri2 models had mixed results. It was worse for
yaounde, the same for gp, and slightly better for yaounde+gp.

I'm not sure what is going on with the tri3 models.



- Mandatory Training:
I finished reading all 137 pages of the 508 version of the Constitution Day
training.

** Goals for tomorrow Friday August 26 2016
- TODO Check if processes have finished.
- TODO Do another sanity check.
- TODO Write more for tr
- TODO Investigate triphone training steps (maybe Dan Povey's paper?)

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 24, 2016 5:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 24 2016

** Goals for Tomorrow Wednesday set yesterday:
- DONE Check if the fst graph maker scripts finished successfully.
The 3 of them finished successfully.
- TODO Decode test and answers with resulting FSTs.
- TODO Get Steve  to eyeball the yaounde ansers output.
- TODO Run through speech to text tutorial for PySpeech.

- Triphone decoding:
I found a paper on the internet by Dan Povey that says that in kaldi they do
not use hand written questions for the decision tree clustering of
triphones. They use a data driven approach.

model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri3 & 63.85 & & 

The tri2 models produce worse results. 
 I think they're only there to get alignments, but I'm not really sure.
The tri3 models don't look good either.
I think these triphone models are a waste of time.


These results continue to look more  reasonable however.
More data is better and adding relevant data is better.


-PySpeech tutorial:
I run the train_sid.sh script to get speaker id models as suggested in the
decode.sh script.
I think pyspeech assumes audio input is telephony quality, i.e. 8bit8k.
It says this in the tutorial.
It supports a couple of file formats, but all are telephony quality.
Anyway ... I gues we can always downsample.
Unfortunately, I don't think PySpeech is what we're looking for if we want
to do demos.
I hope I'm wrong.

- Constitution Day mandatory training:
There are 134 pages to read in hthis training.
This will take me several days to complete.
I'm at page 37 of 134.

** Goals for tomorrow Thursday:
- TODO Check that the yaounde graph making script finished.
- TODO Check that the scripts for the other 2 data sets finished. Some of
the decoding seems to be stuck.
- TODO Do a sanity check on the scripts. They seem to have become
unsynchronized.
- TODO Continue the Constitution training on page 37.
- TODO Start writing the TR

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 23, 2016 5:12 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 23 2016

** Goals for Tuesday: 
- DONE Check if lm finished training
The file with the training data ended up with  13.3 million segments and 143
million tokens.
The lm is only 149 mb. This seems small.
- TODO run monophone systems with new lm.
- TODO Investigate tri2 models and why they do not improve the WER scores.


- Monophone builds:
Yaounde: The steps that involve the new lm take much longer.
This is the drawback to using a large lm. 
GP and Yaounde + GP: I'm skipping a step that creates a lexicon for the
dictionary. I'm not sure this step is needed.




model & yaounde & gp & yaounde + gp
mono with small lm including CA test prompts & 39.23 & 43.96 & 38.59
mono with large lm no CA test prompts & 65.42 & 69.06 & 62.47



These numbers look really bad, but the transcriptions of the yaounde answers
obtained with these models are looking pretty good.

- PySpeech:
Justin set me up with an account on a sentos machine where he has installed
PySpeech.
I was able to run the tutorial script for extracting ivectors.

- Triphone builds:
I ran alignment and training today for the 3 training data sets.
As I'm getting ready to leave, I now have the triphone graph making script
running for the 3 data sets.

** Goals for Tomorrow Wednesday:
- TODO Check if the fst graph maker scripts finished successfully.
- TODO Decode test and answers with resulting FSTs.
- TODO Get Steve  to eyeball the yaounde ansers output.
- TODO Run through speech to text tutorial for PySpeech.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 22, 2016 5:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 22 2016

** Goals for Monday set on Friday:
- TODO Compare the wer for triphones
- TODO Build a better lm.
- TODO Test the new lm with mono system.

The processes I had started on Friday terminated and they (as far as I can
tell so far) terminated with no errors. So I started the next round of graph
making, alignment and training scripts for triphones.


model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59
tri1 & 44.03 & 38.84 & 28.04

This is a bit strange.
We moved from monophone to triphone models. There are 3 training sets:
1. Yaounde: WER went from 39.23 to 44.03
2. Global phone (possibly used Yaounde as dev set): Wer went from 43.96 to
38.84
3. Globalphone and Yaounde: Wer went from 38.59 way down to 28.04.

Why does the WER go up for The Yaounde-only trained models?
Otherwise these results are looking a lot more reasonable.
When we add relevant data, the WER goes down (and pretty substantially).

I did not take my own advice to be patiaent and I continued to the next set
of triphones:

model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59
tri1 & 44.03 & 38.84 & 28.04
tri2 & 47.22 & 41.80 & 29.97

The tri2 models perform worse for the 3 training sets.

I'm testing a guess that the problem is in the alignment step.
The tri1 models are used to align the data before training the tri2 models.
The script points to the directory data/lang_nosp_test_threegram.
I'm trying to point to the directory data/lang_nosp instead.
The alignment script finished without errors.
The training script for the tri2 models also points to the
data/lang_nosp_test_threegram.
The make graph script also points to the data/lang_nosp_test_threegram, but
this is probably correct. We need to make a graph for the test data.
I got the exact same WER score 47.22 when I swapped the
data/lang_nosp_test_threegram and data/lang_nosp directories.



- lm
I'm spending some time on the subs corpus for the lm.
I retrieved the files from the dvd Steve gave me.
Looking at the French English corpus.
I'm not  sure what the .idx file is.
There are 2 text files. I suppose these are the 2 sides of the parallel
corpus.
I'm repeating the steps Steve took.
Lowercase, Normalize, tokenize, and restrict to sentences with between 6 and
25 tokens.


- Mandatory Training:
I completed the AMC records management basic training.
** Goals for tomorrow:
- TODO Check if lm finished training
- TODO run monophone systems with new lm.
- TODO Investigate tri2 models and why they do not improve the WER scores.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 19, 2016 6:36 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday August 19 2016

** Goals from Yesterday:
- DONE Check if processes have finished  for building the gp yaounde ca
models .
This is running the sgmm2 training which takes a long time, but it's worth
it.
step 26.
At the end of the day I've abandoned this for now. I'll get to it later.
- DONE Check if the yaounde only model processes have finished.
steps 16 and 17.
I was decoding with yaounde only triphones. The scores get a lot worse.
Something is wrong.
Ditto, I've abandoned this for now and I'll get to it later.

- DONE lm 
check if the lm processing steps have finished.
This might be creating a training file that is too big.
Currently I'm running it on the gigafren corpus.

There was a bug in the script. I'm restarting this. I'll probably have to
come back later to use better data.
- DONE Answers:
Extract the text output from the log files to see how well the monophones
performed.
I wrote a script that does this.


- Planning:
The long term goal is to get transcriptions for the Yaounde Answers (YA).
Outline:
1. Produce an ASR system with our existing holdings: gp, yaounde read
(centrral accord?)
I guess we'll use ca for testing for now.
2. Decode the Answers with the system in 1.
3. Evaluate this method. Can this work for Central Accord?

As part of 1. Figure out the best system for decoding the Answers.
Concentrate first on monophones. Be patient.
Step 1: Compare the performance of 3 systems:
1. Yaounde  Read (YR)
2. GlobalPhone (Gp)
3. Yaounde Read + GlobalPhone (YRGP)


Note that there are questions about what devset data to use.
I think I'll ignore this question for  now while building the 3 mono
systems.


What about the LM?


- Monophones:
1. Train on YR test on CA. and decode YA
2. Train on GP test on CA and decode YA.
3. Train on YRGP test on CA and decode YA.

model & yaounde YA & gp YA & gp YR & yaounde + gp YA
mono & 39.23 & 43.96 & 30.60& 38.59

In the first row yaound YA means train on Yaounde and test on YA.
So the column labled gp YR  means trained on gp and tested on yaounde read.
Again we see that gp (30.60) is better than Yaounde (39.23). But this is
because the prompts for the test set (YR) are in the lm.
Forget it, I think the gp yr column is confusing. I have it there because
later on I'm going to use the YR as dev data.


model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59

So the Yaounde + GP system is the best.
I think this is better than what I was getting before. These results seem
more reasonable.
When we add the data from the 2 corpora, we get slightly better results.

I put the results of decoding the answers with the mono system in a tsv file
under:
/home/data/scratch/answers_decode_output_y_gp_y+gp.tsv


** Goals for Monday:
- TODO Compare the wer for triphones
- TODO Build a better lm.
- TODO Test the new lm with mono system.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 18, 2016 5:09 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 18 2016

- Alignment:
The script finished yesterday evening despite the power outage.

- Decoding:
Decoding of the test set finished.
WER: 72.53 exactly the same as before. This is good news. We are getting
consistent results.
It looks like decoding of the dev set did not survive.
I wonder if the power outage shutdown the gpu machine?
It's up and running now.
- ansers
I wwrote a script that makes 3 files for the ansers data:
wav.scp
spk2utt
utt2spk
For mfcc extraction, I think only the wav.scp is required.
I run the mfcc extraction script having it point to the ddirectory that
contains the above 3 files.
This extraction script succeeds.
So now I have the mfccs for the ansers data, can I decode it?
I'm trying to decode with the monophone system.
Hey! it looks like it is decoding.
I see text coming out of the log files.
So ...
This really should be done with the yaounde data alone.
This is going to be my major project for the next couple of weeks.
Plan?
What models should I use?
What LM should I use?
Are the decoding results, i.e. the text output, available in an easier
format than the log files?
This is putting our money where our mouth is.
Can we build a system that can decode data we have that is not transcribed.
If this works, this could be a method for data collection.
Collect read speech (maybe around 10 hours).
Collect free range answers to questions or scenarios.
Build a system with the read speech.
Use that system to transcribe the answers.
How well does this work?


- Yaounde only models
I'm going through the steps for building the yaounde only models.
I should probably work on the lm first.

- UN corpus
Should I use the UN corpus of French?
I am writing scripts to process this corpus to be used as data for an lm.

- Mono decoding of the Answers:
This finished.
Of course the scoring failed since there are no reference transcriptions.
I have not found a file containing a clean version of the output text.
There are log files that contain the output text.
I might have to use these files.


- Tomorrow:
Check if processes have finished  for building the gp yaounde ca models .
This is running the sgmm2 training which takes a long time, but it's worth
it.
step 26.
Check if the yaounde only model processes have finished.
steps 16 and 17.

- lm 
check if the lm processing steps have finished.
This might be creating a training file that is too big.
Currently I'm running it on the gigafren corpus.
- Answers:
Extract the text output from the log files to see how well the monophones
performed.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 17, 2016 4:47 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 17 2016

- Decoding with the 5e ensemble script:
The script is still running.
It is at pass 143.
I think it wil continue until pass 240, so it'll take another day.

- pySpeech.
Justin tried to install the rpm packages. 
We need to request the ubuntu or debian packages instead.

- Stupidity
I killed the 5e ensemble training process that had been running for 2 days.

I did this because I was rewriting the data setup script.
I was rewriting the data setup script because I had time on my hands waiting
for the 5e ensemble training script to finish :(

Anyway ... I'm happy with the data setup rewriting.
I separated the scripts by corpus.
This is the style followed by the kaldi scripts.
The scripts are under the local directory.
The main data setup script calls  data setup scripts for each corpus:
central accord, yaounde, and gp.

- Decoding Mono:
started the monophone decoding.

- Answers
I made a little progress on getting the yaounde answers data into a format
for processing.

- Tomorrow:
Continue rebuilding the ASR models.
Did decoding finish correctly?
What about alignment?
Continue processing the yaounde answers data.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 16, 2016 6:04 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 16 2016

- Results from the decoding run on the dev set for the nnet2 5a p-norm
script:




Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93
nnet2 p-norm & 50.55 & 59.65
net2 5c & 51.33 & 57.23


Notice that the sgmm2 scores are much better than the tri3 scores
The speaker adaptation kicks in at some point, but I think it's at the next
step.

- Training with the 5c script:
This training finished.

- Training with the 5e (ensemble) :
This script is still running.

- Decoding the 5c system:
See results above

The results are mixed. 


- Diving into nnet3
There's a script called run_ivector_common.sh.
The first thing it does is to perturb the data.
The mfcc data are perturbed.
Then it tries to do alignment.
Alignment is failing again, just as it did before.
I finally found the log file that shows where a problem is happening.
There are errors in the make_mfcc log files.
It says the wav files are much smaller than what is indicated by the header.
The number of bytes that are actually read looks correct.
The number in the header looks wrong.

- Tomorrow:
Check if the training run for the ensemble system finished.

Fix the problem with the mfcc feature extraction. this should fix the
alignment problem too.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 15, 2016 4:50 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 15 2016

- Monday:
Did the run_5b_gpu.sh script finish successfully?
No.
Alignment is still failing.

What now?
I think the problem might be that there is something specific to the corpus
that the script was intended for.


-Chain
This is the latest recipe being developed.
- tdnn
time delayed neural network

I'm trying a script from another recipe, the librispeech recipe.
It's named run_5a_clean_100.sh
It seems to be working.
At least it's doing neural network training.
It uses a script called:
steps/nnet2/train_pnorm_fast.sh
I think it must be using the alignments from the tri3 run.
The gpu is registering 99% usage.
Looking at some files under the exp directory:
lda_dim
360
ivector_dim
0
Does this mean I'm not using ivectors?
feat_dim
40

It looks like this nnet2 script only requires the tri3 alignment step which
I made in step 24.

Training finished.
- Decoding:



Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93
nnet2 p-norm & 50.55 &


I started training with a script called 5c.
I guess it uses the tanh activation functions.

I'm also looking at 5e, ensemble training.

I'm running the training scripts for both the ensemble and 5c methods
together.
I'm surprised this works. They don't make each other crash.

I just remembered that I also have the decoding script running on the dev
set data.

- Tomorrow:
Check how the 2 scripts finished.
They are  numbered 56 and 58.
Try to finish up with nnet2 and move on to nnet3.
Don't forget about decoding the yaounde answers data.
Also check the results of the decoding of the dev set. It is running the
script numbered 55.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 12, 2016 4:13 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday August 12 2016

- Yesterday I thought I had another step to go for the nnet build. I was
wrong. The only thing missing was the results of the decoding on the dev
set:


Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93


The best score remains an sgmm with mmi.

- nnet2
nnet was written by Karel Vesely.
nnet2 was written by Dan Povey.

Dan first does something with the input features.
Several copies are made and they are perturbed.
VTLN warping and time warping.
vtln is vocal tract length normalization.
He says this is a way to artificially expand the amount of data.
There  is one command to perturb fbank features and one to perturb mfcc
features.
- Align
Dan's script refers to tri4b, I don't have that , so I'm using tri3.
tri4b seems specific to wsj.
Alignment is failing.
Why?
The training graph compiling is finishing successfully.

As I'm getting ready to leave, I am still stuck on the alignment step.
I'm going back and trying to run a script directly (more directly) from the
kaldi wsj recipe.
The wsj script refers to a specific fold of the corpus.
I replace that reference with the training fold in our corpus.
The script I am running now is:
local/nnet2/run_5b_gpu.sh
I think the 5 refers to the fact that 5 copies of the data are perturbed.
This script performs the feature extraction, laignment, training and
decoding.
I was separating out each of these steps into their own scripts.
Now I'm trying to run them all from one script.

- Monday:
Did the run_5b_gpu.sh script finish successfully?
If not, figure out what the problem is?
Otherwise continue with the other scripts in the local/nnet2 directory.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 11, 2016 5:43 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 11 2016

- Deep Belief Network pretraining
The script finished and apparently it finished successfully.
There was a problem getting this script to run.
The tail command probably had nothing to do with the problem.
( tail --pid=$$ -F $LOG_FILE_NAME 2> /dev/null)
This creates and opens the file in $LOG_FILE_NAME.
On the next line we had a command: like
$cuda_cmd $LOG_FILE_NAME SCRIPT_FILE_NAME arguments
The 2 variables in front of the name of the script file to be run are
variables that get assigned. 
The problem is that I had commented out the line where the cuda_cmd variable
gets assigned in my cmd.sh file.
The lesson to learn is that when you run a bash command, you can assign
environment variables on the commandline just before the name of the script
file.
 - Cross Entropy training:
Training neural network

The training stopped after 13 iterations.

- Decoding with nnet models:




Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33


The dbn pretrained dnn models are a little disappointing.
I did not do the make fst graph step.
Is it done somewhere in the scripts?
is it not required?
The next step is to align.
Then the denominator latttices are made.
This involves FSTs.

- smbr training
What is smbr.
I think it's sequential minimum bayes risk.
Train using minimum phone error (mpe)
Run 6 passes of this training.
Then make priors.
Is this smbr?
- Decode :
Decoding seems to be faster with these models.

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 

As I'm preparing to leave I am still waiting for the decoding of the dev
data to finish.
I also started the alignment using the current smbr models and I started the
denominator lattice generator. 
The denominator lattice generation indeed creates a decoding graph.

- Tomorrow:
Check if the decoding, denominator lattices generation and alignment have
finished successfully.
If so, continue with training and then decoding.
this would be the last step for the nnet2 system build.
Start the nnet3 build.




- An overview of nnet in kaldi is at:

http://kaldi-asr.org/doc/dnn.html

It looks like I want to go to nnet3.
Timit is not the recipe for nnet3.

- What is the output of the neural network?
This is an interesting question to keep in mind.
The neural network performs classification.
I think the input is a speech frame, i.e. an mfcc vector (maybe?)
It looks like the input vectors are enhanced with I vectors (maybe?).
Anyway ...
The output  of the neural network  is an assignment to a class.
What are the classes in the output?
There is a class corresponding to each context dependent state in the ASR
system.
The system is a network of states?



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 10, 2016 5:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 10 2016

- The script that makes denominator lattices for mmi training of sgm models
finished successfully. It looks like I just had to use fewer subjobs. I set
the number of subjobs to 1. Does this have something to do with
hyperthreading?
- Fired up the script that does mmi training of the sgmm2 models.
- Decoding 
There are 4 iterations of decoding in the timit recipe.
I guess lattices are rescored after each decoding.

iteration & test & dev & WER
1 & 50.05 & 58.57
2 & 50.70 & 58.45
3 & 50.55 & 58.51
4 & 50.89 & 59.18

There was a  "job failed" error in the dev decoding.
WER does not necessarily get better after rescoring.
I think these methods are geared towards very large systems.


Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18



There's one more method in the timit recipe that combines the  dnn and sgmm
models. 
The scripts for this method depend on specific characteristics of the timit
corpus.
I'm skipping it for now.

- Karel's nnet build:
My first attempt failed.
I'm splitting his script into steps.
I'm stuck on the dbm pretrain step.
dbn stands for deep belief network.
A dbn is a stack of restricted boltzmann machines.

I'm talking to Justin about  getting this script to run.

Hi Justin,
The script I'm struggling with is at:
/home/tools/kaldi/egs/timit/s5/local/nnet/run_dnn.sh
The part I'm having trouble understanding and the place where it is failing
is around line 53.
The command on lines 56 and 57 apparently has 2 arguments in front of the
filename for the script that gets run.
The script steps/nnet/pretrain_dbm.sh gets invoked.

Anyway, I'm struggling with this script under:
/home/john/yaounde/kaldi-trunk/egs/gp_train_yaounde_dev_central_accord_test/
44pretrain_dbm.sh

When I invoke the command separately (not as a script) on the commandline it
seems to be working.
Actually, it runs in a script as long as I don't preceed the command with
the tail line and the 2 variable assignment arguments .

The dbn pretrain script command is still running and using the gpu as I'm
getting ready to leave.
	
I've written the scripts for the next couple of steps that follow.

- Tomorrow:
Check if the dbn pretrain script finished successfully.
If so, continue with step 45 to train nnet with cross entropy optimization.


- Longer term goal:
Use one of these systems (the best) to get a rough transcription of the
yaounde answers.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 09, 2016 5:40 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 9 2016

The mkgraph command finished successfully.
- Started decoding with sgmm2
- Started aligning with sgmm2



Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60

- mmi training
What is mmi?
mmi is maximum mutual information.
Apparently it is an alternative to maximum likelihood 
Discriminative models?
Minimize error instead of maximize probability.
I was having trouble with  the make_denlats_sgmm2.sh script.
I set the number of sub jobs to 1 and it seems to be working now.
the script makes denominator lattices for mmi training with sgmm2 models.
minimum phone error is an alternative to mmi.




- DNN hybrid system
I'm trying to figure out the recipes for building nnet, nnet2 and nnet3
systems.
I'm not even sure about the terminology to use here.
I think nnet, nnet2 and nnet3 are types of models in kaldi, analogous to
mono, tri1 and sgmm2.
The recipes refer to wsj, timit, voxforge etc, roughly to the corpora.
So, I'm trying to make a recipe for our Central Africa corpus using nnet
models.
I'm trying to copy the nnet model build from the timit recipe.
The script credits Karel Vesely.
A comment says that you can use his script local/run_dnn.sh
I should do this at some point, but right now I'm running a different code
chunk from the timit recipe.
There's a section called dnn hybrid.
 I ran a script that does neural network training.
Now I'm running another script that does decoding.
It looks like the script I'm running now builds  nnet2 models.
When I've finished this script, I want to go back to karels script, which
seems to build the nnet models.

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06

It does not look like the dnn hybrid beat the sgmm2 for the test set, but it
is best on the dev set.


As I'm getting ready to leave I'm still waiting for the denominator lattices
script to finish.

- Tomorrow:
Check if the make_denlats_sgm2.sh script finished successfully.
If so, move on to  my step 32 in the train_mmi_sgm2.sh script.
Read papers on  ASR, mmi, dnn hybrid model etc 


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 08, 2016 5:22 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 8 2016

- Decoding dev set with tri2 models succeeded:
Wer: 70.12

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12

I'm not sure what to think of these results.

- Next Step: Align
Align what?
The timit recipe has a comment that says:
Align tri2 system with train data
Yes, we want to align the training data, I get that.
Align the tri2 system?
Yes, that makes sense too.
There is a slight difference.
We use data/lang_nosp instead of data/lang_nosp_test_threegram
these directories contain the FSTs for the dictionary, phone lists, etc.
We use the more general FSTs instead of the test versions.
Somehow this is going to enable speaker adaptation.



- SAT training:
Next we do speaker adaptive training.
The script says that it estimates fmllr transforms.
I guess those utterance to speaker files and speaker to utterance files get
used here.
I don't think I was doing speaker adaptation before, although the only
difference seems to be the data/lang_nosp directory.

- Decoding with tri3 SAT after the alignment

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56

- Remember to return to this step to do dnn.

- Train ubm and sgm2

- Run Karel's dnn training:
There's a script that computes the duration of all the wav files in the gp
corpus.
The mean duration is 9.2.
The min is 0.5 and the max is 23.


- I'm waiting for the sgmm2 training to finish
- Attempted some mandatory training: hopeless
I'll have to get help.

- The sgm2 training finished, it took a while.
Now the mkgraph command is taking forever.

- Tomorrow:
Check if the mkgraph script finished successfully.
If so, decode using the sgm2 models in steps 28 and 29 and run alignment
with the sgmm2 models. in step 30
I wish I could have started these scripts today.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 05, 2016 3:53 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily activities Report for Friday August 5 2016

Decoding with the monophones did not successfully complete.
Actually, I'm not sure if the decoding succeeded, but the computation of the
WER failed.
The error message indicates that a hypothesis file is not found.
The hypothesis file is cameroon_m_001_001.
When I look at the log file for the decoding, I see a text string was
produced.
So I think decoding succeeded.
The problem is with the scoring.
OK, I found another missing script in a best_path log file.
It was a script that maps the integers to symbols.
I got the script and fired up the decoder again.

- Decoding succeeded for monophones trained only on gp and tested on CA:
WER: 72.53
- Note that the yaounde data was held out for dev data.
The lm was the same as before, namely: subs sample, yaounde scenarios and
questions, yaounde prompts, and gp training prompts.

Model & WER
mono & 72.53
tri1 & 65.38

I trained the tri2 models. In this case the tri2 models are trained with lda
and mllt.
I'm now decoding the dev (yaounde) data with the tri2 models.
My understanding is that this produces graphs (I think they're actually
lattices).
These graphs are then used in the next step to align the data again in
preparation for the tri3 models.
So the graphs depend on the dev (yaounde) data and hence the tri3 models are
speaker dependent?
The problem right now is that the dev data takes a long time to decode.
So I probably won't get to the next step until Monday.

Model & WER
mono & 72.53
tri1 & 65.38
tri2 & 65.81



I don't understand why the WER goes up.
What is the difference between tri1 and tri2?
Tri2 models use lda and mllt. It can cluster triphones with a decision tree.
I don't really have a set of questions right now, so I don't think much is
going on with tri2.

- Monday:
Check the results of decoding the tri2 models on the dev set.
If decoding succeeded, move on to step 19 to align the data with the tri2
models and the graphs produced by the decoding in step 18.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 04, 2016 6:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 4 2016

The monophone training I started yesterday evening finished successfully.
- Continue building the gp yaounde ca system:
I moved the steps around a bit:
00 setup data prep
01 prepare dictionary
02  extract mfcc features
03  compute cmvn stats
04 train monophones
05 prepare and train lm
06 make fst decoding graph for monophone system
07 decode dev set with monophones
08 decode test set with monophones
09  start building the tri1 system by aligning the data with the monophone
models
10 Train the triphone models
11  make the fst graph for the triphone tri1 system 
12 Decode the dev set with the  tri1 models
13 Decode the test set with the tri1 models.

Up to this point I mostly copied the scripts from the previous gp system
build.
I found something strange in the next step.
14 Start building the tri2 system by aligning the data with the tri1 models.
In the gp system I had the script pointing to the test dictionary.
At least this is what I understand right now.
The timit recipe has the script point to the general dictionary.
15 Train the tri2 models 

- Problems:
The gp transcripts only need to be lowercased. I was normalizing and
tokenizing them.
I found the problem.
I was using a script from the  yaounde build to remove punctuation.
I should have been using the one from the yaounde+gp build.

The monophone models were failing to decode the test set.
I don't know why yet.
The error mesage says that the hypothesis for  a given file was not found.

I'll start over.

as I'm leaving today, I'm back at the monophone decoding step and the first
step in building the triphone models by aligning the data with the monophone
models.

- Tomorrow:
Did decoding with the monophones succeed?
If not, focus on this problem.
Otherwise continue building triphone models.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 03, 2016 5:40 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 3 2016

Result for Model trained only on Yaounde Read prompts and lm trained using
subs:
model & gp & gp + yaounde & yaounde
mono & 71.54 & 72.80 & 74.62
tri1 & 60.38 & 61.08 & 68.47
tri2a & 60.62 & 62.11 & 67.74
tri2b & 61.98 & 62.39 & 67.53
tri3b si & 60.77 & 62.33 & 68.04
tri 3b & 51.03 & 52.87 & 55.16
sgmm2 & 47.19 & 49.14 & 44.90


- The best performing system was the yaounde trained sgmm2
That number is a surprise. It is out of place in the table.

- nnet
I want to incorporate nnet into our recipe.
I'll start by incorporating it into the gp there than in the yaounde and
yaounde + gp systems.system since I have the steps labeled better 
Up till now I was following the recipe for iban.
To incorporate nnet, I have to look at other recipes.
The recipe for timit looks promissing.
Where do I insert the steps to build a nnet system?
From the timit recipe it looks like this is done after the alignment step
for the sgmm step but before the train ubm step is taken.
In my gp system step 21 uses the tri2b models for alignment.
Step 22 trains the tri3b model with sat.
Timit gets confusing here.
Are they doing nnet or nnet2?
I'll follow the timit recipe in any case for now.
I'm trying to get this to run by using the minimum number of scripts.
so I copy the command from the timit recipe.
I run the command.
I get a "missing script" error.
I copy the missing script into directory and run the command again.
the appropriate directory in my gp 
I've got the neural network training running.
It looks like the gpu is getting used.
I'm afraid this is going to take a long time to train ...
The nnet stuff seems to come later in the timit recipe.

The nnet code in the timit recipe refers to the dev data.
It is time to go back and restore a dev set.
I had ignored the dev set in trying to get a kaldi system up and running.

- Train on GP, Develop on Yaounde, and test on Central Accord.
Writing all the scripts to use gp as training data, yaounde as dev and ca as
test.
Done with step 0 the data prep setup 
done with step 1 dictionary prep
- step 2: lm data prep
I'm using the same script as before.
data:
subs sample, yaounde read prompts, yaounde scenarios and questions, gp
training prompts.
arpa 3gram
- Step 3: Extract mfcc features from dev, test and train data.
- step 4: compute cmvn stats.
- Step 5: train monophones.
I'm stuck here.
I've run into the same problem before.
It has to do with a program called feat-to-dim.
Some utils scripts are missing in my new directory.
They don't get traced when I use nohup.
I'm running the steps/train_mono.sh script without options.
It first initializes the monophone system.
Then it compiles the training graphs.
Then it aligns the data equally.
This is all working when I run the script without options.
The problem must be with one of the options.
The number of jobs is an option. I have it set to 12.
The default is 4.
So this is definitely one problem.
The number of jobs is limited.
I'm rerunning with 10 jobs. It seems to be working.

I'll stop here for today.
I'm going to delay asking Justin to reboot until I get to the tnnet
programs.

I feel like I made good progress today.
I got the nnet programs up and running. I thought that would be harder.
I'm going to incorporate the dev set into my recipes.


- Tomorrow:
Resume at step 5: monophone training.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 02, 2016 6:04 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 2 2016

- Building gp system baselines with sample subs lm:


model & gp & gp + yaounde
mono & 71.54 & 72.80
tri1 & 60.38 & 61.08
tri2a & 60.62 & 62.11
tri2b & 61.98 & 62.39
tri3b si & 60.77 & 62.33
tri 3b & 51.03 & 52.87
sgmm2 & 47.19 & 49.14

- Note that these results are very strange.
When we remove data that is similar to the test data the WER goes down.


- What about yaounde and the subs lm?
model & gp & gp + yaounde & yaounde
mono & 71.54 & 72.80 & 74.62
tri1 & 60.38 & 61.08 & 68.47
tri2a & 60.62 & 62.11 & 67.74
tri2b & 61.98 & 62.39 & 67.53
tri3b si & 60.77 & 62.33 & 68.04
tri 3b & 51.03 & 52.87 & 55.16
sgmm2 & 47.19 & 49.14 & 

Again today I'm leaving with one more result left to filll in the table.
The results in the table above are pretty consistent.
The system trained only on gp does best. The system trained on both gp and
yaounde does slightly worse. The system trained only on Yaounde is the
worst.

- Tomorrow:
Finish this part of the project  by getting the sgmm2 results.
Then move on to nnet.
This might take a while to get setup.
Try not to get distracted. It's important to get the nnet, nnet2 and nnet3
models into our recipe.
The sgmm2 training is currently running.
Pick up tomorrow with the next step that makes the fst graph and then do
decoding.
The lm remains an issue, but I'm going to stick with the lm I have now until
we get training data we're happy with.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 01, 2016 5:39 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily Activities Report for Friday July 29 2016

- Finish current experiment that uses an LM trained on the small sample subs
corpus an no test prompts.
I had started tri2a training.
Resume at step 15 by making fst graph for decoding with tri2a.

model & WER
mono & 72.80
tri1 & 61.08
tri2a & 62.11
tri2b & 62.39
tri3b si & 62.33
tri 3b & 52.87
sgmm2 & 49.14

- Run same baselines for gp with same (subs) lm

model & WER
mono & 71.54

This does not look right.
We added data, that is similar to the test data  and the WER went up from
71.54 to 72.80?

- Tomorrow:
- TODO Resume gp baselines at step 10 training tri1 models.
- Longer term
- TODO incorporate nnet into recipe


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 29, 2016 4:33 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Friday July 29 2016

This morning I fixed the data normalization problems I was having yesterday.
For each corpus I run the moses tokenizer tools:
lowercase
normalize
tokenize
deescape special characters

The deescaper has to be run because the tokenizer replaces special
characters with their xml tags. For example the apostrophe gets replaced
with &apos;. This is  a problem since I want to delete punctuation like the
semicolon.
I am running an experiment by including 4 sources of text in the lm.
1. The training prompts,
2. The scenarios and questions from the Yaounde data collection,
3. The words from the CA test prompts,
4. The small sample of the subs corpus.

- Decode using Monophones and CA word list:
WER: 72.52

- Decode same as above without CA word list:
WER: 72.80

- Decode with CA word list and no subs :
WER: 82.17

- Decode with Subs and CA test prompts and CA word list:
WER: 41.65

- decode with subs and ca test prompts:
WER: 42.06

- Decode with CA test prompts:
WER: 33.63

This is interesting.
The list of words contributes very little.
Including the test prompts in the lm (oracle) makes a huge difference.
The sample subs lm got us about 1/5 of the way to the oracle.

- Decode CA test set with tri1a models and subs lm no cheating:
WER: 61.08

** Goals for Monday morning:
- TODO Finish current experiment  with subs lm step 14 tri2a
- TODO Write script to prepare better lm
- TODO investigate recipes that use nnet

--- 
Thursday July 28 2016



Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 12.40

- Decoded sgmm2 models trained on gp+younde tested on ca includeing ca in
lm:
WER: 12.40

** First Goal for Today:
- TODO Repeat model building without including ca in lm training.
It's not just a matter of training the lm without ca and testing all the
models.
Why?
Because the models are built incrementally.
The tri1 models use the alignments from the mono models.
But ...
The alignments don't use the lm.
So ... I was wrong, it is in fact just a matter of training the lm without
ca and testing all the models.
The lm gets used only for decoding.

I trained an lm without the ca prompts -- call this the no cheating lm.
Now I'm decoding the ca test set with each model using the no cheating lm.
This is really easy to do in kaldi.
No ... I forgot a step.
The fst graph has to be built before decoding.
Before building the fst graph an fst has to be built for the lm.
The fst for the lm has to be run only once and each model can use it.
The fst graph that incorporates all the components has to be run before each
decoding run.
I forgot to run the lm fst building step so I was getting the same results
as before and I restarted from scratch.
In the future if I want to experiment with different LMs, I only need to run
the lm data prep, lm fst once, then the make fst graph for each model.
I'm actually trying to do this in the gp directory.
In order not to hoze the preexisting  work, I need to go back to the step
where the FSTs for the dictionary and lexicon are made.
This step also only needs to be done once.


- First result:
Monophone models trained on GP+Younde and tested on CA.
Lm without CA test prompts : 83.28
Recall that when tested with an lm that included the CA prompts the WER was:
32.50

-  Results for GP:
Without CA prompts in LM: 41.31
With CA prompts 33.46

So something doesn't seem right.
I did not start the GP build from scratch, so maybe there's contamination.


I did indeed make a mistake.
Instead of getting the data for the lm and excluding the CA prompts, I
concatenated the new data to the old data.
I reran the same setup an this time excluding the CA prompts and got the
results:
Monophone models trained on GP and tested  on CA: 
excluding the CA prompts from the LM: WER: 85.15
including the CA prompts: WER: 33.46
So, we have a problem. 

_ Data Prep
Basic text normalization needs attention.
I got bogged down all afternoon with this.
But it has to be done before moving on.
Problem:
Apostrophes.
There are several characters that are used for the apostrophe.
We want only one!
There are moses scripts that normalize and tokenize text.
I think they do a pretty good job.
Problem:
They convert the apostrophe into an xml escape character: &apos;
This is a problem since I want to remove semicolons (probably ampersands
too).
Also I don't think s&apos; is in our dictionary.
Solution:
Moses also has a script to deescape the xml tags for special characters.
This script maps &apos; to '.
That's where I'm at as I'm getting ready to leave.
I'm surprised I was getting good results despite this apostrophe problem.
I was removing all punctuation.
So s'il was mapped to s il
Now s'ill gets mapped to s' il .
I believe this is the correct way to work.

** Goal for Tomorrow:
- TODO Finish data conditioning
- TODO experiment with medium size LM (possibly from subs)
- TODO Focus on getting a good monophone  model set  with training and test
data disjoint.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 28, 2016 6:08 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 28 2016


Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 12.40

- Decoded sgmm2 models trained on gp+younde tested on ca includeing ca in
lm:
WER: 12.40

** First Goal for Today:
- TODO Repeat model building without including ca in lm training.
It's not just a matter of training the lm without ca and testing all the
models.
Why?
Because the models are built incrementally.
The tri1 models use the alignments from the mono models.
But ...
The alignments don't use the lm.
So ... I was wrong, it is in fact just a matter of training the lm without
ca and testing all the models.
The lm gets used only for decoding.

I trained an lm without the ca prompts -- call this the no cheating lm.
Now I'm decoding the ca test set with each model using the no cheating lm.
This is really easy to do in kaldi.
No ... I forgot a step.
The fst graph has to be built before decoding.
Before building the fst graph an fst has to be built for the lm.
The fst for the lm has to be run only once and each model can use it.
The fst graph that incorporates all the components has to be run before each
decoding run.
I forgot to run the lm fst building step so I was getting the same results
as before and I restarted from scratch.
In the future if I want to experiment with different LMs, I only need to run
the lm data prep, lm fst once, then the make fst graph for each model.
I'm actually trying to do this in the gp directory.
In order not to hoze the preexisting  work, I need to go back to the step
where the FSTs for the dictionary and lexicon are made.
This step also only needs to be done once.


- First result:
Monophone models trained on GP+Younde and tested on CA.
Lm without CA test prompts : 83.28
Recall that when tested with an lm that included the CA prompts the WER was:
32.50

-  Results for GP:
Without CA prompts in LM: 41.31
With CA prompts 33.46

So something doesn't seem right.
I did not start the GP build from scratch, so maybe there's contamination.


I did indeed make a mistake.
Instead of getting the data for the lm and excluding the CA prompts, I
concatenated the new data to the old data.
I reran the same setup an this time excluding the CA prompts and got the
results:
Monophone models trained on GP and tested  on CA: 
excluding the CA prompts from the LM: WER: 85.15
including the CA prompts: WER: 33.46
So, we have a problem. 

_ Data Prep
Basic text normalization needs attention.
I got bogged down all afternoon with this.
But it has to be done before moving on.
Problem:
Apostrophes.
There are several characters that are used for the apostrophe.
We want only one!
There are moses scripts that normalize and tokenize text.
I think they do a pretty good job.
Problem:
They convert the apostrophe into an xml escape character: &apos;
This is a problem since I want to remove semicolons (probably ampersands
too).
Also I don't think s&apos; is in our dictionary.
Solution:
Moses also has a script to deescape the xml tags for special characters.
This script maps &apos; to '.
That's where I'm at as I'm getting ready to leave.
I'm surprised I was getting good results despite this apostrophe problem.
I was removing all punctuation.
So s'il was mapped to s il
Now s'ill gets mapped to s' il .
I believe this is the correct way to work.

** Goal for Tomorrow:
- TODO Finish data conditioning
- TODO experiment with medium size LM (possibly from subs)
- TODO Focus on getting a good monophone  model set  with training and test
data disjoint.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 27, 2016 5:54 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Wednesday July 27 2016

- Results for GP training on Yaounde corpus as test:
WER: 87.04
The Yaounde prompts were not included in the lm. Apparently this makes a big
difference.

** Goals for Today:
- TODO Finish Gp system testing on CA
- TODO Finish GP+Yaounde system testing on CA


- Decode GP tri1 on CA:
WER: 29.54

Model & WER
Mono & 33.46
Tri1 & 29.54

- Problem at steps to build tri2a models.
It looks like  I skipped a number in the steps.
Step 14 uses the tri1 models to align.
Step 15 should train the tri2a models.
Step 16 should make the fst graph that includes the trained models from step
15.
So I need to swap my scripts for steps 15 and 16. I had them in the wrong
order.

- Decoded using gp tri2a models on CA data:
WER 29.18

Model & WER
Mono & 33.46
Tri1 & 29.54
Tri2a & 29.18

- decode gp tri2b models on CA:
WER: 31.71

- Decode gp tri3b models on CA:
WER: 26.10

Model & WER
Mono & 33.46
Tri1 & 29.54
Tri2a & 29.18
Tri2b & 31.71
Tri3b & 26.10
Tri3b si & 34.82
Sgmm2 & 21.67


- Build gp+yaounde system
-Decode  with monophones  on CA
WER: 32.50

The corresponding WER for the GP alone system was 33.46

- decode with gp+yaounde tri1 on CA
WER: 22.45

Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 

- Suspicious:
The score for gp+yaounde tri3b and gp sgmm2 are the same: 21.67

I won't have the final sgmm2 score for gp+yaounde until tomorrow,but the
gp+yaounde results look like what we would expect.

- I'm finished with testing the mono, tri1, tri2a, tri2b, tri3b, tri3b si,
and sgmm2 models on the central accord test set after training on global
phone and including the central accord prompts in the lm.
I feel pretty good about the results posted above.

There's only 1 result left to compute for the same models as above but
trained on gp and younde.
- Yaounde Answers:
I spent a lot of time this afternoon trying to rename the files.
It looks like I finally succeeded.
I still have one step to go in renaming the Answers.
For the read data I made the speaker numbers unique.
Right now the speakers from machine ctell 2 start over at 1.


Michelle got me the questions for the Yaounde corpus.
- Plan:
- Incorporate these questions into the lm and decode the Yaounde answers.
Hopefully, this will help transcribing the data.

** Tomorrow:
- TODO Continue gp+yaounde at step 28
- TODO gp and gp+younde without ca prompts in lm
- TODO Finish renaming yaounde Ansers
- TODO Incorporate Scotia's transcripts into lm

** Longer term goals:
- TODO Decode Yaounde Answers (with or without younde read in training?)
- TODO Incorporate nnet, nnet2, and nnet3 into recipes
- TODO Build appropriate lm



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 26, 2016 6:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Tuesday July 26 2016

- Directory renaiming
The structure only has 2 levels now.
COUNTRY_GENDER_SPEAKERNUMBER/COUNTRY_GENDER_SPEAKER_NUMBER_UTTERENCENUMBER.w
av
For example:
Cameroon_m_001/Cameroon_m_001_001.wav
This might cause trouble down the road, since I think the files need to be
ordered by speaker number.

There's a problem with the script that writes the test utt2spk file.
Well ... there was a problem, it seems to be working now.
The utt2spk file has 515 lines, one line per utterance.
Each line has 2 fields: the utterance and the speaker.
-  utt2wav script
This points from each utterance to its wav file.
- Finished preparation of transcriptions
- LM prep
Make an LM with the training prompts?
Should we include the test prompts?
I'm including them for now.
- lang prep
- mfcc extraction
It looks like mfcc extraction succeeded for the new test data
- Ditto for cmvn stats
- mono training and lm2fst
- make fst graph
- decode mono:
WER: 41.61

This is a system trained on 85 speakers from the Yaounde corpus and tested
on 12 speakers from the Gabon corpus.
The lm included the Gabon test prompts.

- align with mono
- make fst graph for tri1
- decode with tri1
WER: 46.02
Up from 41.61 to 46.02
Why?
- align with tri1
- train tri2a
- make tri2a fst graph
- decode with tri2a
WER: 46.98
Even worse? Up from 46.02 to 46.98

- align with tri2a
- train tri2b (lda + mllt)
- make fst tri2b graph
- decode with tri2b
WER: 50.18
What! 

- Align with tri2b
- train tri3b (sat)
- make fst tri3b graph
- decode with tri3b
WER: 31.99
Finally some improvement!

- decode  tri3b si
WER: 48.04
- align with tri3b fmllr
- train ubm (Gaussian mixture models)
- train sgmm2 (subspace gmm)
This seems to take longer.
There's a lot going on in this  sgmm training script.
Realignment seems to be taking a long time.
Tre clustering, model initialization, Gaussian selection, graph compiling,
alignment conversion, 5 training passes, data realignment ..., more training
passes, more realignment, , more training passes ... , more realignment,
alignment model lbuilding, 
- make sgmm2 fst graph
- Decode with sgmm2
WER: 23.84

Summary of WER scores:
Model & WER
Mono & 41.61
Tri1 & 46.02
Tri2a & 46.98
Tri2b & 50.18
Tri3b & 31.99
Tri3b si & 48.04
Sgmm2 & 23.84

- Moving on to GP
I need to incorporate the central accord data as test data instead of the gp
folds.
I'll  use all the gp data for training.
The renaming script I had written is not working.
I found the problem.
The gp names end up out of order.
I have a script that renames them.
I have to point to the place where the new files with good names get stored.
I don't think this is taken care of in the kaldi recipe for gp.

- interesting lm problem
By mistake I made a training set that was the concatenation of the same data
3 times.
This training set makes srilm fail to produce a 3gram lm.
Srilm succeeds when I use only 1 copy of the data.
- gp data prep
- Decode with gp training and ca test, lm includes ca prompts:
WER: 33.46

- decode gp traing on yaounde corpus  as test data:
This will take a while since there is a lot of yaounde data.
I'll get the results tomorrow.

Tomorrow:
- Continue building gp system
I'm running step 11 which trains the tri1 models.
I'll also get the results for running the gp models on the yaounde corpus.
- Continue building the gp+yaounde system
I'm running step 5 that trains the monophones.






- Similarly fixed spk2utt script.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, July 25, 2016 6:12 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

** Goals for today:
- TODO Finish the last step for the GP system.
- TODO Incorporate the Gabon test set into the gp system

** Longer term goals
- TODO Incorporate kaldi's nnet, nnet2, and nnet3 programs into our recipe.

My laptop disk has no more space on it.
I removed some repeated corpora that I had under /home/data.

I have no permissions on /home/data/scratch/CA_test_set

- Ran step 29:the script to make the decoding graph 
- running decoding with sgmm2_5b2
- Decoding GP with sgmm2_5b2 finished:
WER: 24.32

WER Summary of GP system:

Models & WER
Mono & 41.80
Tri1 & 29.08
Tri2a & 29.01
Tri2b & 27.97
Tri3b.si & 29.66
Tri3b & 26.38
Sgmm2_5b2 & 24.32

- processing the central Africa data
- renamed directories.
The pattern is:
COUNTRY/GENDER/SPEAKERNUMBER/xxx.wav

For example:
Cameroon/m/001/001.wav

- Write data setup scripts to handle central Africa test set

- Got bogged down on file names.
There was a file without a leading 0 in the utterance number.
Cameroon/m/001/24.wav


- Maybe a missing wav file for a .txt file?

I renamed gabon/m/004/0{38-50}.wav
and gabon/m/007/u30.wav


- This scheme is not working.
I can only have 2 levels in the hierarchy.
Renaming directories again.
I'm not sure what the best way to do this is.
Some scripts depend on a correct or specific kind of ordering.
I'm not sure I  have the right ordering now.

Tomorrow:
Resume data prep work .
Yaounde directory
00data_setup.sh script
Local/get_utt2spk_test.sh


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 22, 2016 6:42 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

- I'm back !
Training tri2b finished successfully.
Step 20 that makes the graph ran without any logging output. This seems a
little suspicious, but the next step, decoding with tri2b seems to run ok.
For some reason this decoding step had the number of jobs set 2 only 2
instead of 10 as in previous steps.
I think this is a mistake, it's taking longer.
- decoding with tri2b (lda mllt)
WER: 27.97
Improvement from 29.01 to 27.97
- aligning  with tri2b and graphs.
Super fast
- train sat tri3b
- make graph with tri3b
This writes lattices first to a directory called decode_test.si
The scoring using these lattices yields:
WER: 29.66
I wonder if the SI stands for speaker independent?
Now it has gone on to write lattices to the standard decode_test directory
It looks like transforms are also written to this directory.
WER: 26.38
Improvement from 27.97 to 26.38
- aligning using tri3b
- training ubm
- training sgmm2

I'm not going to finish  the last decoding step today.
- Monday: resume with steps 29 make graph 30 decoding.

- Change of plans for next week:
Start over using Gabon as test set.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 22, 2016 3:00 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

- Summary of Yaounde WERs from yesterday:
Model & WER
Mono & 27.71
Tri1 & 24.74
Tri2a & 24.92
Tri2b (lda mllt) & 24.32
Tri3b (sat) & 24.37
Sgmm2_5b2 & 1412

-  GP system today
- Ran data prep, mfc extraction, lm and dict prep.
- The script that creates the fst graph takes a long time. 
Ok, I know what is wron.
I have it pointing to the big lm I created yesterday.
I'm going to back off this and use the small lm.
- I started over since I had some old directories hanging around that  might
have gotten used.
I've been writing separate scripts to run each step and naming the file with
an initial step number so it is easy to build the system by running each
step in sequence.
The kaldi style is to put all the steps in a file called run.sh.
So I'm appending each of my scripts to a file called run.sh.
- Decoded using monophones:
GP:
WER: 41.80
Yaound: 
WER: 88.39

The Yaounde prompts are not in the GP lm, that why this last WER is so bad.

- Aligned
- trained triphones with delta+delta-delta features
- Made fst graph
- Decoded with triphones
WER: 29.08
Big drop here from 41 to 29
- Aligned
- train tri2a
Runs 34 passes
No. I made a mistake here. I ran the training before making the graph. I
don't understand how the training ran without having made the graph
previously. I guess training does not require the graph?
- Now run make graph
- run decoding  with tri2a system
WER: 29.01
Slight improvement from 29.08 to 29.01
- aligning with tri2a
- training tri2b

I'm leaving now to fix my bike with Phil David.
If we finish early, I'll return. Otherwise, next week pick up on step 20 to
make the tri2b graph.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 21, 2016 5:35 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 21 2016

- Working on yaounde system with small lm
- Decoded tri1
WER: 24.74
- Align with tri1 system

There's an incremental building process.
Train models, 
use models to align the data
train models with realignments

So we trained tri1 models, then we used them to get alignments, next we
train tri2a models.

More specifically:
Run steps/align_si.sh 
Run steps/train_deltas.sh 
Run  utils/mkgraph.sh
Run steps/decode.sh

The last mkgraph step ran very quickly. Too quickly?


Stephen Tratz suggested I ask Justin to turn off hyperthreading to speed up
processing. Maybe Justin did this already?

Decoding is taking its time ...

Mkgraph is not multithreaded
Decoding uses multiple jobs
I set the number of jobs to 10.
Too many jobs?

Decoding finished.
WER: 24.92
The WER went up slightly from 24.74 to 24.92.


Now we align again this time using the tri2a models.
Alignment is super fast.

Next we run a new kind of training:
Steps/train_lda_mllt.sh
This will be system tri2b

What is lda_mllt
It is a method for transforming the mfcc features.
Lda refers to linear discriminant analysis not latent dirichlet allocation.
Lda is a method for dimension reduction.
Splicing is performed over several frames.
What is spliced?
The splicing then requires dimension reduction?
Mllt a.k.a. ctc is a diagonalizing transform

Reading: 
http://kald-asr.org/doc/transform.html

lda is typically used for non-speaker specific transforms

lda_mllt training finished
run mkgraph for tri2b system
Again this went really quickly.
Decode using tri2b system

WER: 24.32
It's not clear to me what is being adapted to what here.


- Align again using tri2b system.
Here --use-graphs option is given to the align_si.sh script.
What does this do?
It uses the graphs to do the alignment.
- train using train_sat.sh
Sat stands for speaker adapted training.
It looks like the train_sat.sh is a script for doing mllr adaptation.
The mllr can be done on a system whose features have already been
transformed with lda and mllt.

- Make the graph for a new system tri3b.
What about tri3a?

- decode
This uses steps/decode_fmllr.sh

Lattice rescoring?
It looks like this script decodes once, then rescores the lattices and
decodes again.

WER: 19.22


- alignment
Uses steps/align_fmllr.sh
- train using steps/train_ubm.sh
This trains a mixture of gaussians.
I guess a ubm stands for a mixture of gaussians.
The system is called ubm5b2
I'm guessing this means 5 mixtures and 2 ... ?
Gaussian selection?

- train using  subspace Gaussian mixture model sgmm
This is taking a while ...

We need to work on a legitimate language model.
Right now  we don't really have one.

- Problems
The scripts I'm using from the iban recipe get  confusing at this point.
I don't see decoding for the ubm system.
Apparently, the ubm system is trained for the sgmm system.
?Anyway, I'll have to figure this out later.
- Decoding using the sgmm system:
WER: 14.12



I'm done with this project.
Summary:
I followed the iban recipe where I used the yaounde corpus instead of the
iban corpus.

- lm:
The voxforge recipe uses an lm trained on the transcripts of the training
data.
I did not do this for our yaounde corpus.
This is tricky for our yaounde corpus.
We have to choose our test set so that it does not overlap at the sentence
level with the training set.
I'm not sure this is possible, since the informants on one machine read a
list of prompts that were also read by informatnts on another machine.
Now that we have the Gabon test set, we could use the yaounde corpus for
training, and test on the Gabon data.
We would use the prompts from the yaounde corpus to train the lm.
We could add some other text to the lm training set as long as we do not add
the Gabon prompts.

- G2P:
I had a discussion with Hazrat about the g2p lexicon.
There seem to be 3 approaches:
1. Jalalabad
2. Kanduhar
Uses spelling from 1, but pronunciation from kanduhar.
3. Expat Kanduhari
Uses both spelling and pronunciation from Kanduhar.

We need to figure out what # means in sampa and how it differs from the .
sign.



- Plan for tomorrow:
Run through the same steps for the GP corpus and the younde+gp system.

I'm a little disappointed with the iban recipe. I thought it included more
methods.
It does not go into neural network methods or ivector methods.
I'll have to get these methods from another recipe.
The babel recipe would probably be the best.





-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 20, 2016 3:15 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily Activities Report for Tuesday July 19 2016

The 3 and 4 ngram models are there this morning.
No srilm errors, slthough there are Unicode non-character warnings for the
lower casing of the gigaword corpus.


- Try to use these ngram models in the gp and yaounde systems.

It looks like monophone training does not require an lm.
The lm is used for decoding.

- lm2fst
This took a while
- make graph 
This is taking even longer.
This is the step that expands all the component FSTs into 1 big fst.

- running mk graph on all 3 systems at once: yaound, gp, and yaound+gp
 
Maybe I should stick to the small lm  for a first pass at getting all the
steps to build?

- Ran alignments using yaound+gp monophone system 
This is the first step to building a triphone system.
- Ran alignment diagnostic
This tells you some interesting statistics about each phone.
The proportion of the occurrence of each phone
The average duration in frames of each phone.
A final statistic says  that the corpus has a total of 31 hours of frames 24
of which are non-silence.

I'm giving up on the huge LM for now.
I'm going back and using the prompts to traing the lm, at least for the
yaounde system.

- ran alignment on yaounde data
Diagnostic claims a total of 7.3 hours and 4.2 non-silence.

- running train deltas
I'm not sure what this does.
It uses the questions to build the decision tree to cluster the triphones.
I don't think we have a set of questions yet.
I have a stand in file for the questions.
This adds delta and delta delta features
- Moving along with triphone training on yaounde
Ran the mk_graph script that expands the fst networks with all fst
components.

I'll pick up tomorrow with decoding of the tri1 system.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 19, 2016 6:23 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Tuesday July 19 2016

- lower case the prompts in the gp corpus
This would be easy were it not for the fact that the file names contain
upper case letters.
I have  a script that writes the file name followed by the prompt.
I cannot just lower case each line, since the file name needs to remain in
upper case.
So I first  store the file name and the prompt separately, down case the
prompts, then I paste the resulting files into 1 file.

I put in the time to write the filenames in the kaldi style.

- build the gp system

Why am I having to go through all this trouble to build gp?
There's a recipe for it on kaldi.
It assumes a language model.
But other than that it should work.

- Decoded gp on gp 
WER: 41.80
- Decoded gp on yaounde:
WER: 88.39

- decoded yaounde on gp
WER: 92.86

I think there is a reason these are so bad.
The gp system uses the gp prompts to build the lm and the yaounde system
uses the yaounde prompts.
There is a mismatch between the gp prompts and the yaounde prompts.


- lm
Time to get a fixed lm to work with.
I'm going to try to make an lm from the following corpora:
1. yaounde prompts
2. gp prompts
3. gigafren

I'm making a separate directory to do the lm work.

- training lm

Hopefully when I come in tomorrow the script will have produced ngram
language models.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, July 18, 2016 5:30 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Monday July 18 2016

I left off last week on the yaounde + gp system.
I had setup the train and test folds in a way that I ended up testing on the
training data.
I was running the same script that generates a random train/test split on
the concatenation of the yaounde + gp data, but I was testing on the
previous yaounde and gp random split.

I now setup my scripts to use the split  done previously for yaounde and gp
separately.
I ran through the data setup script.
On the way I'm trying to make the file naming conventions follow the kaldi
conventions.
TThis mostly means using underscores instead of dashes.

I run into a problem when I run the lm prepare script.
I'm using the prompts from yaounde and gp to make the lm training corpus.
There are around 15k sentences.
I get a kn discount error.
I think this means there is not enough data to train 3 grams.
This is a good time to get the yaounde prompts that Steve worked on.
In working with Steve's prompts, I found a problem with the previous prompt
list.
I must have inserted a newline after an "i.e.".
This inserted at least one extra sentence line.
This could really make a difference if we have a lot of recordings of
sentences beyond 1343.
I don't think we do.

I've spent most of the day going back to the basic yaounde system.
I realized that I was not  conditioning the prompts before using them for
training.
Well ... I was, but I should have been doing it earlier in the process.
I separated data preparation into 3 steps:
1. setup
2. lm prep
3. dictionary prep

- extracted mfccs
- convert lm to fst
- train monophones
I get warnings.
This could possibly mean there are problems with transcripts.
- make graph
- decode test data

So I spent the whole day redoing the yaounde system
It looks like the gp data does not have the same problem.
It looks like it was ready to process out of the box.

- Decoding  yaound on yaound finished.
WER: 26.16
So obviously something was wrong before.
I think the labels were way off.
I verified that the sets of speakers in the train and test sets were
disjoint.

- Back to yaounde + gp
I need to set this to use the new yaounde prompts.
It's suspicious that the gp does not do as well as the Yaounde.
Sure enough ... I did not downcase the gp data.
Punctuation was stripped, but no lowercasing.

Tomorrow:
- Return to the basic gp system
Start by lower casing the gp prompts.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 15, 2016 6:10 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Friday July 15 2016

I'm going to skip gender for now on this gp+yaounde monophone system.
- sorting problems
The filenames in the GP corpus have upper case letters.
When the environment variable LC_ALL is set to C upper case letters are
ordered before  lowercase letters.
In a script I sort the filenames.
I was not setting the LC_ALL variable.
The kaldi recipes have scripts that check for these kinds of problems.
This was a problem when combining the filenames from yaounde and gp.
Having trouble with monophone training.
Could it be a sorting problem again.
I sort the feats.scp file.
Maybe I need to set LC_ALL=C here too?
Yup, it looks like that was the problem.

- Trained monophones 
- Decoded yaounde + gp  on yaounde + gp test set
WER: 55.21%
This is a monophone system trained on both Yaounde and GP and tested on 7
speakers from GP and 3 speakers from Yaounde.

- Decoded yaounde + gp system on yaounde test set:
WER: 63%
This is the  same yaounde + gp system tested on the same 10 yaounde speakers
as the yaounde system that got 88% WER.

- Decoded Yaounde + GP system on GP test set:
WER: 25.75
Not sure why this is so good compared to gp on gp 44%.
I'm getting these results from a file called best_wer.
Apparently tests are run using different parameter settings.
For example, I see 3 settings for word penalty.
OK ... I'm being stupid ...
I'm testing on training data.
When I build the Yaounde + GP system I need to use the train/test fold split
from the separate Yaounde and GP systems.
So, I need to  rebuild the Yaounde + GP system.
I'll stop here for this week.
Next week I need to  resume by getting the appropriate training/test sets
for a Yaounde + GP system.
I'll have to fix the scripts that get the lists of training and test data.

Summary table:

Train & test & WER %
Y & Y & 87
GP & GP & 44 
Gp & y & 92.54
Y+GP & Y+GP & 55.21
Y+GP & Y & 63
Y+GP & GP & 25.75

The 55.21 is ok for y+gp on y+gp, but the 63 and 25 for y+gp on y and y+gp
on gp were obtained on contaminated data.


Y stands for Yaounde


The motivation of the problem we're working on is that a system trained on
(even a large) corpus of European French performs poorly on French as spoken
in Africa.  Also the corpora we have for sub populations inAfrica are small
compared to corpora for European French, so systems trained only on those
sub populations still perform poorly. So the fact that we get 87 wer for
Yaounde on Yaounde supports our project. Although I expect that the 87 will
drop with more sophisticated models.

I need the results of gp on Yaounde to fill in the motivation. I think I got
a 92, but I did not save the results
 
GP on gp 44
Y on y 87
Gp on y 92

These numbers support our story. GP on GP is good (44), while gp on Y (92)
and Y on Y (87) are horrible.
 
I'm running the gp on y again now.
Yes.gp on y gives 92.54, really bad.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 14, 2016 5:37 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 14 2016

Mono training finished successfully yesterday.
- mkgraph ran successfully ( I think)
This is an important step in the system building process.
It creates a "fully expanded decoding graph"
It basically creates the "model", including the lm, pronunciation
dictionary, context dependency and hmm structure.
The output is a finite state transducer (fst).
I think each component is expanded to an fst then they are all combined into
1 fst.

- Ran decoding
- Ran scoring
I had trouble with scoring.
The problem was a missing script:
Utils/int2sym.pl
The fact that this script was missing was buried in a log file for the
scoring script run.
I reran scoring:
Wer: 44%
The test data here are from the GP corpus.
I guess what we want now is to test The GP system on Yaounde data.

** TODO Test GP on Yaounde

I have to downsample the Yaounde data from 22050 to 16k.
Done.
I asked  Justin to put this data on the corpora disk.

- Ran decoding on Yaounde test data:
WER: 92.54
I consider this preliminary.
There are a lot of files hanging around and I'm not sure the correct files
are being used in decoding and scoring.
I'm not sure how fst decoding works.
There's an fst built with the training data.
This is the "model" the decoder uses.
The decoder "decodes" test data.
The test data is stored in a directory.
When I ran the GP decoder on the GP test data the decoder pointed at the GP
test directory .
When I ran the GP decoder on the yaounde test data I pointed the GP decoder
to the yaounde test directory.
I'm worried that the yaounde decoder and the gp decoder get the same score
on the yaounde data 92%.
Actually, with Steve's dictionary the Yaounde system got 87.
- Reran Yaound build with downsampled data:
WER: 88.08
The test set is different on every build since it is chosen randomly.
This might explain the difference between 87% for 22050 and 88% for 16000.


- Moving on to a system built on GP concatenated with Yaounde.
- working through the data setup script.
This mostly involves making the script I used for gp to run on the union of
gp and yaounde.
To get one script to run, I made a symbolic link from the corpora disk to
the working directory on the gpu machine.
I'm not sure this will work down range.
I'm leaving today at the point where the speaker genders are extracted from
info files.
I'm not sure if I'm going to do gender at this point.
I might leave it for a later refinement.
Next steps include lexicon preparation and lm prep.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 13, 2016 5:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for July 13 2016

Writing the perl command line in a separate perl script seems to have solved
the problem I was having yesterday with the script that prepares the lm
training data.
Currently I'm only using the prompts to train the lm.
** TODO Decide on what to use for lm training data

- Working on extracting MFCCs.
GP data sampled at 16k
Yaounde data sampled at 22050

I think we need to downsample the Yaounde data to 16k.
I did this in the past, I unintentionally skipped downsampling this time.

I realized that gp file names are not in numerical order.
I renamed wav files and prompts.

The training script was failing.
I finally figured out that the number of jobs must be the same for the mfcc
extraction and the training.
I set the number of jobs to 10 for both cases.
More than 10 fails since there are only 10 speakers in the test set.

- running a script  called format_lms.sh 
I think this script converts the LM into an fst.

- run make graph script

- run decoder

- run scoring

On scoring I found a problem with my transcripts.
The script that writes the prompts to each speaker directory did not write
the speaker directory to the paths.
I'm running each script again  to see if the problem gets fixed in scoring.
I'm surprised this did not cause a problem somewhere else.
I'm leaving having started the run of the mono train script.
I'm not going to wait for it to finish before leaving.
This script basically aligns mfcc vectors with monophones and computes
parameters for the gaussians.
If it succeeds, I need to next run the script that creates the graph.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 12, 2016 4:57 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for July 12 2016

I got the scoring script to run. Indeed the problem was with the 3
arguments.
Score.sh takes 3 arguments:
1. A data directory that contains  a bunch of files and directories
associated with the run of the decoder on the test data.
2. A language directory containing file and directories associated with the
fst that is input to the decoder. There are a lot of files referring to
phones and words  here.
3. A directory where the results and trace from the scoring are placed.

The WER was 92%, so only 8% of the words were correctly recognized.
Hoepfully, this will be the lowest score we get :)


** TODO Rebuild previous system with Steve's dictionary

EU is not a phone in Steve's dictionary.
Eu is a rounded phone as in deux
Ee (schwa) needs to be added to phone list.

The uy phone appears in the dictionary, but not in the phone list.
Add uy to phone list.
Add gn to phone list.

I rebuilt the system with the new dictionary.
WER: 87% down from 92%
Installed 2 diagnostic scripts.
LM:  3gram only trained on 1339 prompts.

Moved on to building similar ASR system with Kaldi for the GlobalPhone
corpus.
Wrote data setup scripts :
This mostly gets lists of files, associates wav files with prompts,
utterences with speakers, speakers with utterences, etc .
Wrote dict setup script:
This mostly deals with preparing the phone lists to build FSTs and what to
do with silence.

I'm leaving in the middle of writing the  script to  prepare the lm
There's 1 script to get the lexicon.
This takes an argument that does not seem to be there yet.
I have a perl command line written inside a bash script.
I'm going to write the perl script in a separate file.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* >Monday, July 11, 2016 3:42 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Monday July 11 2016

I'm going to write a short Daily Journal, mostly so the next day I can
remember where I left off the previous day.
If this annoys you let me know.
John
I'm writing a kaldi recipe to build a monophone ASR system trained and
tested on the Yaounde corpus of French.
I have written the data preparation scripts, the scripts that run the mfcc
extractor the script that runs the training, and the script that decodes
test data.
I'm leaving today in the middle of setting up the script that runs the score
script.
I'm expecting this to give me WER scores for the test data.
It looks like the score script is expecting a file containing words.
I haven't generated this file yet.
I also have to look closely at the 3 arguments to the score script.
. /home/tools/torch/install/bin/torch-activate
I cloned the OpenNMT repo from:
https://github.com/OpenNMT/OpenNMT.git
I followed instructions in the README.md:
cd OpenNMT
To preprocess:
th preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo
To Train:
th train.lua -data data/demo-train.t7 -save_model model
q
