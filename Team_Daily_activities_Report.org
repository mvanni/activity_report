* DAR <2017-03-10 Fri>
**  Goals for Friday set Thursday:
- TODO Map small Santiago Arabic dictionary to Transtac phone  set for SOF Tunis ASR system build with Kaldi.
This is not worth doing.
- TODO Read chapters 13 and 14 of Comptia A+ Complete Study Guide.
- TODO Check on status of Librispeech.

** Goals for Monday:
- TODO Buckwalter to Unicode on QCRI dictionary
- TODO SOFTunisia build.
Read chapters 14 and 15 of Comptia A+ Complete Study Guide.
* DAR <2017-03-09 Thu>
** Goals for Thursday set Wednesday:
- DONE Minimal decoding with gp nnet2 online models
I put these models under:
/home/data/ateam/gp_fr_nnet2_online_components
The README.txt file has a minimal command line.

- DONE Incorporate Transtac Iraqi Arabic Dictionary into SofTunis ASR system build.
I got this to work.
I deleted the # sign from the new dictionary -- it is not allowed in kaldi.

- TODO Read chapters 12 and 13 of Comptia A+ Complete Study Guide.
Chapter 12 is on troubleshooting. It is very long. I read the section on troubleshooting Printers. Next is the  section on networking.
I finally finished chapter 12 today.
That covers the first 220-901 part of the A+ exams.
Chapter 13 is on Operating Systems in general.
- DONE IDp in ACT
I got this started:
Steve told me to go to:
https://actnow.army.mil/
I put Reggie's name and series 1550 and cp16.
I hit submit and accept.
I'm not sure if this worked or not.
Reggie got it.
I probably need to fill out my goals.

- Women's History Month:
I went to Melissa Flag's talk.

** Goals for Friday:
- TODO Map small Santiago Arabic dictionary to Transtac phone  set for SOF Tunis ASR system build with Kaldi.
- TODO Read chapters 13 and 14 of Comptia A+ Complete Study Guide.
- TODO Check on status of Librispeech.

** DAR <2017-03-08 Wed>
** Goals for Wednesday set Tuesday:
- TODO SOFTunis data/lang directory.
- TODO Convert gale arabic dictionary from Buckwalter to utf8.
- DONE GP online models.
The neural network training is running.
It will run for 384 passes.

- TODO Librispeech. 
- TODO Read chapters 11 and 12 of Comptia A+ Complete Study Guide.
At  COB today I am in the middle of chapter 12 Hardware and Network Troubleshooting, Troubleshooting Storage Device problems, Identifying Optical Drive Issues.

** Goals for Thursday:
- TODO Minimal decoding with gp nnet2 online models
- TODO Incorporate Transtac Iraqi Arabic Dictionary into SofTunis ASR system build.
- TODO Read chapters 12 and 13 of Comptia A+ Complete Study Guide.
- TODO IDp in ACT
 DAR <2017-03-07 Tue>
** Goals for Tuesday set Monday:
- DONE Read chapters 9 and 10 from Comptia A+ Complete Study Guide.
At COB today I am in the middle of the Laser Printers section  in chapter 11 on printers.

- TODO Train Online models for gp.
This is running on the GPU.
I had started much earlier on my laptop, but because the GPU is much faster than the CPU, the training on the GPU is on pass 10 while the training on the CPU is only on pass 8.

- TODO Librispeech 
This is also running on the GPU.
It is surprising that 2 jobs can be using the GPU.
- TODO convert github repos to bare repos.
I converted the gp repo to a bare repo.
- TODO SofTunis lexicon and lm.
I am stuck on the step that makes the data/lang directory.
The script that does this is choking on the <unk> symbol.
For now I want to get a minimal example working with our SANTIAGO dictionary.

** Goals for Wednesday:
- TODO SOFTunis data/lang directory.
- TODO Convert gale arabic dictionary from Buckwalter to utf8.
- TODO GP online models.
- TODO Librispeech. 
- TODO Read chapters 11 and 12 of Comptia A+ Complete Study Guide.

* DAR <2017-03-06 Mon>
**  Goals for Monday set Friday:
- TODO Incorporate LDC2017L01 dictionary into the SOFTunis kaldi system build.
- DONE Read chapters 8 and 9 of Comptia A+ Complete Study Guide.
At COB today I'm reading the section Display Port and Thunderbolt under The next Generation of laptop expansion cards under Understanding Laptop Architecture in Chapter 9.

- TODO librispeech tdnn on 960 hours of speech.
The first thing the script does seems to be extracting high resolution mfcc features?
I hope so, since these get used in training the online models.
- DONE Prepare models for delivery to Transapps team.
I prepared a minimal example with gp monophones.

- github repositories
I spoke with Chem Friday about git and github repositories.
I asked him what ARL's policy is concerning github repositories.
He said that a repository on github  that has code created at work should be made into an open source project.
For my purposes though he said what I really want is to create a bare repository.
I think I figured out how to make a bare repository today.
I used the following command on the GPU machine under /home/john:
git init --bare --shared=all softunisia.git
This creates a bare repository with name softunisia.git.
The next question was how to I convert the softunisia repo that I already had to use the new bare repository.
I went into the .git/config file and changed the url variable to point to the new bare repository on the GPU machine.
I set url to john@131.218.130.12:/home/john/softunisia.git
This seems to have done the job.
I pushed a commit.
I was prompted for my password.
Now I can delete the softunisia repo from github and I will ot be going outside our rednet.
I do this same procedure for all my repos.

- Online models:
The Transapps team is requesting the ASR components for decoding.
Now I have to put my money where my mouth is.
I setup a minimal example for the gp corpus system.
The decoder command is at:
/home/data/ateam/gp_mono_components/README.txt.
It only requires 2 files:
1. hclg.fst
2. final.mdl
There is another file:
words.txt
but I don't think it it required.
This is a very minimal example.
I copied the data/eval directory to the /home/data/ateam/gp_mono_components directory so the command could actually run.
The data/eval directory avs the files needed to point the decoder to the data files to be decoded.
I think we'll have to train online models to avoid using the files under data/eval in this way.

** Goals for Tuesday:
- TODO Read chapters 9 and 10 from Comptia A+ Complete Study Guide.
- TODO Train Online models for gp.
- TODO Librispeech 
- TODO convert github repos to bare repos.
- TODO SofTunis lexicon and lm.
* <2017-03-03 Fri>
**  Goals for Friday set Thursday:
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
Interesting fact from chapter 6.
The structure of TCP/IP is based on DoD the model.
The DoD model maps to the OSI model layers.
4 DoD layers map to 7 OSI layers.
Process or Application -> Presentation  + Application + Session
Host to Host -> Transport 
Internet -> Network
Network Access -> Datalink + Physical.

At COB, I've read chapters 6 and 7. 
In chapter 8, I am reading the section on wireless encryption.
- TODO Prepare softunisia data, including lexicon, lm and mfcc features.
Steve found a recent LDC delivery LDC2017L01 that contains a pronouncing dictionary for Arabic.

- TODO librispeech tdnn on 960 hours of speech.
The clean up is still running.
It does a lot of things.

- DONE Conference call with JHU and TransApps
Yenda was very helpful and cooperative.
** Goals for Monday:
- TODO Incorporate LDC2017L01 dictionary into the SOFTunis kaldi system build.
- TODO Read chapters 8 and 9 of Comptia A+ Complete Study Guide.
- TODO librispeech tdnn on 960 hours of speech.
- TODO Prepare models for delivery to Transapps team.

* DAR <2017-03-02 Thu>
**  Goals for Thursday set Wednesday:
- DONE Prepare transcripts for SOFTunis data.
I made the wav.scp, utt2spk, text and spk2utt files for the Recordings_Arabic data.

- TODO Rerun librispeech clean up on 960 hour corpus.
It is still running.
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
I'm not finished with chapter 6 yet.

** Goals for Friday:
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.
- TODO Prepare softunisia data, including lexicon, lm and mfcc features.
- TODO librispeech tdnn on 960 hours of speech.

* DAR <2017-03-01 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Read chapters 5 and 6 of Comptia A+ Complete Study Guide.
I read chapter 5.
I am in the middle of reading chapter 6 on networking.
I am reading the section called "Explaining Ethernet Naming Standards"

- TODO Chain models on librispeech.
There was a problem.
It looks like the clean up process failed yesterday or today with the brown out.
The GPU is reporting about 30% utilization, but I don't see any processes running.
I asked Justin to reboot the GPU machine.

- DONE Data prep for SOFTunis
I downsample the Tunisia data to 16000 HZ.

** Goals for Thursday:
- TODO Prepare transcripts for SOFTunis data.
- TODO Rerun librispeech clean up on 960 hour corpus.
- TODO Read chapters 6, 7, and 8 from Comptia A+ Complete Study Guide.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

* DAR <2017-02-28 Tue>
** Goals for Tuesday set Monday:
- DONE Read chapters 3 and 4 of Comptia A+ Complete Study Guide.
- TODO chain models on Librispeech.
I am running the clean up script.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

- SOFTunisia:
We found a copy of the SOFTunis Arabic speech corpus.
I made a new repository for the scripts to process this corpus.

** Goals for Wednesday:
- TODO Read chapters 5 and 6 of Comptia A+ Complete Study Guide.
- TODO Chain models on librispeech.
- TODO Data prep for SOFTunis

* DAR <2017-02-27 Mon>
**  Goals for Monday set Friday:
- TODO Read chapter 2 of Comptia A+ Complete Review Guide.
I'm actually reading the Comptia A+ Complete Study Guide.
Today, I finished reading Chapters 1, 2 and part of chapter 3.
1. Motherboards, Processors and Memory.
2. Storage Devices and Power Supply.
3. Peripherals  and Expansion.

I'm on the paragraph on input devices, specifically, keyboards.

- TODO Librispeech build.
I am running the quick SAT training with 960 hours of speech (tri6b).

** Goals for Tuesday:
- TODO Read chapters 3 and 4 of Comptia A+ Complete Study Guide.
- TODO chain models on Librispeech.

**  Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.
- TODO Chain model training for librispeech.

* DAR <2017-02-24 Fri>
**  Goals for Friday set Thursday:
- TODO Read Chapter 1 of Comptia A+ Complete Review Guide.

I'm not finished yet at COB today.

** Accreditation Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

** Goals for Monday:
- TODO Read chapter 2 of Comptia A+ Complete Review Guide.

* DAR <2017-02-23 Thu>
**  Goals for Thursday set Wednesday:
- DONE Read lesson 16 from Cyber Security Fundamentals.
I'm ready to take the test.
- DONE Take the cyber security fundamentals test (Can I do this?)
I had to go to the test link on the opening page.
I passed with a 75.

- TODO Linux+ on skillport.
I looked into this.
There does not seem to be anything for LX0-101 and LX0-102.
Justin says it is not a matter of knockingout a couple of test.
The Linux+ is a real Comptia Certification.
Maybe Reggie had it wrong?

** Checklist
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. DONE Sign Privileged Level Access Agreement. 
4. DONE Take Cyber security Fundamentals Course. 
5. DONE Pass Cyber security Fundamentals Course.
6. TODO Study for Comptia A+ certification exam.
7. TODO Take and pass Comptia A+ exam.
8. TODO Study for Security+ exam.
9. TODO Take and pass Security+ exam.
10. TODO Work through Linux+ course on skillport.

** Goals for Friday:
- TODO Read Chapter 1 of Comptia A+ Complete Review Guide.

* DAR <2017-02-22 Wed>
**  Goals for Wednesday set Tuesday:
- Todo Lessons 8-16 of Cyber Security Fundamentals.

** Certification todo list:
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders.
3. TODO Sign PLA. 
4. TODO Take Cyber security Fundamentals Course 
a. DONE Lesson 1: Army Information Assurance Program.
b. DONE Lesson 2: Federal Laws, DoD Regulations and Policies.
c. DONE Lesson 3: Army Regulations and Policies.
d. DONE Lesson 4: Army Information Information Assurance Training Program
e. DONE Lesson 5: Network and Hacker Threats.
f. DONE Lesson 6. Malware
g. DONE Lesson 7: Physical Security
8. DONE Lesson 8: Risk Assessment and Management.
9. DONE Lesson 9: Security and Incident Response  Planning
10 DONE Lesson 10: Continuity Of Operations (COOP)
11. DONE Lesson 11: Department of Defense Information Assurance Certification and Accreditation Process (DIACAP).
12. DONE Lesson 12: Wireless Security (802.11)
13. DONE Intrusion Detection Systems (IDS) and Auditing.
14. DONE Lesson 14 Firewalss and Perimeter Defense.
15. DONE Lesson 15: Encryption and Common Access Cards.
16. TODO Lesson 16: Legal.



5. TODO Pass Cyber security Fundamentals Course:
6. 

I spent the whole day reading the Cyber Security Fundamentals lessons 8-16. 
I  should read lesson 16 again since I only skimmed through it this afternoon.

** Goals for Thursday:
- TODO Read lesson 16 from Cyber Security Fundamentals.
- TODO Take the cyber security fundamentals test (Can I do this?)
- TODO Linux+ on skillport.

* DAR <2017-02-21 Tue>
** Certification todo list:
1. DONE Create account  in ATTCTS.
2. TODO Appointment orders:
3. TODO Sign PLA 
4. TODO Take Cyber security Fundamentals Course 
5. TODO Pass Cyber security Fundamentals Course:
 The lessons for this course are on the web and luckily they seem to be very accessible.
Today I've gone through:
a. Lesson 1: Army Information Assurance Program.
b. Lesson 2: Federal Laws, DoD Regulations and Policies.
c. Lesson 3: Army Regulations and Policies.
d. Lesson 4: Army Information Information Assurance Training Program
e. Lesson 5: Network and Hacker Threats.
f. Lesson 6. Malware
g. Lesson 7: Physical Security

.
6. 
** Goals for Wednesday:
- Todo Lessons 8-16 of Cyber Security Fundamentals.
* DAR <2017-02-14 Tue>
**  Goals for Tuesday set Monday:
- DONE Gp + Gabon Read + Gabon CONV evaluation for mono, tri1, tri2a, tri3b, tri4b and tri5b .
I have to spend some time on writing a script to get the results from the files written by kaldi.
- TODO Build nnet2, sgmm and sgmm2 models on gp+gabon read+gabon conv.
This is running today.
The nnet2 script is using the GPU.
- TODO librispeech nnet5a on {GPU.
This is also using the GPU.
I was surprised to find that you could run 2 scripts that both use the GPU. 
-TODO Demo
 I read some of the kaldi tutorial.
Specifically, about ark and scp files.
This helped me get a better understanding of how to compute mfcc features.
There is a kaldi program called compute-mfcc-feats that extracts mfcc features from .wav files and stores them in a file.
It takes 2 arguments:
1. a wav_rspecifier 
2. a feats_wspecifier

The 2 arguments involve ark and scp files.

- gp + gabon read + gabon conv  nnet2 models on the GPU:
When I left yesterday, I started a script to run the nnet2 p-norm training on the gp + gabon read + gabon conv data.
I was not sure if the script would run.
This morning it has gone through 267 iterations and the GPU is running at 99%.
It will go through 480 iterations, so it might be done this afternoon.

** Goals for Wednesday:
- TODO Kaldi Workshop

* DAR <2017-02-13 Mon>
**  Goals for Monday set Friday:
- TODO Librispeech
I started the training for the tri5b models.
tri5b is built by running more sat on top of tri4b with 460 hours of speech.
Recall that tri4b was SAT on 100 hours of speech.
The tri5b training finished.
I started the evaluation on tri5b.
I also started the nnet5a training.
I killed it because I saw that it is not using the GPU.
It is set to use 16 threads instead.
At COB today, I am restarting the nnet5a training with number of threads set to 1.
This should trigger the GPU.
 
- DONE build gp + gabon read.
Putting everything in one run.sh script was not working, so I've gone back to my method of putting steps in their own scripts that have names beginning with numbers.
The numbers make it easy to reproduce my steps.
I have mllt lda sat tri3b models built on gp.
I took the following steps: 
1. I extracted mfcc features from the gabon read data.
2. I consolidated the gp and gabon read data directories.
This wrote new spk2utt, text, utt2spk and wav.scp files for the consolidated gp and gabon read corpora.
3. I  aligned the gp gabon read data with the tri3b models.
4.  I  ran sat  training on the gp gabon read data to get tri4b models.
This tri4b training step finished.
I also did the following steps:
1. I extracted mfcc features from the gabon conv data.
2. I consolidated the gp, gabon read and gp conv data directories.
This wrote new spk2utt, text, utt2spk and wav.scp files for the consolidated gp,  gabon read and gp conv corpora.
3. I  aligned the consolidated gp, gabon read and gabon conv data with the tri4b models.
4.  I  ran sat  training on the gp, gabon read and gabon conv data to get tri5b models.

At COB today, I am running the decoding scripts for the model sets I've built so far.
- TODO Demo
   I put a little work into this today.
I am trying to get a minimal example for the mfcc extraction step.
The script that wants to split the data into chunks to run in parallel is getting in the way.
The utt2spk and spk2utt files are also being used and I'm not sure they are required.

** Goals for Tuesday:
- TODO Gp + Gabon Read + Gabon CONV evaluation for mono, tri1, tri2a, tri3b, tri4b and tri5b .
- TODO Build nnet2, sgmm and sgmm2 models on gp+gabon read+gabon conv.
- TODO librispeech nnet5a on {GPU.
-TODO Demo
 
* DAR <2017-02-10 Fri>
**  Goals for Friday set Thursday:
- TODO Librispeech 
At COB today I am aligning the 460 hour data set with the tri4b models.
tri4b is a speaker adapted system trained on 100 hours.
- DONE Gp build for gp + gabon 
I am starting to prepare the gabon read to incorporate it into the gp+gabon system.
The test set makes this a little tricky.
Our test set has speaker directory names that are different from the speaker directory names in the training set.
I parsed the speaker number and recording number out of the SRI transcribed test data files. 
I did the same for our test set data files and matched them.
- Caveat: this did not work.
The  recording names do not match.
The speakers can easily be mapped, but the recording names have to be aligned one by one.
We now have 2 test sets from the Gabon data collection.
1. Steve's 515 selected utterances from 12 speakers.
2. The 375 utterances that overlap between the SRI transcripbed read data and Steve's test set.
 
** Goals for Monday:
- TODO Librispeech
- TODO build gp + gabon read.
- TODO Demo

* DAR <2017-02-09 Thu>
**  Goals for Thursday set Wednesday:
- TODO Set up and build a gp + gabon system with the gabon data we sent to Voxtek.
As part of this goal, I made scripts to process the gabon_conv data and put them in the gabon-conversational repo.
At COB today, I've processed the gp data, the dictionary and the language model.
I plan on building sgmm models with the gabon data on top of the gp data.
I'll build the tri3b models only with gp
- DONE Train tri1 librispeech models.
At COB today I am training SAT models tri4b on 100 hours of speech.

- TODO Accent id
- TODO Demo
- DONE LM for gp + gabon
I included:
1. GP training prompts
2. Yaounde training (red) prompts.
3. BIC
4. Gabon read training prompts.
5. Gabon conv training transcripts.
6. subs

** Goals for Friday:
- TODO Librispeech 
- TODO Gp build for gp + gabon 

* DAR <2017-02-08 Wed>
**  Goals for Wednesday set Tuesday:
- TODO Train English monophones on librispeech,
I ran the monophone training.
I started making the fst and decoding with the mono models.
I also started the alignment with the mono models.
These initial steps  are done with a relatively small amount of data.

- TODO Make a separate repo for the conversational gabon/sri/central accord speech transcripts processing scripts,
I made  gabon-read and gabon-conv repos.
I am writing processing scripts.
- TODO Incorporate the above scripts in the accent id repo,
- TODO Drill further down into gp demo.

** Goals for Thursday:
- TODO Set up and build a gp + gabon system with the gabon data we sent to Voxtek.
- TODO Train tri1 librispeech models.
- TODO Accent id
- TODO Demo
- TODO LM for gp + gabon

* DAR <2017-02-07 Tue>
**  Goals for Tuesday set Monday:
- TODO Niger data prep scripts.
- TODO Consolidate Niger scripts into accent id scripts.
- TODO Ditto for Central Accord/sri_gabon conversational speech scripts.
I did not get to this, but instead I put the gp processing scripts into the accent id repo.
Now we have scripts for 3 data sets under the accent id repo:
1. yaounde
2. niger
3. gp
- TODO Librispeech build.
I did not do a lot on this today, but I did run the following:
1. data preparation,
2. downloaded the pre-built language models,
3. Dictionary preparation,
4. lang directory preparation (this is mostly lexicon preparation),
5. format the language models (arpa to fst  and const arpa format),  
6. mfcc feature computation,
7. 

Maybe tomorrow I'll be able to start monophone training.
The first next step is to make data subsets.

- Demo project:
I took a first step at building a  speech to speech demo.
I setup a script to run a single wav file through the kaldi decoder.
I did this for the gp system.
I have  this tiny demo running on hclg fst files for  the following models:
1. mono
2. tri1

I'm in the process of getting it to work for:
3. tri2a
4. tri2b
5. tri3b
6. sgmm

** Goals for Wednesday:
- TODO Train English monophones on librispeech,
- TODO Make a separate repo for the conversational gabon/sri/central accord speech transcripts processing scripts,
- TODO Incorporate the above scripts in the accent id repo,
- TODO Drill further down into gp demo.

* <2017-02-06 Mon>
**  Goals for Monday set Friday:
- DONE Do another pass on the conversational speech transcripts conditioning.
Steve took a look at the results and it looks good.
- TODO  Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.
Today I put the scripts to process the yaounde read transcripts in the accent-id repo.
I'm currently working on the niger scripts in its own repo.
As I am getting ready to leave, the next step to take tomorrow is the script that gets the Niger speaker names. 
- librispeech:
librispeech is a free corpus of English read speech.
I had Justin put it on /mnt/corpora.
I can build an ASR system with it and use it to make an i-vector extractor.

** Goals for Tuesday:
- TODO Niger data prep scripts.
- TODO Consolidate Niger scripts into accent id scripts.
- TODO Ditto for Central Accord/sri_gabon conversational speech scripts.
- TODO Librispeech build.

* DAR <2017-02-03 Fri>
** Goals for Friday set Thursday:
- TODO Process the SRI provided transcripts for the CA16 data. Specifically, the conv tdf files and the read speech transcripts that Steve gave me today.
I worked all day on the conversational speech transcripts.
I removed pronunciation comments.
I processed comments in parens and double parens.
I may have deleted more text than I should have.
I'll have to go back and check what is done to the text in parens.

- DONE Make a repo for the Niger corpus processing scripts. Specifically, we want a place for the official transcripts.
I made  the repo and put the niger transcripts there.

- TODO consolidate the central accord test data and the sri_gabon directory on /mnt/corpora.
This will not happen for a while.

** WAR:
John Morgan conditioned a set of transcripts for the conversational part of the African  French speech corpus that was collected in Libreville, Gabon in 2016. 
The transcripts are intended to be used for adaptation of European French acoustic models to African accented French. 
Adaptted acoustic models will be used to improve speech-to-speech applications on hand-held devices.

** Goals for Monday:
- TODO Do another pass on the conversational speech transcripts conditioning.
- TODO  Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

* DAR <2017-02-02 Thu>
**  Goals for Thursday set Wednesday:
- TODO Investigate deeper the LID recipe in kaldi to see if we can use if for AID.
It would really help if we could get the LDC callfriend corpora, but not absolutely necessary.
We may already have them, but they are not on the /mnt/corpora disk.

- TODO Process the transcripts made by SRI for the CA16 corpus.
I worked on this today.
This will be the priority for the next couple of days.

- TODO Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

** Goals for Friday:
- TODO Process the SRI provided transcripts for the CA16 data. Specifically, the conv tdf files and the read speech transcripts that Steve gave me today.
- TODO Make a repo for the Niger corpus processing scripts. Specifically, we want a place for the official transcripts.
- TODO consolidate the central accord test data and the sri_gabon directory on /mnt/corpora.

* DAR <2017-02-01 Wed>
** Goals for Wednesday set Tuesday:
- TODO Build SGMMs for gp.
training finished over night on my laptop.
I started alignment this morning.

- TODO Build dnn i-vector extractor.
This would be cool, but I'm not sure now if it is necessary.

- Accent ID plan:
Omit seemed to suggest that we could follow the recipe that was used for LID to do Accent ID.
I would like to look into this.
My understanding of LID is that given some speech as input, LID outputs the best guess at the language spoken by the speaker.
The pyspeech tutorial is trained with 57 languages. 
I think we can start with a system that when given some french speech as input, outputs a guess that the speech is from either a speaker with a European accent or an African accent.

** Goals for Thursday:
- TODO Investigate deeper the LID recipe in kaldi to see if we can use if for AID.
- TODO Process the transcripts made by SRI for the CA16 corpus.
- TODO Consolidate all the scripts to process our African French holdings so that we can make an i-vector extractor for African Accented French.

* DAR <2017-01-31 Tue>
** Goals for Tuesday set Monday:
- TODO visit NIST
We had a Very good visit.

- LRE
Omid mentioned the LRE recipe as a model for accent id.
I am looking at the kaldi recipes for LRE.
It looks like all the steps he outlined are in the kaldi LRE recipes.
Specifically, there is a way to use a dnn instead of a gmm to build a ubm.
The ubm is then used to train an i-vector extractor.
The script to train the dnn i-vector extractor requires an sgmm.
The recipe I'm using requires a ubm to build an sgmm.
The ubm here is used to initialize the sgmm.
The first ubm is trained using a gmm.

As I'm getting ready to leave, I'm training SGMMs on gp.
A ubm was previously trained to initialize the sgmm.
I am considering  building a dnn ubm to train the i-vector extractor by following the scripts in the lre recipes.
This will probably require more data than the gp corpus to beat the gmm-based ubm.
I'd eventually like to follow the LRE scripts to build an accent id similarly to the way they built a lid.

** Goals for Wednesday:
- TODO Build SGMMs for gp.  
- TODO Build dnn i-vector extractor.

* DAR <2017-01-30 Mon>
**  Goals for Monday set Friday:
- TODO Finish the gp with simple lm build and consolidate all the steps in 1 run.sh script in the repo.
- TODO clean uup the gp repo.
- TODO Make a yaounde build and put all the scripts in the yaounde repo (wait for Steve's test set fold).
- TODO Work on the other builds and scripts in their own repos.
- TODo Consider a repo for gp + yaounde chain model scripts.

** Goals for Tuesday:
- TODO visit NIST

* DAR <2017-01-27 Fri>
** Goals for Friday set Thursday:
- TODO Clean up scripts in yaounde, gp+yaounde, etc. in their own repos.
- TODO Write results for gp with simple lm.

- Experiment:
My goal was to build a minimal gp chain model system for use as a baseline.
The previous system  I was using started the chain model build from the tri3b system.
How is the tri3b system built?
1. Train mono
2. Get alignments from mono
3. Train tri1 with mono alignments.
4. Train tri2b with mllt, lda and tri1 alignments.
5. Get SI alignments with tri2b (I think the SI stands for speaker independent).
6. Train tri3b with sat and tri2b alignments.
7. Get fmllr alignments with tri3b (this get used later).
8. Generate phone-level alignment lattices containing alternative pronunciations using fmllr alignment with tri3b models.
9. Build a tree using tri3b alignments made in step 7.
10.  Train a ubm for online i-vector extraction. This step uses the tri3b models to transform features (this is still a mystery).
11. Train tdnn. this step uses both the alignment lattices generated in step 8 and the tree built in step 9. Note that the tree built in step 9 uses the allignments from step 8. So, this tdnn training step uses both alignments generated using the tri3b models.
 
In attempting to build a minimal gp chain model system, I skipped the tri3b model and used the tri2b model instead. 
This means I skipped sat training.
The results were slightly worse for the system that only used tri2b.

I'm going to make the minimal gp chain model include the tri3b step.
I did this and the results wer as expected: wer: 49.59.

** Goals for Monday:
- TODO Finish the gp with simple lm build and consolidate all the steps in 1 run.sh script in the repo.
- TODO clean uup the gp repo.
- TODO Make a yaounde build and put all the scripts in the yaounde repo (wait for Steve's test set fold).
- TODO Work on the other builds and scripts in their own repos.
- TODo Consider a repo for gp + yaounde chain model scripts.

* DAR <2017-01-26 Thu>
**  Goals for Thursday set Wednesday:
- TODO Build gp system
I'm on the denominator lattice generation step for the sgmm system.
This takes forever.
- DONE Build gp chain model system
I found a problem with the way I was trying to build the gp-chain system.
I wanted to build the ubm directly from the tri1 models.
When I ran the script to train the diab ubm, I got and error saying that the splice options were not available.
I found that these splice options are associated with the triphone models that are trained with mllt and lda.
So, I am taking a step back to train triphones with mllt and lda.
As I'm getting ready to leave, the gp chain system has trained and even tested.
The scoring scripts were not accessible, so I'm running the decoding step over again.
  
- TODO Incorporate open subs into fr-lm training scripts.


- I spent more time cleaning up my repos.
 I put the eesen scripts in their own repos.
I worked on getting the yaounde scripts cleaned up in their own repo.

** Goals for Friday:
- TODO Clean up scripts in yaounde, gp+yaounde, etc. in their own repos.
- TODO Write results for gp with simple lm.
 
* DAR <2017-01-25 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Meet with Jacquin.
Jacq gave us models and results.
- TODO Train GP chain model system with only gp trainin prompts in lm. 
I created a new git repo called gp-chain for the chain models on the gp corpus scripts.
I have scripts to do the following:
1. build up to tri1.
2. Make alignment lattices.
3. Make the template model with a special topology.
4. Build a tree.
5. Train a diagonal UBM.
6. Train an i-vector extractor.
7. Do something with speakers. (not sure, but it looks like only 2 utterences from each speaker are put aside).
8. Extract i-vectors (online).
9. Train a tdnn.
10. Make a decoding fst graph.
11. Extract test i-vectors (online) for dev and eval data.
12. Decode dev and eval.

- TODO Incorporate open subs into fr-lm training scripts.
Did not get to this.

** Goals for Thursday:
- TODO Build gp system
- TODO Build gp chain model system
- TODO Incorporate open subs into fr-lm training scripts.

* DAR <2017-01-24 Tue>
** Goals for Tuesday set Monday:
- DONE Get more data to build LM and write scripts to process this data.
I processed the data from gp,train, dev and test,  yaounde, questions and prompts, sri gabon conversational tdf and bic

- gp training
I went ahead and started training a gp system only using the gp training prompts in the lm training.
Again, the mono WER scores are bad: 56 and 58.

** Goals for Wednesday:
- TODO Meet with Jacquin.
- TODO Train GP chain model system with only gp trainin prompts in lm. 
- TODO Incorporate open subs into fr-lm training scripts.
 
* DAR <2017-01-23 Mon>
** Goals for Monday set Thursday:
- TODO Build our own lm.
I created a new git repo for fr-lm.
I'm going to put the scripts and files for building French LMs here.
I have scripts for gettting the gp prompts.
I have the Yaounde prompts and questions.
I am in the middle of collecting the transcripts from SRI.
There are 4 files for the Canada and Gabon transcripts.
2 for conversational speech and 2 for read speech.
There are also transcripts under the tdf directory.
The files under the tdf directory contain more information than just the transcripts.
I'm not sure if the above 4 files overlap the tdf files.
The conversation transcripts contain words in parens, double parens, some asterisks and some comments in braces.
I am in the process of removing these by hand in the 4 above files.
Should I have done this in scripts?

I sent a message to the kaldi help mailing list asking Bogdan about what lm and lexicon he used to get his results.
 
- TODO Continue building system with lm trained on gp training prompts.

** Goals for Tuesday:
- TODO Get more data to build LM and write scripts to process this data.

* DAR <2017-01-19 Thu>
**  Goals for Thursday set Wednesday:
- Todo Reproduce the    gp results for models beyond monophones from the kaldi repo.
The best on tri1 dev:
%WER 36.70 [ 8182 / 22297, 2382 ins, 478 del, 5322 sub ]


- Taking a step back.
Steve found language models for global phone.
Justin put the Bremen 3gram lm for French under:
/mnt/corpora/Globalphone/gp/LMs/FR.3gram.lm.gz
I rebuilt the  lang fst with this lm and restarted the building steps.
- Mono results:
dev:
%WER 58.61 [ 13069 / 22297, 1181 ins, 2271 del, 9617 sub ]
eval:
%WER 56.44 [ 12247 / 21698, 1154 ins, 1917 del, 9176 sub ]

So forget it.
- Take another step back.
I'm going to move forward with the gp training prompts as the data for training the lm.

** Goals for Monday:
- TODO Build our own lm.
- TODO Continue building system with lm trained on gp training prompts.

* DAR <2017-01-18 Wed>
** Goals for Wednesday set Tuesday:
- DONE Meet with Jacquin to work on ASR for the voxtec device.
We got the test set and transcripts to Jacq
- TODO Reproduce the rest of the gp results from the kaldi repo.
There were problems building the LMs.
The srilm version seems to work on the gpu machine, but not on my laptop.
I ran the test using the standard lm and the enhanced sri lm.
the enhanced sri lm version gives slightly better results.
41.16 versus 40.98
- TODO Contact Bogdan Vlasenko (the person who got the kaldi repo results)

** Goals for Thursday:
- Todo Reproduce the    gp results for models beyond monophones from the kaldi repo.
* DAR <2017-01-17 Tue>
**  Goals for Tuesday set Friday:
- DONE Reproduce the results in the kaldi gp repo.

Why are my WER results for mono so bad compared to the results published in the kaldi gp repo?
They report 41.80 my best so far is 49.05.
I'm going back to the lm preparation scripts.

I made 3 changes:
1. I added the dev and test prompts to the lm training corpus. I doo not really want to do this since it feels like cheating.
2. I used the utils/format_lm_sri.sh  script to convert the lm to an fst.
3.  I also used the irstlm pruning commands.  

OK: I achieved some lower WERs.
Eval low: 
%WER 41.14 [ 8927 / 21698, 1206 ins, 1305 del, 6416 sub ] 
Dev low:
43.88
these are lower than the 41.80 and 45.69 that in the gp repo results.
I'll try again without the test and dev prompts in the  lm
Here is the srilm command to build the lm:
ngram-count \
    -order 3 \
    -kndiscount \
    -interpolate \
    -unk \
    -map-unk "<UNK>" \
    -limit-vocab \
    -text $corpus \
    -lm $lmthreegram || exit 1;

Then this command gets run:
utils/format_lm_sri.sh \
    --srilm-opts "-subset -prune-lowprobs -unk -tolower" \
    data/lang \
    language_models/lm_threegram.arpa.gz \
    data/local/dict/lexicon.txt \
    data/lang_sri

Then the following irstlm command is run on the original lm data, not the results of the previous command.
prune-lm \
    --threshold=1e-7 \
    language_models/lm_threegram.arpa.gz \
    /dev/stdout | \
    gzip -c > \
	 data/local/lm/lm_threegram.arpa.gz

Finally, the following irstlm command is run on what looks like the output from both the previous commands:
utils/format_lm.sh \
    data/lang_sri \
    data/local/lm/lm_threegram.arpa.gz \
    data/local/dict/lexicon.txt \
    data/lang_test

In the training I am using the data/lang_test directory.

Do I really need the 2 irstlm commands to get the low WERs?

- The dev and test prompts really make a difference.
The best for eval without the dev and test prompts is 56.07
The best for dev without the dev and test prompts is 58.54

I think they used the dev and test prompts in their lm.
 
** Goals for Wednesday:
- TODO Meet with Jacquin to work on ASR for the voxtec device.
- TODO Reproduce the rest of the gp results from the kaldi repo.
- TODO Contact Bogdan Vlasenko (the person who got the kaldi repo results)

* DAR <2017-01-13 Fri>
** Goals for Friday set Thursday:
- DONE Train the tdnn for the chain models on the gp corpus.
The 73 iterations of training finished after setting mini-batch to 128 instead of 512.

I am now running the mkgraph script to make the decoding fst.
it took several hours for the mkgraph program to finish.
I decoded the eval data set.
The best WER was 42.29
Something must be wrong.
The results from the gp kaldi directory gives WERs in the low 20s.
My best was a 38.78 for the sgmm system.



I'm going to try to get their results for the mono system before moving on.

** Goals for Tuesday:
- TODO Reproduce the results in the kaldi gp repo.

* DAR <2017-01-12 Thu>
**  Goals 
- TODO build a chain model on top of the gp cd gmm hmm.
I'm starting to work on this.
I have so far built the cd gmm hmm on the gp data.
I'm starting the process of building a chain model system.
1. lattice alignments
2. Generate topology 
From the script:
Generate a topology file.  
This allows control of the number of states in the non-silence HMMs, and in the silence HMMs.  
This is a modified version of 'utils/gen_topo.pl' that generates a different type of topology, one that we believe should be useful in the 'chain' model.  
Note: right now it doesn't have any real options, and it treats silence and nonsilence the same.  
The intention is that you write different versions of this script, or add options, if you experiment with it.

3. Build a tree
This script builds a tree for use in the 'chain' systems (although the script itself is pretty generic and doesn't use any 'chain' binaries).  
This is just like the first stages of a standard system, like 'train_sat.sh', except it does 'convert-ali' to convert alignments to a monophone topology just created from the 'lang' directory (in case the topology is different from where you got the system's alignments from), and it stops after the tree-building and model-initialization stage, without re-estimating the Gaussians or training the transitions.

4. Train a diagonal ubm.
5. Train an i-vector extractor.
6. Speaker Info
7. Extract I-vectors
8. train tdnn  on gpu.
This was failing on iteration 10.
I removed all the non-default options and it passed iteration 10.
It got to iteration 19 and crashed.
It looks like the GPU ran out of memory.
I am trying again with mini-batch set to 128 instead of 512.
** Goals for Friday:
- TODO Train the tdnn for the chain models on the gp corpus.


* DAR <2016-12-09 Fri>
**  Goals for Friday set Thursday:
- TODO Restart the gp cd gmm hmm build,
Steps done:
4. dict prep,
5. lang prep,
6. convert lm to fst,
7.  prune lm,
8. convert pruned lm to fst,
9.   extract MFCC features,
10. Train monophones,
exp/mono: nj=8 align prob=-101.34 over 22.74h [retry=0.5%, fail=0.0%] states=118 gauss=992
Does this mean 22.74 hours of training data?
14. align with mono,
15. train tri1,
-Good News:
The tri1 training is not showing the warnings about phones not being associated with data.
exp/tri1: nj=8 align prob=-96.77 over 22.73h [retry=0.5%, fail=0.0%] states=2998 gauss=50126 tree-impr=5.11
16. align with tri1,
20. train tri2a,
exp/tri2a: nj=5 align prob=-97.60 over 22.73h [retry=0.6%, fail=0.0%] states=1000 gauss=20047 tree-impr=4.88
24. Train tri2b,
tri2b also uses alignments from tri1.
tri2b does lda and mllt training
# LDA+MLLT refers to the way we transform the features after computing
# the MFCCs: we splice across several frames, reduce the dimension (to 40
# by default) using Linear Discriminant Analysis), and then later estimate,
# over multiple iterations, a diagonalizing transform known as MLLT or CTC.
# See http://kaldi-asr.org/doc/transform.html for more explanation.
exp/tri2b: nj=5 align prob=-49.03 over 22.73h [retry=0.5%, fail=0.1%] states=4972 gauss=75122 tree-impr=5.88 lda-sum=26.25 mllt:impr,logdet=1.09,1.82
28. align with tri2b,
29. train tri3b
# This does Speaker Adapted Training (SAT), i.e. train on
# fMLLR-adapted features.  It can be done on top of either LDA+MLLT, or
# delta and delta-delta features.  If there are no transforms supplied
# in the alignment directory, it will estimate transforms itself before
# building the tree (and in any case, it estimates transforms a number
# of times during training).
exp/tri3b: nj=5 align prob=-48.63 over 22.72h [retry=0.5%, fail=0.1%] states=5994 gauss=75100 fmllr-impr=2.85 over 19.88h tree-impr=8.68
33. align with tri3b
34. train ubm4,
# This trains a UBM (i.e. a mixture of Gaussians), by clustering
# the Gaussians from a trained HMM/GMM system and then doing a few
# iterations of UBM training.
# We mostly use this for SGMM systems.
35. train sgmm2b
# SGMM training, with speaker vectors.  This script would normally be called on
# top of fMLLR features obtained from a conventional system, but it also works
# on top of any type of speaker-independent features (based on
# deltas+delta-deltas or LDA+MLLT).  For more info on SGMMs, see the paper "The
# subspace Gaussian mixture model--A structured model for speech recognition".
# (Computer Speech and Language, 2011).
39. align with sgmm2_4a,
40. Make sgmm2 denominator lattices,

- TODO Write a good set of goals for when I return from leave.

As I'm getting ready to leave, the script to make sgmm2 denominator lattices is running.

For my goal of building chain models with the gp corpus, I'm basically done with this part of the project.
The chain model recipe builds on the cd gmm hmm.

** Goals for Next Year
- TODO build a chain model on top of the gp cd gmm hmm.
 
* DAR <2016-12-08 Thu>
**  Goals for Thursday set Wednesday:
- ToDO Restart the gp+yaounde with aligned speakers.
- TODO After finishing the gp cd gmm hmm build, start the chain model build on top of it.
 I am splitting up the gp kaldi recipe into 1 step per command.
Steps done:
1. Get tools (sox, probably not needed),
2. Prompts prep,
3. data prep,
4. dict prep,
5. lang prep,
6. convert lm to fst,
7.  prune lm,
8. convert pruned lm to fst,
9.   extract MFCC features,
10. Train monophones,
11. make decoding graph for mono system,
12. decode dev with mono,
13. decode eval with mono system,
14. align with mono,
15. train tri1,
16. align with tri1,
17. train tri2a,
18. 

- Problem:
I see warnings in step 15 about phones not being associated with data.
I found the source of the problem.
The gp scripts suck.
They do not handle utf8 encoding well.
The script that normalizes the dictionary is to blame.
I fixed the problem.
I wrote new scripts that write the lexicon with good utf8.
I guess this was hard when this recipe was first written.
My solution is to use the Encode module and the decode_utf8 function.
The decode_utf8 function is a one line solution.
  
I have to start the training all over again.

** Goals for Friday:
- TODO Restart the gp cd gmm hmm build,
- TODO Write a good set of goals for when I return from leave.

* DAR <2016-12-07 Wed>
**  Goals for Wednesday set Tuesday:
- DONE Make sure the lang_test directory is correct in the gp cd gmm hmm system.
It looks like there was a bug in an older version of the kaldi format_lm_sh script.
I I was using the newer one on my laptop.
It copies the topo file over to the lang_test directory.

- DONE Restart the gp cd gmm hmm system.
I am running into a strange sorting problem.
The following 2 line are out of order:
ctell1-01-001	ctell1-01
ctell1-01-0010 ctell1-01
Ah! it's the space between the fields. One is a tab and the other is a space.
I am also running the decoding on the answers again. 
For some reason the scoring failed before.
To do the semi supervised training, I use the transcripts from the decoding with the boosted mmi trained sgmms.
The transcripts that were available were done with the old naming.
Actually, here is where the difference really should happen.
The answers and read parts of the yaounde corpus have their speakers are now aligned.
The effect of this should appear in the semi supervised training.

- DONE Run the semi supervised gp+yaounde system with aligned speakers.
I had to rebuild the data/train_semi_supervised directory.
I also need to extract features from the sri_gabon_read data.

** Goals for Thursday:
- ToDO Restart the gp+yaounde with aligned speakers.
- TODO After finishing the gp cd gmm hmm build, start the chain model build on top of it.
 
* DAR <2016-12-06 Tue>
**  Goals for Tuesday set Monday:
- DONE Get the gp+yaounde cd gmm hmm system running again with the new speaker aligned directories.
I worked a lot on writing the yaound answers and sri gabon read data preparation scripts.
As I'm getting ready to leave, I started the script to run the semi supervised part of this system build.
- TODO Get the gp cd gmm hmm system running again.
I'm separating the run.sh script into scripts for each command.
install tools
get prompts
prepare data
prepare dictionary
prepare lang
convert lm to fst
create pruned lm ( this is done with irstlm)
convert pruned lm to fst
extract mfcc features
train cd gmm hmm

I thought I had all of this running today.
As I'm getting ready to leave I found a bug.
The topo file under the lang_test directory is missing.
I'm not sure why.
The script utils/format_lm.sh has a for loop that copies this file over to from the lang directory to the lang_test directory.
Why did the topo file not get copied over?
This file is used to initialize the monophone system.

** Goals for Wednesday:
- TODO Make sure the lang_test directory is correct in the gp cd gmm hmm system.
- TODO Restart the gp cd gmm hmm system.
- TODO Run the semi supervised gp+yaounde system with aligned speakers.
 
* <2016-12-05 Mon>
**  Goals for Monday set Friday:
- TODO Run standard kaldi gp recipe.
The run.sh script fails when invoking the gp_format_lm.sh script.
This script converts the arpa lm into an fst.
The gp_format_lm.sh script runs srilm via a script called utils/format_lm_sri.sh.
That script says that you can convert an arpa lm to an fst with another script called format_lm.sh.
I'm running that script now.
The srilm script apparently restricts the vocabulary to the intersection of ...
the words in the training data for the lm and the words in the pronouncing dictionary.
This actually might be a cool thing, if it worked.
Srilm is failing for some reason.
When I use the script that does not use srilm, it succeeds in formatting the lm as an fst.

- TODO Run kaldi chain model on gp.
- DONE Fix answers data and continue building gp+yaounde cd gmm hmm system.
I wrote scripts specific for yaounde answers data.
The run.sh script is not crashing where it crashed last week.

I got stuck again on the answers data.
This time it was the text file.
It's a nightmare. 
The file was being appended to each time I ran the utt2text writer.
As a work around, I am deleteing any file with prefix yaounde_answers_ in the data/local/tmp directory.

** WAR:
John Morgan continued working on a project to adapt French Speech Recognition models to African accented speech.
He is taking a step back and training a baseline context dependent gaussian mixture model hidden markov model (CD GMM HMM) system only on a reference corpus of European French. 
The models in his previous systems were trained on data that included African Accented speech. 
After completing the baseline system, he will train chain models on top of the cd gmm hmm and perform adaptation experiments with i-vectors. 

** Goals for Tuesday:
- TODO Get the gp+yaounde cd gmm hmm system running again with the new speaker aligned directories.
- TODO Get the gp cd gmm hmm system running again.

* DAR <2016-12-02 Fri>
** Goals for Friday set Thursday:
- DONE Setup dictionary preparation for the standard kaldi gp fr recipe. 
- DONE Setup lang preparation for the standard kaldi gp fr recipe. 

I ran out of space on the GPU machine.
I deleted a lot of directories to free up some space.

- gp+yaound cd gmm hmm rerun with aligned speaker directories.
The script crashes when trying to decode the yaounde answers with boosted mmi sgmm.
I have not figured out why this is happening.
I see that decoding with mono, tri1, tri2, tri3, tri4, tri5, sgmm and sgmm fmllr all succeed.
Why is decoding the yaounde answers failing with boosted mmi trained sgmm?
The data/answers/text file is empty.

** Goals for Monday:
- TODO Run standard kaldi gp recipe.
- TODO Run kaldi chain model on gp.
- TODO Fix answers data and continue building gp+yaounde cd gmm hmm system.
 
* DAR <2016-12-01 Thu>
**  Goals for Thursday set Wednesday:
- DONE Make prompt lists for dev, eval, and train folds of gp data.
- DONE Run norm script on fold prompt lists

I spent the whole day working on the data preparation steps for the standard gp fr recipe.
I think I'm finished with the data preparation step.
I mostly went back to my gp+yaounde scripts, although I had to modify them to fit the dev, eval and train folds for the kaldi gp recipe.
I now have wav.scp, text, utt2spk and spk2utt files.
The kaldi gp recipe assumed these files already existed.
Writing these files is a large part of the work involved in building a kaldi system. 

As I'm getting ready to leave, I'm working on the gp_dict_prep.sh script
** Goals for Friday:
- TODO Setup dictionary preparation for the standard kaldi gp fr recipe. 
- TODO Setup lang preparation for the standard kaldi gp fr recipe. 

* DAR <2016-12-02 Fri>
* DAR <2016-11-30 Wed>
**  Goals for Wednesday:
- TODO Setup and build the standard gp cd gmm hmm system with the kaldi recipe
- Data preparation
I have lists of the dev, eval, and train speaker directories.
Next step:
get prompt lists for each of these folds.
run the normalization script from the kaldi gp egs recipe scripts on each fold list.
hopefully the rest of the recipe will work from that point on.
- TODO build chain models for the gp system

- gp+yaounde cd gmm hmm withnew naming
The recipe script crashed when trying to make the the decoding graph.
The g.fst was missing.
Sure enough, the commands for making the g.fst are missing from the run.sh script.
This involves the lm.
Replacing epsilon and <s> and </s> symbols
Finding  and removing oovs.
Compiling the fst into binary format
Sorting the fst
I think I have the script running now.

** Goals for Thursday:
- TODO Make prompt lists for dev, eval, and train folds of gp data.
- TODO Run norm script on fold prompt lists

* DAR <2016-11-29 Tue>
** Goals for Tuesday set Monday:
- TODO rebuild the gp+yaounde sgmm system with the new yaounde naming.
I saw these stats on the training trace after monophone training finished:
states=115 gauss=988
tri1:
states=821 gauss=10030 tree-impr=4.18
tri2:
states=831 gauss=20038 tree-impr=4.50
tri3:
states=4891 gauss=75119 tree-impr=5.16
tri4:
states=5079 gauss=75101 tree-impr=6.19 lda-sum=28.81 mllt:impr,logdet=1.05,1.77
tri5:
states=5120 gauss=75078 fmllr-impr=2.92 over 28.77h tree-impr=8.88

The system build had failed at the denominator lattice training.
I saw that the graph construction had been skipped because exp/sgmm5_denlats/dengraph/HCLG.fst already existed.
I put in a command to delete exp/sgmm5_denlats/dengraph/HCLG.fst and rerun.
It seems to have gotten passed the problem for now.

- TODO build the an4 chain model toy system.
Steps:
1. Data prep including utt2spk and spk2utt files 
2. Feature extraction MFCCs in this case
3. lang preparation
4. train context dependent gaussian mixture model hidden markov model (cd gmm hmm)
I'm going to use the term cd gmm hmm from now on to refer  to this kind of system.
5. build a tree. This is standard, but I do not know enough about it to explain it. Is it different from the fst decoding graph? 
6. train a universal background model (ubm).
7. train an i-vector extractor.
8. extract i-vectors for training data.
It looks like an i-vector is extracted for each recording file.
9. Train a chain model temporal delay neural network (tdnn)
10. Extract test i-vectors.
11. Decode.
I have succeeded in all these steps on my laptop, but the training step is failing on the GPU machine.
The WER is 20% compared to 6% for the cd gmm hmm system.

Do I move on to setting up chain models for a real corpus like gp and  gp+yaounde  or do I fix the problem on the GPU?

- DONE write a toy cfg example with nltk to demo for Luis

- Jamal's data
I asked Justin to put it under /mnt/corpora
I'll process it after I've built the new models.

As I'm getting ready to leave the gp+yaounde cd gmm hmm system build with the new naming is doing boosted mmi train of sgmm models.

** Goals for Wednesday:
- TODO Setup and build the standard gp cd gmm hmm system with the kaldi recipe
- TODO build chain models for the gp system

* DAR <2016-11-28 Mon>
** Goals for Wednesday set Tuesday:
- TODO Run gp+yaounde semi supervised sgmm system build and test with new file naming.
The recipe script was getting stuck on the lang directory building.
I removed the lang directory and it seems to be running now.

- TODO Take a pass on TR.
- TODO Decode Jamal's  recordings.
- TODO Read adaptation and iVector papers. 

I only came in to work for a short while. 
I had 2 health related appointments.
My upper GI imaging gave me some good news. 
As expected, I have a hernia, but it is very small, only the size of an almond.

- Incorporating Steve's corrected Niger transcripts into gp+yaounde cd gmm hmm system.

- an4 chain models
I want to investigate i-vectors.
The chain model recipes use i-vectors.
I am starting first with a toy system.
I am starting with the an4 recipe.
This is a relatively new recipe in the kaldi toolkit.
The data is free and there is a script to download it.
The recipe is very basic.
I am running the chain model recipe from the rm system  with the an4 data.
It would be nice to have the rm corpus and run the rm chain model recipe on the rm corpus.

** WAR
John Morgan is exploring the use of i-vectors as a method for adapting French speech recognition models  to accents from regions of Africa.
He is building a toy chain model system that uses a hybrid of a Deep Neural Network and a GMM HMM system together with i-vectors. 

** Goals for Tuesday:
- TODO rebuild the gp+yaounde sgmm system with the new yaounde naming.
- TODO build the an4 chain model toy system.
- TODO write a toy cfg example with nltk to demo for Luis
* DAR <2016-11-22 Tue>
**  Goals for Tuesday set Monday:
- TODO Setup semi supervised data directory with new renamed corpus.
I had a weird experience again today with the scripts.
For some reason that I still have not figured out, a tab separated values files is appearing with a regular white space instead of the tab character.
I finally just had the script split either on tab or white space and it is now running, but it took me most of the day to get to this point.

- DONE Run semi supervised speaker training experiment with eesen yaounde + gp phone and speaker split answers data.
I now have the eesen training running with semi supervised training data.
As I'm getting ready to leave the training is on epoch 8.
- TODO Run experiment with larger test set.
- TODO Take a pass on tr.

** Goals for Wednesday:
- TODO Run gp+yaounde semi supervised sgmm system build and test with new file naming.
- TODO Take a pass on TR.
- TODO Decode Jamal's  recordings.
- TODO Read adaptation and iVector papers. 
* DAR <2016-11-21 Mon>
**  Goals for Monday set Friday:
- DONE Fix the directory and file naming of the yaounde data to make the speakers in the read and answers parts of the corpus coincide.
I have not run the training scripts with the new naming, so I am not sure I did this correctly yet.
I gave the read and answers parts  of the yaounde corpus the same names for the same speakers.
There is a possible issue here with kaldi.
Kaldi uses the basename of the file as the utterance name.
The danger here  is that the speaker name is the first part of the basename of each file.
The rest of the utterance name is the recording number.

I don't think this will cause a collision because  I allocated 4 digits for the read recording names and only 3 digits for the answers recordings names.
I padded with leading zeroes  in both cases.
For example the file with name 1.wav was given the recording name 001.wav if it was an answer and 0001.wav if it was a read recording.
I ran into problems when I started the semi supervised training.
Here's one of the problems.
When I make the spk2utt file I have to consolidate all the utterances corresponding to one speaker.
This is not as easy as jut concatenating the spk2utt files for the read training data and the answers data.
The utterances come from the same speakers.
It took me a long time to figure out that this was a problem.
Another problem is the sorting issue.
The utterances do not appear in the correct order unless I explicitly sort them.
Anyway, I spent the afternoon working on this problem and I'm still not finished.
I'm pretty sure there are similar issues with the utt2spkand other files.  
- DONE Follow Steve's comments on the Niger corpus data to remove bad recordings.
I sent Justin a message requesting him to remove files.
- DONE Incorporate Steve's transcription of the Niger corpus into system build recipes.
I have not run scripts yet to validate the new transcripts.
- TODO Setup the Speaker test experiment.
Justin replaced the Yaounde corpus with the new corpus with renamed directories and files under /mnt/corpora/Yaounde.

** Goals for Tuesday:
- TODO Setup semi supervised data directory with new renamed corpus.
- TODO Run semi supervised speaker training experiment with eesen yaounde + gp phone and speaker split answers data.
- TODO Run experiment with larger test set.
- TODO Take a pass on tr.

* DAR <2016-11-18 Fri>
**  Goals for Friday set Thursday:
- DONE Doctors appointment at 9:30
Complete waste of time.
- TODO map yaounde answers directory names to match read directory names.
This is very tedious.
- TODO Setup new experiment.
I need to align the speakers first.
- TODO Take a pass on the tr.

** Goals for Monday:
- TODO Fix the directory and file naming of the yaounde data to make the speakers in the read and answers parts of the corpus coincide.
- TODO Follow Steve's comments on the Niger corpus data to remove bad recordings.
- TODO Incorporate Steve's transcription of the Niger corpus into system build recipes.
- TODO Setup the Speaker test experiment.
* DAR <2016-11-17 Thu>
** Goals for Thursday set Wednesday:
- TODO Setup new experiment for TR
I realized when working on this that I was very sloppy about naming the directories for the yaounde answers.
In order to perform the experiment, I have to align the speakers in the read data and the answers data.
My current naming of the speaker directories does not do this.
ctell1 speakers 1 through 17 should be good.
Things go haywire after that.
I am going back to the original data collection directories and aligning the read answers speaker directories.
I am writing a list of the renaming that needs to be preformed.
Here are the first couple of directory renames.
ctell2_17 -> ctell2_18
ctell1_18 -> ctell2_19
ctell2_01 -> ctell2_20
ctell2_02 -> ctell2_21

I think this pattern will hold for ctell2
- TODO Decode with eesen gp+younde phone models  (assuming it finishes tonight)
When I arrived in the morning epoch 17 was running. 
EPOCH 16 RUNNING ... ENDS [2016-Nov-17 01:50:13]: lrate 3.125e-07, TRAIN ACCURACY 95.4533%, VALID ACCURACY 86.4480%
The training finished:
EPOCH 18 RUNNING ... ENDS [2016-Nov-17 06:34:24]: lrate 7.8125e-08, TRAIN ACCURACY 95.4285%, VALID ACCURACY 86.6688%
finished, too small rel. improvement .0511
Training succeeded. The final model exp/train_phn_l4_c320/final.nnet
Removing features tmpdir /tmp/tmp.vZV7b1RFLz @ A-TEAM19054
cv.ark
train.ark

I had to do some digging to figure out how scoring works in the eesen scripts.
The WER ended up as 44.13 for 3gram and 43.48 for 4gram.
I'm not sure yet if this is good.
This is a system trained on yaounde read and gp.
No semi supervised training.


- TODO Doctor's appointment in the morning.
Today's appointment is actually at 11:30.
Tomorrow's appointment is at 9:30
- TODO Take a pass on the tr.

** Goals for Friday:
- TODO Doctors appointment at 9:30
- TODO map yaounde answers directory names to match read directory names.
- TODO Setup new experiment.
- TODO Take a pass on the tr.

* DAR <2016-11-16 Wed>
** Goals for Wednesday set Tuesday:
- DONE Get the device hypothesis file for only the files in our reference set (currently 532 utterances)
I wrote a script to do this. I called it niger_select_files.pl.
I end up with 531 utterances instead of 532.
I had deleted one line. I restore it and we now have 532 uttereances.

- DONE Run the scoring again for the device output of the 532 reference utterances.
The WER is 61.89.
- TODO Train the eesen gp+yaounde phone system. 
I had started the training with eesen yesterday before leaving and the process is still running.
As I am getting ready to leave It is on epoch 11.
I think it runs for no more than 25 epochs.
So it might still be running tomorrow.

- TODO possibly ask Justin to install eesen system wide.
- TODO Take a pass on the tr.

- Test gp+yaounde tri5-sgmm semi supervised systems on new Niger 532 data set:
I'm running into a problem.
The WER is over 200.
There seems to be a problem with matching the output with the reference transcript during scoring.
I'm trying to run the test on my laptop.
I am copying the minimal components to run the decoder from the GPU machine to  my laptop.
The HCLG.fst file is big.
final.mdl
tree/
graph/words.txt
graph/phones/silence.csl
Forget it ... this bogged down my laptop
I think I solved the problem I described above. I think there were old output files hanging around. 
The test results are slightly worse for the Niger 532 data set than for our previous central accord test set.
The WER for the same system that got 21.75 is:
%WER 22.77 [ 920 / 4041, 110 ins, 158 del, 652 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it4/wer_15_0.0
The semi supervised sgmm models are yielding WERs around 19.
The best WER was:
%WER 18.73 [ 757 / 4041, 85 ins, 133 del, 539 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it1/wer_11_0.0

- Experiment for tr
I am setting up an experiment that I think is missing for our tr.
Split the younde  data into 2 parts A and B. 42 speakers for each part.
1. Method A (our method):
Supervised Training on gp + yaounde read part A  and semi supervised  training on yaounde answers part A.
2. Method b:
Supervised training on gp + younde read part A and semi supervised training on yaounde part b.
So both systems get supervised training on the same corpus, but system A gets semi supervised training on yaounde answers part A while system B get semi supervised training on yaounde answers part B.
The idea is to test the claim that the overlap of speakers gives a boost to the accuracy of the models.  
** Goals for Thursday:
- TODO Setup new experiment for TR
- TODO Decode with eesen gp+younde phone models  (assuming it finishes tonight)
- TODO Doctor's appointment in the morning.
- TODO Take a pass on the tr.

* DAR <2016-11-15 Tue>
**  Goals for Tuesday set Monday:
- TODO Run the gp+yaounde system tests on the new 545 utterances Niger test set.
I started this, but the test set has changed in the mean time.
We now have a niger test consisting of 532 utterances.

- TODO Get the eesen gp+younde phone system setup. Compile FSTs.
I found a bug. My script uses a tab to separate the utterence id from the transcript. An eesen python script that makes numerized label files uses a plain space. I fixed the script and I am using my own version of this script.
I started working on the training.
The training script uses a program called net-initialize.
Apparently, this is a new program in eesen.
I have to recompile the eesen programs.
Maybe I'll ask Justin to put it on the GPU machine under a directory like /home/tools or /usr/local/share.
The GPU was dead.
Justin rebooted the GPU machine for me. 
- TODO Take a pass on the tr.
- DONE package the Niger test set for Jacquin.
I copied the data from /mnt/corpora to /home/data/ateam/

- Scoring the Voxtec device output
I reran the test with  an updated test set (not the latest yet) and the WER was 62.47.

** Goals for Wednesday:
- TODO Get the device hypothesis file for only the files in our reference set (currently 532 utterances)
- TODO Run the scoring again for the device output of the 532 reference utterances.
- TODO Train the eesen gp+yaounde phone system. 
- TODO possibly ask Justin to install eesen system wide.
- TODO Take a pass on the tr.

* DAR <2016-11-14 Mon>
** Goals for Monday set Thursday:
- TODO Prepare niger test for packaging and sending to Jack.
Steve did a pass on the niger recordings.
He got through system 2 speaker 5.
We now have 545 utterances from the Niger corpus with human validation.

- DONE Run our own test on the niger test set.
I ran the test on the current niger data set which consists of 5 speakers.
%WER 14.43 [ 1165 / 8074, 23 ins, 523 del, 619 sub ] exp/sgmm5_semi_supervised_3/decode_test_niger/wer_11_0.0
%WER 18.22 [ 1471 / 8074, 56 ins, 590 del, 825 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it1/wer_11_0.0
%WER 19.22 [ 1552 / 8074, 65 ins, 613 del, 874 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it2/wer_11_0.0
%WER 20.60 [ 1663 / 8074, 52 ins, 688 del, 923 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it3/wer_11_0.5
%WER 21.23 [ 1714 / 8074, 94 ins, 633 del, 987 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_niger_it4/wer_11_0.0
%WER 23.09 [ 1864 / 8074, 95 ins, 688 del, 1081 sub ] exp/sgmm5_semi_supervised_2/decode_test_niger/wer_11_0.5
%WER 23.51 [ 1898 / 8074, 120 ins, 662 del, 1116 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it1/wer_11_0.0
%WER 23.94 [ 1933 / 8074, 141 ins, 626 del, 1166 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it2/wer_10_0.0
%WER 24.37 [ 1968 / 8074, 100 ins, 724 del, 1144 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it3/wer_12_0.5
%WER 24.61 [ 1987 / 8074, 133 ins, 698 del, 1156 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_niger_it4/wer_13_0.0
%WER 26.73 [ 2158 / 8074, 155 ins, 686 del, 1317 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it1/wer_14_0.0
%WER 26.75 [ 2160 / 8074, 202 ins, 630 del, 1328 sub ] exp/sgmm5_semi_supervised/decode_test_niger/wer_12_0.0
%WER 27.37 [ 2210 / 8074, 170 ins, 701 del, 1339 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it2/wer_14_0.0
%WER 27.45 [ 2216 / 8074, 174 ins, 684 del, 1358 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it3/wer_13_0.0
%WER 27.73 [ 2239 / 8074, 134 ins, 758 del, 1347 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_niger_it4/wer_14_0.5
%WER 28.07 [ 2266 / 8074, 188 ins, 715 del, 1363 sub ] exp/tri5_semi_supervised_3/decode_test_niger/wer_15_0.0
%WER 29.29 [ 2365 / 8074, 190 ins, 717 del, 1458 sub ] exp/sgmm5/decode_fmllr_test_niger/wer_13_0.5
%WER 29.32 [ 2367 / 8074, 190 ins, 719 del, 1458 sub ] exp/sgmm5/decode_test_niger/wer_13_0.5
%WER 29.33 [ 2368 / 8074, 196 ins, 719 del, 1453 sub ] exp/tri5_semi_supervised_2/decode_test_niger/wer_18_0.0
%WER 29.60 [ 2390 / 8074, 220 ins, 700 del, 1470 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it1/wer_13_0.0
%WER 30.02 [ 2424 / 8074, 209 ins, 704 del, 1511 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it2/wer_13_0.0
%WER 30.18 [ 2437 / 8074, 198 ins, 731 del, 1508 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it3/wer_14_0.0
%WER 30.29 [ 2446 / 8074, 182 ins, 774 del, 1490 sub ] exp/sgmm5_mmi_b0.1/decode_test_niger_it4/wer_16_0.0
%WER 34.30 [ 2769 / 8074, 260 ins, 836 del, 1673 sub ] exp/tri5_semi_supervised/decode_test_niger/wer_20_0.0
%WER 37.22 [ 3005 / 8074, 303 ins, 843 del, 1859 sub ] exp/tri5/decode_test_niger/wer_20_0.0
%WER 40.10 [ 3238 / 8074, 233 ins, 910 del, 2095 sub ] exp/tri5_semi_supervised_3/decode_test_niger.si/wer_14_1.0
%WER 42.40 [ 3423 / 8074, 291 ins, 898 del, 2234 sub ] exp/tri5_semi_supervised_2/decode_test_niger.si/wer_14_0.5
%WER 49.00 [ 3956 / 8074, 280 ins, 1243 del, 2433 sub ] exp/tri5_semi_supervised/decode_test_niger.si/wer_14_1.0
%WER 52.07 [ 4204 / 8074, 311 ins, 1328 del, 2565 sub ] exp/tri5/decode_test_niger.si/wer_18_1.0

- TODO Debug eesen gp+yaounde char system.
- TODO Setup eesen gp+yaounde phone system.
- TODO Take a pass on tr.

- Score niger test device hyps
I ran the compute-wer program with the test reference transcripts I got last week from Steve against the device hypotheses.
The WER was 78.10.
This is on the 162 transcripts Steve wrote for speakers 1 through 5 on system 1.
I ran it again this evening on the larger 545 utterance Niger test set and the WER went down to 71.83.

- Eesen gp+younde phone system.
I spent a lot of time debugging my scripts for preparing the dictionary, lexicon, and lm for compiling into FSTs.
I'm not sure I have it working yet.
There was a problem that an empty line was creeping into my dictionary after some processing.
 
** Goals for Tuesday:
- TODO Run the gp+yaounde system tests on the new 545 utterances Niger test set.
- TODO Get the eesen gp+younde phone system setup. Compile FSTs.
- TODO Take a pass on the tr.
- TODO package the Niger test set for Jack.

* DAR <2016-11-10 Thu>
** Goals for Thursday set Wednesday:
- DONE Package the niger and central accord test sets if Steve wants them.
- TODO Take a pass on the TR.
- TODO Debug gp+younde eesen char system 
- DONE Attend the Voxtec meeting.

- niger test set
I spent most of the day working on the niger test set.
This was pretty tedious. 
I had Justin delete the files that Steve commented on. 
As I'm getting ready to leave, I'm only working with 5 speakers from system1.
Steve's comments end in the middle of speaker 6.
My scripts depend on retrieving the wav files from the directories.
I have a directory for each speaker.
My scripts would pick up all the files in directory 6 and 7, so I'm going to wait until I know what files will finally be in those directories before processing with them.

** Goals for Monday:
- TODO Prepare niger test for packaging and sending to Jack.
- TODO Run our own test on the niger test set.
- TODO Debug eesen gp+yaounde char system.
- TODO Setup eesen gp+yaounde phone system.
- TODO Take a pass on tr.

* DAR <2016-11-09 Wed>
** Goals for Wednesday set Tuesday:
- TODO Train eesen gp+yaounde system (work on 1lang.sh and python scripts).
I got the python scripts to run.
They make the label files.
I updated the command lines from the latest eesen in the repo.
Training fails. I'm not sure why.
- TODO another pass on the tr. 
- Speech package for voxtec
I spent the day putting together the wav data and labels for the Yaound Answers and Read dataset and the Central Accord data.
The package is at:
/home/data/ateam/African_speech
on the GPU machine.

I  included neither the central accord test set nor the niger test.
I could do those tomorrow morning if needed.
I probably want to delete the 12 files Steve commented on.
I think we should delete them from the /mnt/corpora disk.

** Goals for Thursday:
- TODO Package the niger and central accord test sets if Steve wants them.
- TODO Take a pass on the TR.
- TODO Debug gp+younde eesen char system 
- TODO Attend the Voxtec meeting.

* DAR <2016-11-08 Tue>
** oals for Tuesday set Monday:
- DONE Vote
- DONE Run yaounde + gp systems tests only on niger corpus.
It turns out I don't have to do this.
I don't have reference transcripts yet, so I can't get real scores.
Below I included the WER scores taken between the output of our system and the device system.
I'm running the tests again anyway with only the niger data to check if the reference labels line up.
- TODO Train the yaound+gp eesen char system with the lm and lexicon I prepared today.
I made some progress on this.
As I'm getting ready to leave, I am trying to get the lang preparation script to run.
I have to get 2 python scripts working in a script I have called 1lang.sh.
The next step is training.
- DONE take another pass on the tr.
Steve wrote more on the lm section.
I made some changes in the intro and discussion sections.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.

- Yaound + gp systems test on central accord + niger test set:
%WER 48.29 [ 5740 / 11887, 415 ins, 1516 del, 3809 sub ] exp/sgmm5_semi_supervised_3/decode_test_central_accord+niger/wer_11_0.0
%WER 48.57 [ 5773 / 11887, 407 ins, 1528 del, 3838 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it1/wer_10_0.0
%WER 48.61 [ 5778 / 11887, 433 ins, 1501 del, 3844 sub ] exp/sgmm5_semi_supervised_2/decode_test_central_accord+niger/wer_10_0.0
%WER 48.80 [ 5801 / 11887, 388 ins, 1587 del, 3826 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it2/wer_11_0.0
%WER 48.80 [ 5801 / 11887, 413 ins, 1535 del, 3853 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it1/wer_10_0.0
%WER 48.99 [ 5824 / 11887, 370 ins, 1652 del, 3802 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it2/wer_14_0.0
%WER 49.02 [ 5827 / 11887, 398 ins, 1602 del, 3827 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it3/wer_11_0.0
%WER 49.03 [ 5828 / 11887, 399 ins, 1592 del, 3837 sub ] exp/sgmm5_semi_supervised_3_mmi_b0.1/decode_test_central_accord+niger_it4/wer_11_0.0
%WER 49.07 [ 5833 / 11887, 380 ins, 1620 del, 3833 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it3/wer_10_0.5
%WER 49.10 [ 5836 / 11887, 335 ins, 1722 del, 3779 sub ] exp/tri5_semi_supervised_3/decode_test_central_accord+niger/wer_16_1.0
%WER 49.13 [ 5840 / 11887, 375 ins, 1659 del, 3806 sub ] exp/sgmm5_semi_supervised_2_mmi_b0.1/decode_test_central_accord+niger_it4/wer_14_0.0
%WER 49.50 [ 5884 / 11887, 408 ins, 1592 del, 3884 sub ] exp/tri5_semi_supervised_2/decode_test_central_accord+niger/wer_18_0.0
%WER 49.64 [ 5901 / 11887, 354 ins, 1644 del, 3903 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it2/wer_15_0.5
%WER 49.64 [ 5901 / 11887, 359 ins, 1645 del, 3897 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it1/wer_15_0.5
%WER 49.73 [ 5911 / 11887, 389 ins, 1601 del, 3921 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it3/wer_13_0.5
%WER 49.83 [ 5923 / 11887, 363 ins, 1647 del, 3913 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_central_accord+niger_it4/wer_15_0.5
%WER 49.89 [ 5930 / 11887, 363 ins, 1653 del, 3914 sub ] exp/sgmm5_semi_supervised/decode_test_central_accord+niger/wer_15_0.5
%WER 51.79 [ 6156 / 11887, 450 ins, 1651 del, 4055 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it2/wer_12_0.5
%WER 51.84 [ 6162 / 11887, 447 ins, 1655 del, 4060 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it1/wer_12_0.5
%WER 51.87 [ 6166 / 11887, 502 ins, 1598 del, 4066 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it3/wer_13_0.0
%WER 52.03 [ 6185 / 11887, 482 ins, 1635 del, 4068 sub ] exp/sgmm5_mmi_b0.1/decode_test_central_accord+niger_it4/wer_14_0.0
%WER 52.19 [ 6204 / 11887, 537 ins, 1580 del, 4087 sub ] exp/sgmm5/decode_fmllr_test_central_accord+niger/wer_14_0.0
%WER 52.23 [ 6209 / 11887, 541 ins, 1542 del, 4126 sub ] exp/sgmm5/decode_test_central_accord+niger/wer_13_0.0
%WER 53.25 [ 6330 / 11887, 454 ins, 1771 del, 4105 sub ] exp/tri5_semi_supervised/decode_test_central_accord+niger/wer_18_0.5
%WER 54.05 [ 6425 / 11887, 365 ins, 1826 del, 4234 sub ] exp/tri5_semi_supervised_3/decode_test_central_accord+niger.si/wer_15_1.0
%WER 54.58 [ 6488 / 11887, 367 ins, 1926 del, 4195 sub ] exp/tri5_semi_supervised_2/decode_test_central_accord+niger.si/wer_19_1.0
%WER 54.95 [ 6532 / 11887, 414 ins, 1906 del, 4212 sub ] exp/tri5/decode_test_central_accord+niger/wer_18_1.0
%WER 62.02 [ 7372 / 11887, 571 ins, 2010 del, 4791 sub ] exp/tri5_semi_supervised/decode_test_central_accord+niger.si/wer_13_0.5
%WER 63.31 [ 7526 / 11887, 570 ins, 2077 del, 4879 sub ] exp/tri5/decode_test_central_accord+niger.si/wer_13_1.0

The WER scores range from 63.31 to 48.29

** Goals for Wednesday:
- TODO Train eesen gp+yaounde system (work on 1lang.sh and python scripts).
- TODO another pass on the tr. 
* DAR <2016-11-07 Mon>
**  Goals for Monday set Friday:
- TODO Take another pass on the tr
- TODO Run gp+yaounde on niger corpus and compare it to the s2s device transcripts.
This is taking longer than I thought.
I spent most of the day writing the data prep script for the central_accord + niger test data set.
One problem was that the prompts files that I write are concatenated, so I have to delete them before I write them.
As I'm getting ready to leave, it looks like I have this script running.
I'm repeating the test  I ran before with the gp+yaounde system on the concatenation of the central accord and niger test data sets.
I realize now that what I really want is to only test on the niger data.
I'll let this run and I'll change it tomorrow.
- TODO Setup eesen on gp + yaounde training set.
I spent time working on training the lm
I had a bug that I just finally fixed, I was missing the final e on the word yaounde in one of my scripts.
I'm only doing the between 6 and 20 tokens restriction for this lm. 
I also worked on the lexicon for eesen char. 
I think this takes all the words in the training corpus-- which includes subs -- and makes a word to character map for each.
It looks like this ran successfully.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.


I got a flu shot today.
I also got some blood extracted.
I guess they'll test it for cholesterol and sugar. 
My blood pressure was 125 over 87 ( a little high).
It was 147 over 90 the first time they tried.
** Goals for Tuesday:
- TODO Vote
- TODO Run yaounde + gp systems tests only on niger corpus.
- TODO Train the yaound+gp eesen char system with the lm and lexicon I prepared today.
- TODO take another pass on the tr.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.
- TODO Another pass on the tr.

* DAR <2016-11-04 Fri>
** Goals for Friday set Thursday:
- DONE  Incorporate Steve's section on the Lexicon into my latex version of the tr.
There might be some problems with accents and apostrophes.
- TODO Take another pass on the tr.
- DONE Setup eesen scripts for gp+yaounde.
I got through the basic data preparation step, including the Niger corpus.
I even extracted the filterbank features.

** Goals for Monday:
- TODO Take another pass on the tr
- TODO Run gp+yaounde on niger corpus and compare it to the s2s device transcripts.
- TODO Setup eesen on gp + yaounde training set.
- TODO Do speaker independent or country dependent version of gp+younde sgmm system.
- TODO Investigate what went wrong with discriminative training of pretrained dnn nnet system.

* DAR <2016-11-03 Thu>
**  Goals for Thursday set Wednesday:
- TODO Put the Niger corpus in kaldi format.
I spent the whole afternoon doing this.
I have listened a second time to all the recordings in the Niger West African corpus.
I had skipped some files yesterday.
There are a couple of recordings with a rooster in the background.
I've renamed the files and directories as I had planned yesterday.
I also put the device transcripts for each speaker in their directory.
It seems like there might be some directories where there are more transcripts than recordings.
But I feel pretty confident that there is some kind of transcript for each recording that I kept in the directories.
I separated out more noise files and files with Eddies voice.
There was one recording of only a cough.
There were a couple of recordings where the speaker spoke in English.
I separated out those too.


- TODO Setup eesen for gp+yaounde 
no work on this today.

- TODO Given Steve's identification of the speakers in the sri_gabon corpus that are in the test set, remove these speakers from the training data corpus.
I spent the morning working on this.
I had to do a search of my own.
The names we had given to the speakers in the test set did not align with the sri_gabon speakers names.
There was a case or two where the country was wrong.
There is a speaker that is not in the sri_gabon directory.
Justin moved the data with the new names to  the /mnt/corpora/central_accord.
He also moved the directories  containing the test data from the sri_gabon directory to a directory called held_out.

** Goals for Friday:
- TODO  Incorporate Steve's section on the Lexicon into my latex version of the tr.
- TODO Take another pass on the tr.
- TODO Setup eesen scripts for gp+yaounde.
** Goals for Monday:
- TODO Attend meeting with Voxtec.

* DAR <2016-11-02 Wed>
** Goals for Tuesday set Monday:
- DONE Go to hospital for endoscopy
Need to have hernia surgery
- TODO Data preparation for eesen gp+yaounde.
I got through the basic data preparation for gp and yaounde and the central accord test set. 

- I spent the whole day working with the Niger West African French Corpus.
The plan is to use this as test data.
I've listened at least once to the recordings.
There are transcripts generated by the voxtec device.
This is a special kind of data.
I'm renaming the files and directories to conform to the kaldi requirements.
I've tried to separate the recordings by speaker.
So far I have:
- 7 speakers on device sys1
- 9 speakers on device sys2
- 7 speakers on devide sys3
These are my best guesses right now.
There are several recordings with Eddie's voice that I've place in a separate directory.
Also a couple of files with only noise.

- kaldi format
niger_west_african_fr/sys{1|2|3}/sys{1|2|3}_{[1-9]}_[0123456][0-9][0-9].wav
At least this is my understanding of the kaldi format.
e.g. 
niger_west_african_fr/sys1/sys1_1_001.wav
So this would be the path to the file of the 001 recording for speaker 1 on device sys1.

I just saw Steve's message about the speakers in the test set.
I'll ask Justin to remove these speakers from the training corpous on /mnt/corpora.
I think it would not take much effort to extend the current test set to the entire set of recordings from these speakers.
Maybe Steve has a reason for not doing this.

** Goals for Thursday
- TODO Put the Niger corpus in kaldi format.
- TODO Setup eesen for gp+yaounde 
- TODO Given Steve's identification of the speakers in the sri_gabon corpus that are in the test set, remove these speakers from the training data corpus.

* Team DAR <2016-10-31 Mon>
** Goals for Monday set Friday:
- DONE Finish preparing recipe for gp+yaounde sgmm.
I think this is ready.
- TODO Get Steve up and running with recipe in his environment.
Steve is now a collaborator on my yaounde repo.
- TODO Make a pass on TR.
- TODO nnet2 dnn for gp+yaounde.


- dnn
I've built 2 dnn systems with different methods.
I'm not sure I've reached the best results with these systems.
1. dnn6 Pretrained dbn 
This is my understanding of how to explain the name.
dnn stands for deep neural network
The 6 is there because we build on the tri5 system.
dbn stands for deep belief network.
My understanding is that a deep belief network is another  name for a restricted boltzmann machine.
I think Hinton is to blame for the name.
Pretrain is there because the dbn is used for pretraining.
Here is the comments in the kaldi script: 
train a DNN on top of fMLLR features. 
The training is done in 3 stages,
1) RBM pre-training:
    unsupervised   train stack of RBMs, 
    starting point for frame cross-entropy trainig.
2) frame cross-entropy training:
objective:  classify frames to correct pdfs.
3) sequence-training optimizing sMBR: 
objective:  emphasize state-sequences with better 
frame accuracy w.r.t. reference alignment.

After steps 1 and 2, I decoded the test set and I got a WER of 22.53.
After step 3 I am getting really bad WER scores -- in the 90s.
Something must be wrong.
2. p-norm tri6 nnet2
Best WER: 22.88

3. online nnet2
The best I've done so far is a WER of 53.46 and this is not even real online yet. It apparently is only a simulation of online decoding.

I'm looking into eesen again.
** Goals for Tuesday:
- TODO Go to hospital for endoscopy
- TODO Data preparation for eesen gp+yaounde.

* Team DAR <2016-10-28 Fri>
**  Goals for Friday set Thursday:
- TODO Write tr
I'm ready to had it off to Steve.
- TODO Read more papers on dnn and sgmm accent adaptation.
- TODO Run smbr training on dnn models.
The denominator lattice generation script is still running.
I'm getting ready to leave and it is still running.

I had started the nnet2 training recipe.
I started it from the  tri5_semi_supervised directory so I could compare results with the sgmm system.
The training is on iteration 150 as I'm getting ready to leave.
It went all the way to iteration 159.
This seems like a lot.
And it looks like the training finished.
I'm not sure what is the next step.
** Goals for Monday:
- TODO Finish preparing recipe for gp+yaounde sgmm.
- TODO Get Steve up and running with recipe in his environment.
- TODO Make a pass on TR.
- TODO nnet2 dnn for gp+yaounde.

* Team DAR <2016-10-27 Thu>
** Goals for Thursday set Wednesday:
- TODO Write tr.
- TODO Chec if there is a problem with the gmm online decoding. Why is it so bad?
- TODO Take next step on dnn nnet system. I need to get a unigram lm somehow.
I run the script that makes the unigram lm.
There is a line that checks if the l times g fst is stochastic.
The answer is no.
The script says that this is an error.
I'm not sure this is a problem.
The script was not set to exit with an error status of 1, so I'm not sure if the fst is supposed to be stochastic or not.
Anyway, I'm moving on to make the fst graph.
The fst graph was made.
I am decoding the test set with the unigram lm.
The WER was 68.51.
I think this is run to get many alternative paths in the lattice of hypothesis.
When I ran the decoding yesterday before leaving with the pretrained dnn models the WER was 22.53
now I'm aligning with the pretrained dnn models.
Alignment finished. I see warnings in the log files.
I started denominator  lattice generation.
I think this will take a while.
- TODO Setup nnet2 online training
** Goals for Friday:
- TODO Write tr
- TODO Read more papers on dnn and sgmm accent adaptation.
- TODO Run smbr training on dnn models.
 Team DAR <2016-10-26 Wed>
** Goals for Wednesday set Tuesday:
- TODO Write tr.
- TODO Run hybrid dnn/hmm system with discriminative sequence training.
It is still running.

I read the paper for Karel Vesely's nnet setup.
I also started reading the paper for the nnet2 setup.
It has a section on online system.
- online decoding bad news
 I ran a script that does online decoding for the gmm system.
I started with the tri5_semi_supervised models.
It does some preparation then decodes.
The bad news is that the WER is 100 and 99 percent.
- nnet training
As I'm getting ready to leave the nnet system build is at iteration 14 of training.

** Goals for Thursday
- TODO Write tr.
- TODO Chec if there is a problem with the gmm online decoding. Why is it so bad?
- TODO Take next step on dnn nnet system. I need to get a unigram lm somehow.
- TODO Setup nnet2 online training

* Team DAR <2016-10-25 Tue>
** Goals for Tuesday set Monday:
- DONE Cyber Security Awareness Challenge mandatory training. Try to resume where I left off. I was working on the home security.
Finally got this done thanks to Steve.
- TODO Write TR.
- TODO Start investigating what step to take next: chain models, nnet2, nnet2 online, nnet3 ...
I found some scripts for nnet. 
This is a hybrid dnn/hmm system.
So the dnn replaces the gmm emission probability distribution. 
I've run this before.
Now I'm running it starting on the last tri5 models I produced. 
tri5 semi supervised 3.
Now that I write this, I realize that this is probably not a good idea, since these models have seen the test data.
I really probably should start at tri5 semi supervised.
Yeah ... I messed up.
I am using the semi_supervised_3 models with the original gp training data set.
Before I leave I'll restart the scripts using the correct data and models.

There's a paper by Karel Vesely about this system.
** Goals for Wednesday:
- TODO Write tr.
- TODO Run hybrid dnn/hmm system with discriminative sequence training.

* Team DAR <2016-10-24 Mon>
**  Goals for Monday set last Friday:
- TODO Cyber Challenge Mandatory Training
I've spent the afternoon trying to complete this training.
I am stuck again.

- TODO Read self training papers
- DON Finish work on gp+yaounde  system training and decoding.
The current project is done.
- TODO Show best transcripts to Steve for qualitative  evaluation.
- TODO Write tr

I spent most of the morning cleaning up the gp+yaounde directory.
I put all the scripts that run commands into one run.sh script.

** Goals for Tuesday:
- TODO Cyber Security Awareness Challenge mandatory training. Try to resume where I left off. I was working on the home security.
- TODO Write TR.
- TODO Start investigating what step to take next: chain models, nnet2, nnet2 online, nnet3 ...
* <2016-10-21 Fri>
** Goals for Friday set Thursday:
- DONE Doctor's appointment in the morning.
The doctor scheduled me for an endoscopy on November 1 at 7:30. 
He scheduled a follow up on Friday, November 18 at 9:15.
- TODO <2016-11-01 Tue 07:30> Endoscopy at Laurel 
- TODO <2016-11-18 Fri 09:15> Follow up with Dr. Lawrence
- TODO Cyber Challenge Training <2016-10-24 Mon>
- DONE Last Pass on objectives
- TODO Read self training papers.
- TODO Decode sri_gabon_conv datawith gp+yaounde system and show transcripts to Steve.
- TODO Write tr.
- TODO Get bibtex for tr citations.

** Goals for Monday:
- TODO Cyber Challenge Mandatory Training
- TODO Read self training papers
- TODO Finish work on gp+yaounde  system training and decoding.
- TODO Show best transcripts to Steve for qualitative  evaluation.
- TODO Write tr
* <2016-10-20 Thu>
** Goals for Thursday set Wednesday:
- DONE Take another pass on objectives
- TODO Cyber Challenge mandatory training.
Work on this Tomorrow with Steve
- DONE Writing pass on TR.
I'm getting references in bibtex .
I am finding several very relevant papers that were written a couple of years ago for the babel project.
They deal with semi supervised training a.k.a. self training.
I need to read these papers carefully.
 
- DONE Make decoding graph for gp sgmms.
- TODO Make decoding graphs for gp + yaounde stage 3 system.
I am making  two of them now as I am getting ready to leave.
The tri5 and sgmm ones.
- DONE Decode test data with gp sgmm boosted mmi system to get results to put in report.
Surprisingly, the boosted mmi training and lattice rescoring did not help.
The best WER was 32.84 by the sgmm system.
- TODO Decode sri_gabon_conv data with stage 3 gp + yaounde system.
As I am getting ready to leave, the training script is at the denominator lattice generation step.
Hopefully, this will finish by tomorrow.

I am done with the experiments  that I plan on reporting on with one possible exception.
I want to report speaker independent results.
I can do this for the tri5 systems,but I'm not sure I can do it for the sgmm systems.

After reading some of the semi supervised self training papers, I am wondering if I might change the training regime. 
those papers work with deep neural networks, so I'm eager to move on and consider this work as a baseline.
** Objectives Draft
*** 1. TECHNICAL COMPETENCE
**** ASR Adaptation:
It is not clear that the advances made last year can be implemented in applications that would directly benefit the Army. 
This year I propose to capitalize on last year's successes by investigating ASR models that have well defined pathways to implementation  in speech to speech devices. 
I plan on focusing on developing models that result in software that can be demoed with realtime interaction. 

**** kaldi:

The ASR systems I have built this year are based on HMMs and SGMMs. 
I will consider these systems as baselines for the work I will do using neural network models. 
I will continue developing with the Kaldi ASR toolkit. 
Specifically, I will implement systems with the following models:
Bottle Neck Features
Chain Models
nnet2
nnet3
TDNNs
RBMs
Eesen end to end rnn and lstm models.

i. European French to African accents
ii. Standard Arabic to Tunisian accent.
b. Language Modeling:
i. Dialogue modeling:
A. French
B. Arabic 
ii. Lexicon expansion
A. French
B. Arabic.
C. Dari
2. Machine Translation
a. Variable Computation Graphs

**** Research:
***** Variable Structured computational graphs.
Many models used in NLP applications have a network of connected nodes. 
Training these networks has been restricted to computing weights associated with the connections. 
The topology of the networks has largely remained fixed. 
Lately there have been attempts to develop training methods that change the network topology with each training example. 
I propose to learn to use a toolkit called DyNet (or one like it) that is designed to build systems with variable graph structures. 

I plan on using DyNet or a toolkit similar to it to build a Machine Translation System and to compare its performance with systems built with other reference toolkits like Joshua, Moses, Tensorflow, etc.  
*** 2. COOPERATION

Collaborate with colleagues to write papers that report on advances made in our projects. 
Collaborate with the Basic Research team by contributing speech recognition components to efforts such as the bot language project. 
*** 3. COMMUNICATIONS

Write weekly activity reports to team members to keep them up to date on my work. 
Read and comment on reports made by my team and branch mates.

*** 4. MGMT. OF TIME & RESOURCES

Set aside time during the day to practice some kind of  activity for physical fitness. 
Stay abreast of possible areas where hardware upgrades could improve work efficiency. 
*** 5. CUSTOMER RELATIONS

Establish relationships with MFLTS and CERDEC to remain aware of Army requirements.
Establish contacts with researchers in the ASR and NLP fields. 
Establish contacts with s2s device manufacturers.

*** 6. TECH TRANSITION

Contribute recipes for building ASR systems with our corpora to the MFLTS. 
Transition ASR components and our other products to USA Army Africa and MFLTS.  
*** 7. DIVERSITY: 
Support ARL's diversity initiatives by participating in locally-sponsored diversity training, broad outreach, and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees. 
*** 8. SHARP: 
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all. 
Demonstrate support for the SHARP program by actively participating in required training and other educational programs. 
Intervene and appropriately respond to any instances of sexual harassment or sexual assault and encourage others to do the same.

Third, Pls be sure to include the fixed values for "Wgt Assigned" for DB-3s to total 100: 
40 - 15 - 10 - 15 - 10 -10 
Also, check the box with an X for Tech Competence.

** Goals for Friday:
- TODO Doctor's appointment in the morning.
- TODO Cyber Challenge Training
- TODO Last Pass on objectives
- TODO Read self training papers.
- TODO Decode sri_gabon_conv datawith gp+yaounde system and show transcripts to Steve.
- TODO Write tr.
- TODO Get bibtex for tr citations.
* <2016-10-19 Wed>
** Goals for Wednesday set Tuesday:
- TODO Write objectives
- DONE Decode sri_gabon_conv data with sgmm boosted mmi models
- DONE Start stage 3 of gp+yaounde system
This took all morning.
I found some bugs in the scripts for the previous stage that caused problems in the current stage.
I had a bug in the way I named the directories and files.
When I was working only with the read files, theer was no problem with sorting.
When I added the conv files, the naming caused a problem with sorting. 
I finally got the acoustic model training to start after fixing the data prep scripts.
- DONE Start stage 2 of gp system to get semi supervised results
This was relatively easy.
- TODO Write TR
Made some progress. I wrote a first pass on the abstract.
I filled out some of the arlticle form. It is starting to look like an ARL report.

As I'm getting ready to leave, there are 3 jobs running on the gpu machine:
- stage 3 of semi supervised training on the gp + yaounde system.
I am using the transcripts of the sri_gabon_conv obtained from models trained in stage 2 to train new models that will be used to get another (hopefully better) transcription of the sri_gabon_conv data.
Once this step is finished, I will stop working on this project and move to neural network methods.
- semi supervised training of the gp system.
I am using the answers transcripts obtained in the first stage of training the gp system.
I am only doing this for completeness. 
We need the results to show that ...
Well ...
that collecting the read   part of the corpus makes a difference in WER scores.
- Tri5 Decoding graph for gp system.
The gp system training is passed the tri5 stage, so I can start generating the decoding graph for the tri5 models.
I'll need this graph for decoding with the tri5 models and the sgmm models. 
** Goals for Thursday:
- TODO Take another pass on objectives
- TODO Cyber Challenge mandatory training.
- TODO Writing pass on TR.
- TODO Make decoding graph for gp sgmms.
- TODO Make decoding graphs for gp + yaounde stage 3 system.
- TODO Decode test data with gp sgmm boosted mmi system to get results to put in report.
- TODO Decode sri_gabon_conv data with stage 3 gp + yaounde system.
* <2016-10-18 Tue>
** Goals for Tuesday set Monday:
- TODO Write Objectives
I've been procrastinating on this.
- DONE Finish training gp only system.
The first stage is done training.
I have decoded the test data with the tri5 models:
WER: 48.55 for speaker dependent models
WER: 62.04 for speaker independent models
I have made the decoding graph for the sgmm models and I am currently decoding with them.

- TODO Write tr.
I spent a lot of time on this today.
In summary, I installed the arlticle document class and typeset the current draft of the tr with it.
I'll include it below
- TODO Run stage 3 of the gp+yaounde system which uses the sri_gabon_read transcripts from stage 2.
I'm working on this.
The training goes fast, but making the decoding graphs and actually decoding all the data takes a while.
As I am getting ready to leave, I am decoding the sri_gabon_conv data with the sgmm models.
After this I'll have to decode with the boosted mmi trained sgmm models.

** TR draft
ARL-IR-0000 JAN 2015
US Army Research Laboratory
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
by John J Morgan, and Stephen A LaRocca
Approved for public release; distribution is unlimited.
NOTICES
Disclaimers
The findings in this report are not to be construed as an official Department of the
Army position unless so designated by other authorized documents.
Citation of manufacturers or trade names does not constitute an official endorse-
ment or approval of the use thereof.
Destroy this report when it is no longer needed. Do not return it to the originator.
ARL-IR-0000 JAN 2015
US Army Research Laboratory
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
by John J Morgan
Computational and Information Sciences Directorate, ARL
Stephen A LaRocca
Computational and Information Sciences Directorate, ARL
Approved for public release; distribution is unlimited.
REPORTDOCUMENTATIONPAGE
FormApproved
OMBNo.07040188
Publicreportingburdenforthiscollectionofinformationisestimatedtoaverage1hourperresponse,includingthetimeforreviewinginstructions,searchingexistingdatasources,gatheringandmaintainingthe
dataneeded,andcompletingandreviewingthecollectioninformation.Sendcommentsregardingthisburdenestimateoranyotheraspectofthiscollectionofinformation,includingsuggestionsforreducing
theburden,toDepartmentofDefense,WashingtonHeadquartersServices,DirectorateforInformationOperationsandReports(07040188),1215JeffersonDavisHighway,Suite1204,Arlington,VA22202
4302.Respondentsshouldbeawarethatnotwithstandinganyotherprovisionoflaw,nopersonshallbesubjecttoanypenaltyforfailingtocomplywithacollectionofinformationifitdoesnotdisplaya
currentlyvalidOMBcontrolnumber.
PLEASEDONOTRETURNYOURFORMTOTHEABOVEADDRESS.
1.REPORTDATE(DDMMYYYY)

2.REPORTTYPE

3.DATESCOVERED(FromTo)
4.TITLEANDSUBTITLE

5a.CONTRACTNUMBER
5b.GRANTNUMBER
5c.PROGRAMELEMENTNUMBER
6.AUTHOR(S)

5d.PROJECTNUMBER
5e.TASKNUMBER
5f.WORKUNITNUMBER
	
7.PERFORMINGORGANIZATIONNAME(S)ANDADDRESS(ES)

8.PERFORMINGORGANIZATIONREPORT
NUMBER
9.SPONSORING/MONITORINGAGENCYNAME(S)ANDADDRESS(ES)

10.SPONSOR/MONITORSACRONYM(S)
11.SPONSOR/MONITOR'SREPORTNUMBER(S)
12.DISTRIBUTION/AVAILABILITYSTATEMENT
13.SUPPLEMENTARYNOTES
14.ABSTRACT
15.SUBJECTTERMS
16.SECURITYCLASSIFICATIONOF:
17.LIMITATION
OF
ABSTRACT
18.NUMBER
OF
PAGES
	
19a.NAMEOFRESPONSIBLEPERSON
a.REPORT

b.ABSTRACT

c.THISPAGE

19b.TELEPHONENUMBER(Includeareacode)
 StandardForm298(Rev.8/98)
 PrescribedbyANSIStd.Z39.18
January 2015 Internal Report
Bootstrapping A Question Answering Speech Recognizer With Read Speech
John J Morgan, and Stephen A LaRocca
ARL-IR-0000
Approved for public release; distribution is unlimited.
October 2014-November 2014
AH80
US Army Research Laboratory
ATTN: RDRL-CII-T
Adelphi Laboratory Center, MD 20783-1138
primary authors email: <john.j.morgan50.civ@mail.mil>.
This report is about Automatic Speech Recognition.
document style, arlticle, revision, sans serif, L
A
TEX
16
John J Morgan
301-394-1902
Unclassified Unclassified Unclassified UU
ii
Approved for public release; distribution is unlimited.
Contents
List of Tables iv
Acknowledgments v
1. Abstract 1
2. Introduction 1
3. Methods 3
3.1 Data 3
3.2 Acoustic Model Training 3
3.3 Language Model Training 4
4. Results 4
5. Discussion 5
6. References 6
Distribution List 7
iii
Approved for public release; distribution is unlimited.
List of Tables
Table 1 WER scores for models and training sets. .....................................5
iv
Approved for public release; distribution is unlimited.
Acknowledgments
John Morgan wishes to sincerely thank his co-author, Dr. Stephen LaRocca.
v
Approved for public release; distribution is unlimited.
INTENTIONALLY LEFT BLANK.
vi
Approved for public release; distribution is unlimited.
1. Abstract
A recommended method for data collection that enables automatic rough draft tran-
scription after semi supervised adaptation of acoustic models.
2. Introduction
Speech to speech (S2S) devices enable dialogues between people who speak dif-
ferent languages. S2S devices for communicating between languages L1 and L2
consist of three major components: two Automatic Speech Recognizers, ASR1 and
ASR2 for languages L1 and L2 respectively, also known as Speech to text; one Ma-
chine Translation (MT) system; and two Speech Synthecizers, T2S1 and T2S2 also
known as text to speech. Speaker 1 speaks sentence s1 in language l1. ASR system
asr1 converts s1 into text t1 in language l1. t1 is translated into t2 in language l2 by
machine translation system mt. Text t2 is converted into the spoken sentence s2 in
language l2 by T2S2.
The U.S. Army is interested in using high quality S2S technology to help com-
municate with soldiers in allied military units during training missions. Frequently,
these soldiers speak an accented version of a world language like French or Arabic.
S2S devices are trained on the speech data that is most widely available, which is
most often the standard version of the language. Accented speech can be different
enough from the standard speech to make the ASR component of an S2S device
fail. Adaptation techniques have been used to remedy this problem.
Large amounts of recorded speech is used to train the acoustic models for ASR
systems. ASR systemss for S2S devices are ideally trained on speech that is similar
to the task for which the device will be used. Collecting this ideal kind of dialogue
data is expensive. In order for the data to be used as training data for an ASR system
it must be transcribed at the word-level. This transcription task is a major part of
the reason why the data collection is expensive. A way to cut back on this cost
is to obtain an automatically generated rough draft of the dialogue type of speech
collected then to have a human correct the rough draft.
If the data being collected comes from a language that lacks a corpus of speech data
or if the data comes from a highly accented flavor of a well-resourced language,
automatic transcriptions of the data that are useful for humans to correct will not be
possible. One way to solve this problem is to collect a small corpus of recitations
1
Approved for public release; distribution is unlimited.
by each speaker as part of the data collection. We will refer to this as the read part
of the corpus. The other part will be refered to as the conversational part. Note that
each informant contributes both a read and conversational part to the corpus. The
small read corpus will not be sufficient to serve as a training set for an ASR system
to be used in an S2S device. However, it can serve as a corpus to train an ASR
system that can be used to obtain rough draft transcriptions of the conversational
speech part. one reason this is possible is because as noted above the speakers in
the read part are the same speakers that are in the conversational part.
For scientific evaluation, for any ASR task, the speakers in the test set and training
set are kept disjoint. The ASR taskk becomes much easier when the speakers in the
training and test sets are the same.
The cost of building an ASR system with read speech is much lower than building
one with conversational speech. A pronouncing dictionary is the most expensive
component of a phone-based ASR system. For a system built with read speech
there is no cost involved with transcribing the data. The transcriptions are given by
the prompts. The dictionary can be used to obtain a phone-level transcription from
a word-level transcription.
Previous work has shown that ASR for accented speech can benefit from the use
of subspace gaussian mixture models (SGMM)s instead of triphone models.1
One
of our contributions in this paper is a improvement to the above work. We describe
a two step semi supervised process for building an ASR system that can be used
effectively to get a rough transcription of the conversational part of a corpus. The
first step uses the read part of the corpus to train acoustic models which are used
to get a rough transcription of the conversational part of the corpus. The second
step trains new models by adding to the training data the conversational part of the
corpus with its automatically transcribed labels. We will refer to this as quasi semi
supervised training (not quite semi supervised), since the speakers in the unlabeled
training set are the same as those in the supervised read part of the training corpus.
There are two benefits to this kind of data collection. First, we show that the quasi
semi supervised training results in lower WER scors. Second, the automatic label-
ing results in a transcription of the conversational part of the corpus that can lower
costs for human in the loop labeling. The results of this two stage approach to sys-
tem building also supports our recommendation that both read and conversational
2
Approved for public release; distribution is unlimited.
speech be collected in data collection of accented speech.
3. Methods
3.1 Data
Three speech corpora were used in this project.
The Yaounde corpus: collected in Yaounde ,the capital city of Cameroon. It has
two parts: the read part which consists of recitations fof prompts and the
conversational part which consists of answers to questions.
The French part of the Globalphone corpus: This corpus consists of 100 native
French speakers. They recorded a total of 10478 utterences.
The Central Accord Corpus: Collected in Gabon from speakers from four Cen-
tral African countries. A small part of the read part of this corpus was used as
test data.
3.2 Acoustic Model Training
All the experiments performed in this project used the kaldi toolkit. The standard
kaldi recipe framework was used.
As recommended by the Babel project, we trained models on plp and pitch fea-
tures. The following model building sequence was followed: We tried to follow the
naming conventions used in the recipes for the babel project.
Monophones (mono) Flat start and 40 iterations of monophone training, with delta-
delta features. Per speaker cepstral mean normalization was applied.
Triphones (tri1)
Triphones (tri2)
Triphones (tri3)
Triphones (tri4) Trained with lda and mllt transforms.
Triphones (tri5)
3
Approved for public release; distribution is unlimited.
Supspace Gaussian Mixture Models (sgmm)
SGMMs with boosted mmi (sgmmb)
Two configurations of the training folds of the data were compared.
GP Consisting of the Globalphone prompts.
GP + Yaounde Consisting of both the Yaounde and Globalphone prompts.
The set of unlabeled data consists of answers to questions in the Yaounde corpus.
The answers were given by the same speakers who made the recitations in the read
part of the Yaounde corpus.
After training Boosted MMI SGMM models on the supervised training sets, QUASI
SEMI SUPERVISED transcriptions were obtained for the Answers by decoding
with the resulting ASR system. decoding was done with lattice rescoring, where
lattices were generated from a previous SGMM system. Speaker vectors and MLLR
transforms were also used. The Yaounde Answers data together with their quasi
semi supervised labels were Then appended to the training set and the same training
regime was run again.
3.3 Language Model Training
A three gram statistical language model was trained with srilm on the following text
data sets:
 Subtitles
 GP transcripts

4. Results
sgmm boosted mmi
4
Approved for public release; distribution is unlimited.
Table 1 WER scores for models and training sets.
models trainingspeaker supervisionspeaker GP GP + Yaounde
tri5 dependent full 48.55 34.78
tri5 independent full 62.04 44.02
tri5 dependent semi 29.87
tri5 independent semi 46.94
sgmm dependent semi 21.25
sgmm dependent full 38.28 25.85
sgmm independent full
sgmm independent semi
5. Discussion
The Semi supervised method yields gains when speaker dependent models are
trained. In this case the WER goes down from 25.85 to 21.25. However, when
speaker independent model training methods were used, we saw the WER go up. In
the triphone case, the scores went up from 44.02 to 46.94.
Unfortunately, this observation implies that our method will not be useful for our
target S2S device application where speaker dependent models are not practical.
In future work, we plan on exploring neural network models and deep learning
techniques to extend our ideas to the online decoding scenario.
5
Approved for public release; distribution is unlimited.
6. References
1. Motlicek P, Garner PN, Kim N, Cho J. Accent adaptation using subspace
gaussian mixture models. In: The 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP); 2013 May; Vancouver, BC, Canada.
(38; no. Idiap-RR-38-2013) Rue Marconi 19, Martigny, Switzerland: p. 7170
7174.
6
Approved for public release; distribution is unlimited.
1
(PDF)
DEFENSE TECHNICAL
INFORMATION CTR
DTIC OCA
2
(PDF)
DIRECTOR
US ARMY RESEARCH LAB
RDRL CIO L
IMAL HRA MAIL & RECORDS MGMT
1
(PDF)
GOVT PRINTG OFC
A MALHOTRA
7
Approved for public release; distribution is unlimited.
INTENTIONALLY LEFT BLANK.
8

** Goals for Wednesday:
- TODO Write objectives
- TODO Decode sri_gabon_conv data with sgmm boosted mmi models
- TODO Start stage 3 of gp+yaounde system
- TODO Start stage 2 of gp system to get semi supervised results
- TODO Write TR
* <2016-10-06 Thu>
** Goals for Thursday set Wednesday:
- DONE Write a first pass on objectives
- DONE Wrap up yaounde answers semi supervised training 
- DONE Start on sri_gabon_read semi supervised training.

The sgmm5 denominator lattices generation had finished this morning.
I started the boosted mutual maximum information (mmi) sgmm training.
There might be something wrong.
I'm getting the following warning:
Frame-counts disagree 10969869 versus 9789113
This might have something to do with the problem I had yesterday when I reran the data prep and feature extraction scripts.

The decoding of the test set yesterday with sgmm5 models gave a best wer:
WER: 22.03
The similar decoding of the answers also succeeded:
WER: 18.29
This is testing on a lot of the training set.
The similar decoding  on the sri_gabon_reaed data failed.
This is because I had not decoded the sri_gabon_read  data with the tri5 models to get  transforms.
Recall that these are speaker dependent models.
I'm running the decoding of the sri_gabon_read data now with tri5 models.

The boosted sgmm5 mmi training finished.
There is the warning about different numbers of frames still.
I am running the rescore decoding with the boosted mmi trained sgmm models.
On the test set:
Best WER: 21.25

The decoding of the sri_gabon_read set with the tri5  yaounde answers semi supervised models finished.
So now I have automatically generated transcripts of the sri_gabon_read data (modulo the problem yesterday).
Now I need to decode with the sgmm5 semi supervised models.
All done for this stage.

I'm going to use these transcripts as supervision in the next stage of training.

The next stage will use both the answer and sri_gabon_read automatically generated transcripts as training labels.
Can I delete the data/sri_gabon_read directory and regenerate it?

I ran the feature extractor for the train_semi_supervised_2 data set.

I fired up a script that is supposed to run the steps  for the second stage of mono to sgmm semi supervised training.

**  all the scores in my experiments
Below are all the wer scores from the experiments I've run in the past few weeks.
I'm surprised I got this far without deleteing my working directory.
I sorted them in reverse numerical order.

 
%WER 99.51 [ 14247 / 14317, 2614 ins, 3802 del, 7831 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it3/wer_20_1.0
%WER 99.33 [ 14221 / 14317, 2579 ins, 3812 del, 7830 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it4/wer_20_1.0
%WER 99.25 [ 14210 / 14317, 1461 ins, 5427 del, 7322 sub ] exp/tri1_semi_supervision_2/decode_answers/wer_20_1.0
%WER 99.23 [ 14207 / 14317, 2579 ins, 3794 del, 7834 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it1/wer_19_1.0
%WER 99.15 [ 14195 / 14317, 2568 ins, 3812 del, 7815 sub ] exp/sgmm5_mmi_b0.1/decode_answers_it2/wer_20_1.0
%WER 99.09 [ 14187 / 14317, 2541 ins, 3819 del, 7827 sub ] exp/sgmm5/decode_answers/wer_20_1.0
%WER 99.02 [ 14176 / 14317, 2539 ins, 3793 del, 7844 sub ] exp/sgmm5/decode_fmllr_answers/wer_20_1.0
%WER 98.84 [ 14088 / 14253, 1186 ins, 6097 del, 6805 sub ] exp/mono_semi_supervision/decode_answers/wer_17_1.0
%WER 98.74 [ 14073 / 14253, 1318 ins, 5990 del, 6765 sub ] exp/tri1_semi_supervision/decode_answers/wer_19_1.0
%WER 96.67 [ 13840 / 14317, 1504 ins, 5443 del, 6893 sub ] exp/tri2_semi_supervision_2/decode_answers/wer_17_1.0
%WER 96.05 [ 13751 / 14317, 1134 ins, 6449 del, 6168 sub ] exp/tri2_semi_supervision/decode_answers/wer_16_1.0
%WER 93.56 [ 13395 / 14317, 1157 ins, 6075 del, 6163 sub ] exp/sgmm5_semi_supervision/decode_answers_no_mllr/wer_20_1.0
%WER 93.01 [ 13316 / 14317, 2282 ins, 4169 del, 6865 sub ] exp/tri5_semi_supervision/decode_answers/wer_19_1.0
%WER 92.99 [ 13314 / 14317, 2173 ins, 4329 del, 6812 sub ] exp/tri5_semi_supervision_2/decode_answers.si/wer_17_1.0
%WER 92.39 [ 13227 / 14317, 2349 ins, 3957 del, 6921 sub ] exp/tri5_semi_supervision_2/decode_answers/wer_15_1.0
%WER 92.38 [ 13226 / 14317, 1802 ins, 5256 del, 6168 sub ] exp/tri5_semi_supervision/decode_answers.si/wer_19_1.0
%WER 89.05 [ 12749 / 14317, 1714 ins, 4596 del, 6439 sub ] exp/sgmm5_semi_supervision/decode_answers/wer_17_1.0
%WER 88.96 [ 12737 / 14317, 1615 ins, 5171 del, 5951 sub ] exp/tri3_semi_supervision_2/decode_answers/wer_17_1.0
%WER 88.75 [ 12707 / 14317, 1851 ins, 4299 del, 6557 sub ] exp/sgmm5_semi_supervision_2/decode_answers/wer_14_1.0
%WER 88.64 [ 12691 / 14317, 1156 ins, 6298 del, 5237 sub ] exp/tri3_semi_supervision/decode_answers/wer_16_1.0
%WER 88.52 [ 12674 / 14317, 1747 ins, 4781 del, 6146 sub ] exp/tri4_semi_supervision_2/decode_answers/wer_19_1.0
%WER 86.16 [ 12335 / 14317, 1917 ins, 4064 del, 6354 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it1/wer_11_1.0
%WER 86.15 [ 12334 / 14317, 1980 ins, 4056 del, 6298 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it1/wer_11_1.0
%WER 85.70 [ 12270 / 14317, 2001 ins, 4035 del, 6234 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 85.33 [ 12217 / 14317, 2044 ins, 4002 del, 6171 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it4/wer_11_1.0
%WER 85.24 [ 12204 / 14317, 1920 ins, 4038 del, 6246 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 85.23 [ 12203 / 14317, 2023 ins, 4008 del, 6172 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_answers_it3/wer_11_1.0
%WER 85.02 [ 12172 / 14317, 1991 ins, 3924 del, 6257 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it3/wer_10_1.0
%WER 84.77 [ 12137 / 14317, 1944 ins, 4011 del, 6182 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_answers_it4/wer_11_1.0
%WER 72.88 [ 9528 / 13073, 585 ins, 2216 del, 6727 sub ] exp/mono_semi_supervised/decode_answers/wer_9_0.5
%WER 62.89 [ 10499 / 16694, 666 ins, 4244 del, 5589 sub ] exp/tri1_semi_supervised/decode_answers/wer_12_0.5
%WER 62.86 [ 2006 / 3191, 67 ins, 772 del, 1167 sub ] exp/mono_semi_supervision_2/decode_test/wer_9_0.0
%WER 60.17 [ 1920 / 3191, 117 ins, 1025 del, 778 sub ] exp/tri5_semi_supervision_2/decode_test.si/wer_10_0.0
%WER 59.39 [ 1895 / 3191, 135 ins, 426 del, 1334 sub ] exp/mono_semi_supervision/decode_test/wer_13_0.0
%WER 58.95 [ 9841 / 16694, 725 ins, 4054 del, 5062 sub ] exp/tri2_semi_supervised/decode_answers/wer_14_0.0
%WER 57.98 [ 1850 / 3191, 111 ins, 475 del, 1264 sub ] exp/mono_semi_supervised/decode_test/wer_11_0.0
%WER 56.06 [ 1789 / 3191, 84 ins, 1075 del, 630 sub ] exp/tri4_semi_supervision_2/decode_test/wer_9_0.0
%WER 54.00 [ 1723 / 3191, 78 ins, 963 del, 682 sub ] exp/tri3_semi_supervision_2/decode_test/wer_10_0.0
%WER 49.67 [ 1585 / 3191, 168 ins, 423 del, 994 sub ] exp/tri5_semi_supervision/decode_test.si/wer_13_1.0
%WER 47.54 [ 1517 / 3191, 51 ins, 830 del, 636 sub ] exp/tri2_semi_supervision_2/decode_test/wer_11_0.0
%WER 47.51 [ 1516 / 3191, 96 ins, 613 del, 807 sub ] exp/tri1_semi_supervision_2/decode_test/wer_10_0.0
%WER 46.94 [ 1498 / 3191, 200 ins, 264 del, 1034 sub ] exp/tri5_semi_supervised/decode_test.si/wer_13_0.0
%WER 44.02 [ 1410 / 3203, 149 ins, 306 del, 955 sub ] exp/tri5/decode_test.si/wer_19_1.0
%WER 43.18 [ 1378 / 3191, 83 ins, 384 del, 911 sub ] exp/tri1_semi_supervision/decode_test/wer_12_1.0
%WER 42.78 [ 1365 / 3191, 108 ins, 458 del, 799 sub ] exp/tri3_semi_supervision/decode_test/wer_18_0.0
%WER 42.24 [ 1348 / 3191, 145 ins, 336 del, 867 sub ] exp/sgmm5_semi_supervision/decode_test_no_mllr/wer_10_0.0
%WER 41.52 [ 1325 / 3191, 137 ins, 268 del, 920 sub ] exp/tri1_semi_supervised/decode_test/wer_12_0.0
%WER 41.49 [ 1324 / 3191, 157 ins, 345 del, 822 sub ] exp/tri4_semi_supervision/decode_test/wer_14_0.0
%WER 41.18 [ 21 / 51, 5 ins, 0 del, 16 sub ] exp/tri5_semi_supervised/decode_answers.si/wer_16_0.0
%WER 39.05 [ 1246 / 3191, 95 ins, 332 del, 819 sub ] exp/tri2_semi_supervision/decode_test/wer_15_0.0
%WER 37.01 [ 1181 / 3191, 111 ins, 277 del, 793 sub ] exp/tri2_semi_supervised/decode_test/wer_12_0.5
%WER 34.78 [ 1114 / 3203, 168 ins, 225 del, 721 sub ] exp/tri5/decode_test/wer_19_1.0
%WER 34.66 [ 1106 / 3191, 141 ins, 350 del, 615 sub ] exp/tri5_semi_supervision_2/decode_test/wer_13_0.0
%WER 32.81 [ 1047 / 3191, 91 ins, 367 del, 589 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it4/wer_13_0.0
%WER 32.62 [ 1041 / 3191, 134 ins, 270 del, 637 sub ] exp/tri5_semi_supervision/decode_test/wer_15_0.0
%WER 31.96 [ 1020 / 3191, 109 ins, 313 del, 598 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it3/wer_10_0.0
%WER 31.12 [ 993 / 3191, 110 ins, 293 del, 590 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it2/wer_10_0.0
%WER 29.87 [ 953 / 3191, 142 ins, 197 del, 614 sub ] exp/tri5_semi_supervised/decode_test/wer_18_0.0
%WER 29.84 [ 3901 / 13073, 485 ins, 598 del, 2818 sub ] exp/tri5_semi_supervised/decode_answers/wer_16_0.5
%WER 29.52 [ 942 / 3191, 131 ins, 199 del, 612 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_test_it1/wer_9_0.0
%WER 29.22 [ 936 / 3203, 162 ins, 135 del, 639 sub ] exp/dnn6_pretrain-dbn_dnn/decode_test/wer_10_1.0
%WER 28.61 [ 913 / 3191, 129 ins, 163 del, 621 sub ] exp/sgmm5_semi_supervision_2/decode_test/wer_9_0.0
%WER 27.72 [ 888 / 3203, 160 ins, 128 del, 600 sub ] exp/sgmm5/decode_test/wer_17_0.0
%WER 27.41 [ 878 / 3203, 110 ins, 183 del, 585 sub ] exp/sgmm5/decode_fmllr_test/wer_16_1.0
%WER 26.16 [ 838 / 3203, 145 ins, 131 del, 562 sub ] exp/sgmm5_mmi_b0.1/decode_test_it1/wer_18_0.0
%WER 25.98 [ 832 / 3203, 107 ins, 163 del, 562 sub ] exp/sgmm5_mmi_b0.1/decode_test_it3/wer_20_0.5
%WER 25.94 [ 831 / 3203, 107 ins, 169 del, 555 sub ] exp/sgmm5_mmi_b0.1/decode_test_it4/wer_20_0.5
%WER 25.85 [ 828 / 3203, 149 ins, 120 del, 559 sub ] exp/sgmm5_mmi_b0.1/decode_test_it2/wer_15_0.0
%WER 24.38 [ 778 / 3191, 99 ins, 201 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it4/wer_12_0.0
%WER 24.22 [ 773 / 3191, 95 ins, 200 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it3/wer_12_0.0
%WER 23.97 [ 765 / 3191, 118 ins, 148 del, 499 sub ] exp/sgmm5_semi_supervision/decode_test/wer_11_0.0
%WER 23.82 [ 760 / 3191, 94 ins, 188 del, 478 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it2/wer_12_0.0
%WER 23.79 [ 759 / 3191, 97 ins, 169 del, 493 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_test_it1/wer_12_0.0
%WER 22.38 [ 714 / 3191, 91 ins, 141 del, 482 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it4/wer_12_0.5
%WER 22.03 [ 703 / 3191, 92 ins, 148 del, 463 sub ] exp/sgmm5_semi_supervised/decode_test/wer_17_0.0
%WER 21.69 [ 692 / 3191, 116 ins, 111 del, 465 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it2/wer_11_0.0
%WER 21.62 [ 690 / 3191, 96 ins, 126 del, 468 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it3/wer_14_0.0
%WER 21.25 [ 678 / 3191, 93 ins, 126 del, 459 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_test_it1/wer_14_0.0
%WER 20.78 [ 2693 / 12960, 283 ins, 516 del, 1894 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it4/wer_14_1.0
%WER 20.56 [ 2665 / 12960, 299 ins, 482 del, 1884 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it3/wer_12_1.0
%WER 19.91 [ 2580 / 12960, 290 ins, 466 del, 1824 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it2/wer_11_1.0
%WER 19.31 [ 2502 / 12960, 309 ins, 419 del, 1774 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_answers_it1/wer_11_0.5
%WER 18.29 [ 2370 / 12960, 317 ins, 399 del, 1654 sub ] exp/sgmm5_semi_supervised/decode_answers/wer_11_0.5
%WER 129.10 [ 83893 / 64984, 18938 ins, 23081 del, 41874 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 129.08 [ 83884 / 64984, 18930 ins, 23127 del, 41827 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 129.02 [ 83845 / 64984, 18890 ins, 23122 del, 41833 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 129.02 [ 83842 / 64984, 18888 ins, 23086 del, 41868 sub ] exp/sgmm5_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 128.74 [ 83663 / 64984, 20719 ins, 22972 del, 39972 sub ] exp/tri5_semi_supervised/decode_sri_gabon/wer_20_1.0
%WER 128.71 [ 83643 / 64984, 20832 ins, 23084 del, 39727 sub ] exp/tri5_semi_supervised/decode_sri_gabon.si/wer_20_1.0
%WER 127.02 [ 82542 / 64984, 19483 ins, 23425 del, 39634 sub ] exp/tri5/decode_sri_gabon/wer_20_1.0
%WER 126.76 [ 82376 / 64984, 19248 ins, 23295 del, 39833 sub ] exp/sgmm5/decode_fmllr_sri_gabon/wer_20_1.0
%WER 126.69 [ 82327 / 64984, 19196 ins, 23309 del, 39822 sub ] exp/sgmm5/decode_sri_gabon/wer_20_1.0
%WER 126.29 [ 82069 / 64984, 18948 ins, 23521 del, 39600 sub ] exp/tri5/decode_sri_gabon.si/wer_20_1.0
%WER 122.17 [ 79518 / 65089, 16340 ins, 27735 del, 35443 sub ] exp/tri5_semi_supervision/decode_sri_gabon.si/wer_20_1.0
%WER 121.86 [ 79320 / 65089, 16043 ins, 27404 del, 35873 sub ] exp/tri1_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 121.01 [ 78761 / 65089, 15386 ins, 27515 del, 35860 sub ] exp/tri5_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 119.70 [ 77787 / 64984, 14549 ins, 27388 del, 35850 sub ] exp/sgmm5_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 119.62 [ 77735 / 64984, 14497 ins, 27461 del, 35777 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 119.56 [ 77697 / 64984, 14442 ins, 27484 del, 35771 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 119.54 [ 77680 / 64984, 14435 ins, 27484 del, 35761 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 119.52 [ 77670 / 64984, 14419 ins, 27498 del, 35753 sub ] exp/sgmm5_semi_supervision_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 117.68 [ 76598 / 65089, 12998 ins, 29308 del, 34292 sub ] exp/mono_semi_supervision/decode_sri_gabon/wer_20_1.0
%WER 117.44 [ 76320 / 64984, 13110 ins, 29970 del, 33240 sub ] exp/mono_semi_supervised/decode_sri_gabon/wer_20_1.0
%WER 115.39 [ 74982 / 64984, 11584 ins, 30617 del, 32781 sub ] exp/tri5_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 114.57 [ 74453 / 64984, 11177 ins, 29937 del, 33339 sub ] exp/sgmm5_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 114.41 [ 74346 / 64984, 11038 ins, 30111 del, 33197 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it1/wer_20_1.0
%WER 114.35 [ 74309 / 64984, 11030 ins, 30215 del, 33064 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it3/wer_20_1.0
%WER 114.35 [ 74307 / 64984, 11026 ins, 30203 del, 33078 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it2/wer_20_1.0
%WER 114.32 [ 74290 / 64984, 11028 ins, 30189 del, 33073 sub ] exp/sgmm5_semi_supervision_2_mmi_b0.1/decode_sri_gabon_it4/wer_20_1.0
%WER 105.41 [ 15091 / 14317, 2817 ins, 3819 del, 8455 sub ] exp/tri5/decode_answers.si/wer_20_1.0
%WER 104.25 [ 67748 / 64984, 3837 ins, 45236 del, 18675 sub ] exp/tri5_semi_supervision_2/decode_sri_gabon.si/wer_20_1.0
%WER 104.13 [ 39389 / 37827, 2166 ins, 4183 del, 33040 sub ] exp/tri5_semi_supervised/decode_sri_gabon_read.si/wer_20_1.0
%WER 104.11 [ 67658 / 64984, 3680 ins, 44021 del, 19957 sub ] exp/tri1_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 103.37 [ 14799 / 14317, 2846 ins, 3682 del, 8271 sub ] exp/tri5/decode_answers/wer_20_1.0
%WER 103.14 [ 39015 / 37827, 1944 ins, 2591 del, 34480 sub ] exp/tri5_semi_supervised/decode_sri_gabon_read/wer_20_1.0
%WER 103.10 [ 66998 / 64984, 2974 ins, 45341 del, 18683 sub ] exp/mono_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 102.91 [ 66873 / 64984, 2721 ins, 47246 del, 16906 sub ] exp/tri2_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 102.08 [ 38613 / 37827, 1579 ins, 2191 del, 34843 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it4/wer_20_1.0
%WER 102.06 [ 38607 / 37827, 1584 ins, 2198 del, 34825 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it3/wer_20_1.0
%WER 102.05 [ 38602 / 37827, 1577 ins, 2216 del, 34809 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it2/wer_20_1.0
%WER 102.00 [ 38583 / 37827, 1561 ins, 2232 del, 34790 sub ] exp/sgmm5_semi_supervised/decode_sri_gabon_read/wer_20_1.0
%WER 101.98 [ 66270 / 64984, 2044 ins, 49862 del, 14364 sub ] exp/tri3_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 101.97 [ 38571 / 37827, 1549 ins, 2225 del, 34797 sub ] exp/sgmm5_semi_supervised_mmi_b0.1/decode_sri_gabon_read_it1/wer_20_1.0
%WER 101.89 [ 66212 / 64984, 1966 ins, 50793 del, 13453 sub ] exp/tri4_semi_supervision_2/decode_sri_gabon/wer_20_1.0
%WER 100.28 [ 14357 / 14317, 853 ins, 6401 del, 7103 sub ] exp/mono_semi_supervision_2/decode_answers/wer_19_1.0
john@A-TEAM19054:~/yaounde/kaldi-trunk/egs/gp+yaounde$ 

** WAR:
Since I'm going on leave tomorrow, I'm writing a WAR today.
John Morgan achieved a new best word error rate (WER) score for the speech recognizer he is building with the kaldi toolkit on African accented French. 
The new best WER is 21.25 down from the previous best of 23.79. 
The improvement was obtained by automatically cleaning the transcripts of the data that was transcribed by the recognizer in the previous supervised stage of training. 

** Performance Objectives
The form is a nightmare.
General Objectives:
ASR Adaptation:
What is practical?
What method works in an S2S device?
What kind of speaker adaptation can be done online?
The best results we are getting with kaldi are speaker dependent.
Can these models be used in a S2S device?
If not, what are the best models for an S2S device?
Latest Methods:
Variable computational graphs.
Learn pycnn.
How can these methods be used for Army needs?
kaldi:
Chain Models.
nnet2
nnet3

1. Speech Recognizer Adaptation:
a. Acoustic Modeling.
i. European French to African accents
ii. Standard Arabic to Tunisian accent.
iii. Neural Network Models:
A. RBM
B. TDNN
C. RNN/LSTM
D. Chain 
b. Language Modeling:
i. Dialogue modeling:
A. French
B. Arabic 
ii. Lexicon expansion
A. French
B. Arabic.
C. Dari
2. Machine Translation
a. Variable Computation Graphs

After I sent out this report to Steve, I continued working on the next stage of semisupervised training.
The acoustic models have trained through tri5.
I am making the fst decoding graph as I am laeaving.
This will also decode the test, answers  and sri_gabon_read sets.
I have also prepared the sri_gabon_conv data directory.
I have extracted plp  pitch features from it too.
The next step will be to decode the sri_gabon_conv set with the tri5 models to get transforms.

** Goals for When I come back from leave:
- TODO Write objectives and put them in the form (I'll need help with the form).
- TODO Finish second stage of semi supervised training.
- TODO Use best resulting models to transcribe sri_gabon_conv data.
- TODO Get qualitative evaluation of these transcripts from Steve.
- TODO Wrap up sgmm ASR system build recipes.
- TODO Start on neural network approaches to ASR
- TODO Compare neural network approaches to baseline sgmm approach (this is a long term goal. To be achieved by Xmas)  
* <2016-10-05 Wed>
** Goals for Wednesday set Tuesday:
- TODO finish writing the script to prepare the sri_gabon_read data
- TODO Wrap up my current run of the first stage of semi supervised training.


The mono2sgmm script failed at sgmm denominator lattice making.
Why did it fail?
The fst decoding graph already existed, so it did not remake it.
This could be the problem.
I deleted the directory where the work on the denominator lattices is done and I am rerunning the denlats making script.

I decode the test set with the tri5 semi supervised models.
WER: 29.87 for speaker dependent models
WER: 46.94 for speaker independent models.

I screwed up again.
I deleted the data/train_semi_supervised directory while working on the script to do the next stage.
So the sgmm denominator lattice making died.
I still might be able to run the test decoding with the regular sgmm models.
I reran the data prep script and the plp pitch extractor again.
I'm rerunning the denlats script just in case it works without having to start over.
Well ... it seems to be running.
It seems to have crashed and restarted?
it is still running.
It does decoding as part of the denlats making.
I think half of the jobs died.
There are only 5 directories and there should be 10.

The decoding graph script for sgmm5 semi supervised finally finished. I'm surprised it did not crash.
The decoding is running, also surprising.

The denlats generation is also running. 

** Goals for Thursday
- TODO Write a first pass on objectives
- TODO Wrap up yaounde answers semi supervised training 
- TODO Start on sri_gabon_read semi supervised traing.
* <2016-10-04 Tue>
** Goals for Tuesday set Monday:
- DONE Write script to remove asterisks from per_utt file.
I just had to debug the perl script I had written yesterday.
- TODO Rerun semi supervised training with the improved labels. 
I'm starting over again at the semi supervision step.
I'm extracting the plp and pitch features.
Can I skip to tri5?
tri5 requires the alignments from tri4.
The first step, the monophone training, uses a flat start.
I decode the answers data to get the transcripts.
As part of this decoding, alignment is performed (I think).
Maybe not by default.
I do not know how to jump to tri5 with the semi supervised data.

I am now training monophones with the semi supervised data.
The labels for the semi supervised data now does not contain the asterisks.
I killed the process I was running and I'm starting over again.
I am going  to use a script that runs all the steps from feature extraction through monophones to sgmm5.
I had such a script for the first supervised stage of training.
I had to modify it or the semi supervised stage.
I decoded the test set with semi supervised monophones. 
WER: 57.98
- TODO Separate out read sri_gabon data:

- lexicon work:
I'm removing the numbers in parens after some words in the lexicon.

As I'm getting ready to leave, I'm working on the script to prepare the sri_gabon_read data.
** Goals for Wednesday:
- TODO finish writing the script to prepare the sri_gabon_read data
- TODO Wrap up my current run of the first stage of semi supervised training.
* <2016-10-03 Mon>
** Goals For today Monday:
- TODO Rerun the semi supervised training experiment

I found the problem with the automatic transcripts I was using for semi supervised training.
I already made a mistake in stage 1 when I used the Answers transcripts for training.
The file I was using was actually the reference transcripts, which in the answers case were the questions.
I was considering extracting the recognizer output from the decoding logs, but the log files are different when you get up into the sgmm modeling.
I found another file that contains the hypotheses from the recognizer.
I have to be careful, because it also contains the reference and some other files used in scoring.
I am going back to the first semi supervised stage and using these new transcripts.
They contain symbols like "*" that worry me a little.

I have finished training monophone through sgmm acoustic models in supervised mode. 
I am trying to decode with the sgmm models.
First, I need to decode with the tri5 models to get transforms.
Yes. Now that I have the tri5 transforms, the decoding is going forward with sgmm models.
I made the mistake of firing up all the decoding steps at one.
This bogged the machine and some of the processes failed.
I'm going to go through them all 1 at a time.
First monophones

The monophones score lower 

I've been looking at the output from the decoding with semi supervised monophones on the sri_gabon data.
Look at speaker 048.
She seems to be reading the whole list of prompts in one recording.
There are a lot of very long utterences in several directories.
No wonder it takes so long to decode the sri_gabon data. this speaker 113 goes on and on and on ...

- problem:
The three asterisks that appear in the wer_details/per_utt file make the scoring fail when I use it as the reference.
I'll have to delete them before using it as a reference or as the source of my labels.
** Goals for Tuesday:
- TODO Write script to remove asterisks from per_utt file.
- TODO Rerun semi supervised training with the improved labels. 
* Goals for Friday set Thursday<2016-09-30 Fri>:
- TODO Finish second stage of semi supervised training with sri_gabon

The mmi training is still running on the gpu machine on the sri_gabon semi supervised stage.
Actually, it hasn't even got to training yet, it's still on denominator lattice generation.
- DONE Run model building scripts for gp only system on gpu machine ( maybe put several steps in 1 script)

I started a script that (if all goes well) will train the acoustic models tri2 through sgmm5.
The scripts I had run before got through tri1 training. So this one starts with alignment using tri1.

I started a similar script starting at monophones for yaounde+gp on my laptop.
The difference with these scripts is that I'm only concentrating on training the acoustic models. I don't make the decoding graphs and I do not decode. 
I'll make scripts for decoding graphs, decoding and the lm separately.
* Goals for Thursday set Wednesday: <2016-09-29 Thu>
- DONE GP model building on gpu machine

I'm starting to run the gp model building scripts on the gpu machine and I'm finding an interesting problem. 
The files don't seem to get sorted in the same wasy.
I think the   sorting problem  depends on environment variables
There might also be a file concatenation problem.
I am concatenating files somewhere and I am not deleting the old file before starting the concatenation. 
I had done a lot of these fixes under the yaounde+gp directory.
I am copying those fixes to the gp directory.
Problems remained.
There were concatenation problems at every step.
I spent most of the morning fixing these problems.
I think I'm good now on data prep.
I spoke a little too soon. I had do more fixes for the sorting.
Now I'm getting similar problems with the test data.
I was finally able to move on to the next steps.

- TODO Write TR
- TODO Finish gp + yaounde sgmm model building

It's still cooking.
- TODO Read papers

I've got several things running as I'm getting ready to leave.
- The second stage of the semi supervised training with sri_gabon is still making the denominator lattices.
- The model building for the gp only system is training monophones on the gpu machine
- sgmm models for the gp only system are being trained on my laptop
- Decoding of  sri_gabon  with sgmm5 modles
I'll only be here for half a day tomorrow, so I'm not expecting to much done.
The modle building steps from monophones to sgmm5 are pretty stable in my scripts. I might put them all in 1 script and run them on the gpu machine.
The step after the first sgmm5 step require an decoding fst from tri5, but this requires decoding with tri5 models. So, I'll do this step in a separate script before starting the next sgmm5 step.

** Goals for Friday:
- TODO Finish second stage of semi supervised training with sri_gabon
- TODO Run model building scripts for gp only system on gpu machine ( maybe put several steps in 1 script)

* Goals for Wednesday set Tuesday:
- TODO Another pass at big 6 accomplishments
** Big 6 accomplishments
Name: John Morgan
Office:
Team Leader (No):
Grade: DB03
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations  at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  Yes
Recognition (awards, letters of appreciation, etc.)
 List significant projects you are working on & your specific responsibilities:
Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.
Documentation of processes used in the project.
Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.
6 Most significant Actions / Impacts
1.  
Action: 1. Applied    algorithms from the reference kaldi ASR toolkit to in-house data sets for   speech recognition tasks of interest to the US ARMY.
Impact: An important outcome of this effort is a capability and expertise at using GPU-based technology in the MCAB branch. 
2. 
Action: 1. Applied  Deep Learning approaches with  toolkits to in-house data sets for machine translation  tasks of interest to the US ARMY.
Impact: As a requirement for this project I became proficient at python programming, which the branch can now count on as a capability. 
3. 
Action: 1. Research work with UMD professors on simultaneous translation. 
Impact: As a requirement for this project, I bcame familiar with the area of machine learning  called reinforcement learning, which should have an important impact on future applications of Deep Learning to NLP applications.
4. 
Action: 1. Coded Recurrent Neural Networks for prediction    in simultaneous translation. 
Impact: Understanding of the functioning of RNNs at an elementary level.
5.  
Action: 1. Investigated a potentially novel approach to adaptively training an ASR system to an accented version of a world Language.
Impact: Potential cost savings in transcription of collected speech data.
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY

** Accomplishments
Technical Competency:
Apply methods, theories, techniques, and skills learned in Computer Science Ph.D. corsework and research at UMD to projects of interest to ARL and the ARMY.

In this past year I took and passed 2 graduate courses, 1 in Scientific Computing and 1 in Database Management Systems.
The course in Scientific Computing covered the fundamental theory of optimization, which is  relevant to the computational solutions of many if not most problems in NLP and virtually all problems in Deep Learning. 
I was introduced to matlab and octabe in this course and I studied the implementations of important algorithms in the python modules numpy and scipy. 
Our branch has several of its own speech and text corpora that have not yet been curated. 
The DBMS course I took will help our branch utilize and share our data for the benefit of the Army.

As part of a research team at UMD, I developed software that uses Deep and Reinforcement Learning techniques to predict language behaviors of a simultaneous translator. 

Develop machine translation software using the theories and methods emerging from the field of Deep Learning.

I became familiar with several toolkits for developing neural networks including google's open source Tensorflow, theano and keras. 
I used recurrent neural networks (RNN)s with long short term memory and gated recurrent units to perform Machine Translation. 
I wrote code to implement an RNN that makes predictions about future words a simultaneous translators will choose to interpret. 

Use python programming skills to develop software in ARL projects.

Both the Tensorflow and Theano toolkits are implemented in python. 
I used my python programming skills to apply the tools in those toolkits to develop the MT systems I worked with.

Support the team project to implement an Automatic Speech Recognition system adapted to  speech as spoken in African countries. 

I made good progress on this objective. 
I dedicated a lot of hard work to preparing our inhouse speech corpora  for processing by ASR system development tools. 

I can now build subspace gaussian mixture model based ASR systems with the kaldi toolkit. 
The recipes I have developed to prepare and process the African accented speech can be applied to our other holdings.
I am investigating a semi supervised  acoustic model training strategy that could potentially cut the cost of transcribing collected speech data.

Cooperation:

I collaborated closely with Dr. Stephen LaRocca on his project to implement an algorithm for selecting data to be used to train statistical n-gram language models. 

Serve as a bridge between the multilingual computing branch and the academic community at UMD. 

Communication:

Publish a journal paper as first author on research on simultaneous translation.


Customer Relations:

Respond to requests from team customers for advice and technical support on
issues concerning machine translation and machine learning.

Technology Transition:

Transition simultaneous translation code developed for research into branch projects.


Diversity:
Support ARL's diversity initiatives by participating in locally sponsored training, outreach and/or special emphasis programs to increase personal awareness and understanding of the various cultures that exist among laboratory employees.

I learned a lot about the sacrifices African American soldiers made during World War II for the U.S. Army by attending a film in the ALC auditorium.
I also served as a speaker on a panel for Disability Awareness Month.

SHARP:
Support leadership's efforts to address and prevent sexual harassment and sexual assault and ensure a respectful work environment for all.
Demonstrate support for the SHARP program by actively participating in required training and other educational programs.  
Intervene and appropriately respond to any instances of sexual harassment or sexual assault; encourage others to do the same.
End of Accomplishments

- TODO Finish sri_gabon semi supervised model building and decoding.

The WER results are really looking bad for the second stage of the semi supervised training strategy. 
The transcripts that we thought look really good actually are really bad. 
I'm not sure why?
The output looks very fluent, but they don't look like they are aligned to the speech. 
I think they are just bad. 

- TODO Write the tr.

The bad results on the sri_gabon actually supports the point I wanted to make in the tr. 
My hypothesis is that if you put just a little effort in collecting some read speech, it makes a big difference. 
Our results might show this.
The results on the yaounde answers are very good (maybe). 
This is the case where there is an overlap in the speakers, so it is quasi semi supervised.
When you don't have this overlap, that is, the semi supervised case, you get bad results.

- Experiment:
I think I need to run another experiment in order to support the point I'm making in the tr.
My point is that there is a large payoff to collecting some read data on a speech data collection   mission. 
- Strategy 1:
Do supervised training first with the read speech concatenated to the out of domain corpus (gp+yaounde read)
Automatically transcribe the unlabeled corpus yaounde answers.
Do semi-supervised training with  yaounde answers automatically generated transcriptions 
Use these models to decode yaounde answers
- Strategy 2:
Do supervised training with only the out of domain corpus.
Automatically transcribe the unlabeled corpus sri_gabon
Do semi-supervised training with  automatically generated transcriptions of sri_gabon
Use these models to transcribe the sri_gabon data.

Find the amount of read data required to get similar results.


- MAP:
This comment is in the steps/train_map.sh script in kaldi:
# Train a model on top of existing features (no feature-space learning of any
# kind is done).  This script does not re-train the tree, it just does one iteration
# of MAP adaptation to the model in the input alignment-directory.  It's useful for
# adapting a system to a specific gender, or new acoustic conditions.

# Note: what we implement here is not the MAP from the paper by Gauvain and Lee,
# it's the simpler (and, I believe, more widely used) so-called "relevance MAP",
# implemented in HTK, where we add a fixed count "tau" of fake Gaussian stats
# generated from the old model, to the new 'in-domain' stats from the features
# and alignments provided;  and we only update the mean.  So if the new count
# is zero it just gives you the Gaussian parameters from the old model, but as
# you get more than about tau counts, it approaches the in-domain stats.
# We use 'gmm-ismooth-stats' in the command line because the equations for this
# are the same as the equations for i-smoothing in discriminative training
# (for which, see my [Dan Povey's] PhD thesis).

There is also a script called steps/decode_with_map.sh in kaldi. 

- GP:
I've started the process of building models on the GP corpus alone.
I'm doing this on my laptop for now. 
** Goals for Thursday:
- TODO GP model building on gpu machine
- TODO Write TR
- TODO Finish gp + yaounde sgmm model building
- TODO Read papers
* Goals for Tuesday set Monday:
- DONE Continue with the sri_gabon part of semi supervised model ASR training.

I'm not sure why I did not start this before leaving yesterday. The decoding of the sri_gabon data by the sgmm5 mmi models had finished.
I wrote the scripts for all the steps needed to complete the second stage of semi supervised training. 
I have so far run the mono, tri1 and tri2 training and decoding scripts.
As soon as the training finishes, I start the alignment for the next model set. I don't have to wait for decoding to finish.

- TODO Another pass on accomplishments and top 6.
- TODO Write more for tr.
- DONE The IDP form
I took a first pass at filling out the IDP form. It may not be possible for me to do this with JAWS.

I've spent most of the day writing and running the scripts to do the sri_gabon stage of supervised training.
I've almost gone through the whole process of training. 
The decoding lags way behind.
Something weird that has me worried.
The tri3 model decoding yielded results before the tri2 model decoding.
I checked and the tri2 models is decoding, it has not died.
The tri3 models are currently decoding the sri_gabon data.

I also read the sgmm accent adaptation paper.

As I'm getting ready to leave, the sgmm5 model set is being trained with the answers and sri_gabon semi supervised data.
Hopefully, this will be finished tomorrow and I'll start the last steps of this model building with the denominator lattice generation, mmi training and decoding.
I also hope all the graph building and decoding up to tri5 and sgmm5 is finished. 
I want to start writing the tr more seriously. 
This will motivate me to wrap up this project before I go on leave.
I'll have to fill in the missing pieces.
I might want to do an experiment where  I use the sri_gabon data as unsupervised training data with gp.
 
** Goals for Wednesday:
- TODO Another pass at top 6 accomplishments
- TODO Finish sri_gabon semi supervised model building and decoding.
- TODO Write the tr.

 
* Goals for Monday set Friday:
- TODO Fix the mess I got myself into  for trying to get rid of mllr.

I reran the script that makes the decoding fst graph. This takes a long time.
After this finished I ran the script to decode the sri_gabon data.
It still fails.
My guess is there is a problem with the features. 
My guess is that there is a problem with the file containing the map between the file names containing the extracted eatures and the files actually containing the extracted features.
My guess is that this file got corrupted when I ran the program to generate the mfcc features.
I am now rerunning the plp pitch feature extractor to see if this works and if my guess is correct.
The decoding is now running, although I won't know for sure until it finishes if my guess was right.
This is a good lesson for anyone who wants to learn kaldi.
The files like utt2spk, spk2utt, feats.scp, wav.scp, ... are very important in kaldi.
You're not going to get very far if you don't copy the pattern in these files.
these files get created/modified when I run the plp and pitch feature extractor.
Scripts downstream will fail if you change these files upstream.
So, I probably didn't have to do all the script rerunning I did.
I probably just had to rerun the plp and pitch feature extractor.
The decoder has an argument pointing to the directory containing information about the test data or in my particular case right now the data/sri_gabon data directory.
That information was incorrect, because I had run the mfcc extractor over the sri_gabon and that information was written to the data/sri_gabon directory.
The models I had been building used the previously stored plp pitch feature vectors.
So when I went to decode the data the input vectors were mfccs which have their standard dimensions.
The models on the other hand had been trained with plp pitch vectors which have a different standard dimension.

 
- TODO If this gets fix, decode the sri_gabon data

It apparently got fixed and I am going through the process of decoding the sri_gabon data.
It is taking a long time to decode the sri_gabon data with the sgmm5 models.

- TODO Use the automatic transcripts of the sri_gabon as semi supervised training data (try to finish this before moving on to nnet2 stuff).
- TODO make another pass on top 6 list and accomplishments
** Top 6 accomplishments
Name: John Morgan
Office:
Team Leader (No):
Grade: 
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations  at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  No
Recognition (awards, letters of appreciation, etc.)
 List significant projects you are working on & your specific responsibilities:
Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.
Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.
6 Most significant Actions / Impacts
1.  
Action: 1. Applied state-of-art Deep Learning approaches with several toolkits to in-house data sets for machine translation and speech recognition tasks of interest to the US ARMY.
Impact: An important outcome of this effort is a capability and expertise at using GPU-based technology in the MLCAB branch. 
2. 
Action: 1. 
Impact: As a requirement for this project I became proficient at python programming, which the branch can now count on as a capability. 
3. 
Action: 1. Research work with UMD professors on simultaneous translation. 
Impact: Also as a requirement for this project, I bcame familiar with the area of machine learning  called reinforcement learning, which should have an important impact on future applications of Deep Learning to NLP applications.
4. 
Action: 1. 
Impact: 
5. 
Action: 1. 
Impact: 
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY


As I'm preparing to leave, the sri_gabon decoding with sgmm5 models step is still running. 
I'm not sure what step I'll take tomorrow when this decoding  finishes. 
I'll try to skip directly to decoding with the discriminative mmi models, but I suspect this will fail. 
IIn that case, I'll have to take the next steps, wich are to align, extract denominator lattices and mmi train.
After that I should finally be able to decode the sri_gabon data.

The decoding finally finished.
So now I have sgmm5 transcripts of the sri_gabon data.
I also started the decoding with the discriminative mmi trained sgmm5 models.
it looks like this is running and I don't have to run the alignment, denominator lattice generation and mmi training over again.
The only problem was with the feature maps that I explained above.
I was thinking of waiting for this decoding to finish because it is for some reason very fast decoding, but the corpus is pretty big 7400 utterences.

** Goals for Tuesday:
- TODO Continue with the sri_gabon part of semi supervised model ASR training.
- TODO Another pass on accomplishments and top 6.
- TODO Write more for tr.
- TODO The IDP form
* Goals for Friday set Thursday: <2016-09-23 Fri>
- TODO Write scripts to run semi supervised training with sri_gabon data in addition to the answers data.
- TODO Write accomplishments
- TODO Write top 6 accomplishments document.
- TODO Write more on tr.
- TODO Run scripts to train sgmms discriminatively with mmi.

The script to train the denominator lattices had finished when I came in this morning.
I started the script to do mmi training on the sgmms.
Training finishes relatively quickly (maybe 2 hours).
I looked at the script to decode with the mmi discriminatively trained models. 
I needed to add the command to decode the sri_gabon data. 
Transforms are required to run this decoding.
You get the transforms by decoding the test data with the tri5 data.
So there 's some cheating going on here.

- Data preparation for sri_gabon semi supervision:
When I wrote the script to prepare the answers semi supervision training, I wrote the output training files to a directory called data/train_semi_supervision.
I did this so as not to overwrite the training files from the supervised training stage.
Now when I add the sri_gabon data, I am going to write the training files to a directory called data/train_semi_supervision_2.
The best WER was 23.79 slightly better than the best so far.
I think we need another test set.
The test set we've been using will be the devtest.

- online nnet2 
I'm trying to start the process of building an online nnet2 system on our African Accented corpus.
I got the recipe scripts from kaldi/egs/wsj/s5/local/online
The first step is to extract MFCCs.
These recipes use a different config file for the mfcc extraction.
They extract higher resolution features.
I guess they work better with neural nets.
They add the suffix hires to some files and directories.
Then a diag ubm set is trained.
They need to run an fmllr alignment step to do this training.
Then the lda mllt training is run.
Then the ubm training is run on top of this.

I am running into trouble.
The hires mfcc features hozed my old mfcc features.
I don't  think that's a big deal, I just have to run the old script over again.
It copied the data/ directories, so it should not hurt them. Just in case, I'm rerunnig the prepare script to remake them.
I'm going to skip the hires features for now.
I'm going to continue the training from tri5.
This is what the rm recipe does.
The wsj recipe gets fancy with the new hires features and copying directoreis ect...
So, I definitely need to rerun the script that extracts the mfcc features.

As I'm getting ready to leave I'm trying to take the last step of the first stage of semi supervised training.
I stillhave to decode the sri_gabon data with the best sgmm models.
The problem was that in order to do this, I had to decode the sri_gabon data with the tri5 models in order to get the mllr transforms.
That is now done.
I am waiting to get the mfcc features back after having hozed them.
Then I'll run the sri_gabon data through the decoder with the sgmm models.
This will give me sri_gabon automatic transcripts.
then I can start the building process over again with both answers and sri_gabon as semi supervised data.
Oops, I was not using mfcc features for the model building, so I did not actually hoze anything.
So, what do I do now for the nnet2 recipe?
Can I use plp + pitch   features?
I'm having problems decoding  the sri_gabon data too.
It looks like I messed up things when I tried to get rid    of mllr.
I'm going back to the step where I tried to make that change.
I started doing it at the ubm training stage.
I'm running that training script over again.
I'm going back that far, because the next step fails.

I ran the ubm training again.
Now I'm running the sgmm training (step 57)

** Goals for Monday:
- TODO Fix the mess I got myself into  for trying to get rid of mllr.
- TODO If this gets fix, decode the sri_gabon data
- TODO Use the automatic transcripts of the sri_gabon as semi supervised training data (try to finish this before moving on to nnet2 stuff).
- TODO make another pass on top 6 list and accomplishments

* Goals for <2016-09-22 Thu>
- TODO run scripts to build sgmms.
- TODO Write accomplishments.

Technical Competency:
Apply methods, theories, techniques, and skills learned in Computer Science
Ph.D. corsework and research at UMD to projects of interest to ARL and the
ARMY.

In this past year I took and passed 2 graduate courses, 1 in Scientific Computing and 1 in Database Management Systems.
The course in Scientific Computing covered the fundamental theory of optimization, which is  relevant to the computational solutions of many if not most problems in NLP and virtually all problems in Deep Learning.
Our branch has several of its own speech and text corpora that have not yet been curated. The DBMS course I took will help our branch utilize and share our data for the benefit of the Army.

As part of a research team at UMD, I developed software that uses Deep and Reinforcement Learning techniques to predict 
language behaviors of a simultaneous translator. 

Develop machine translation software using the theories and methods emerging
from the field of Deep Learning.

I became familiar with several toolkits for developing neural networks including google's open source Tensorflow, theano and keras. I used recurrent neural networks with long short term memory and gated recurrent units to perform Machine Translation. 

Use python programming skills to develop software in ARL projects.

Both the Tensorflow and Theano toolkits are implemented in python. I used my python programming skills to apply the tools in those toolkits to develop the MT systems I worked with.

Support the team project to implement an Automatic Speech Recognition system
adapted to  speech as spoken in African countries. 

I made good progress on this objective. 
I dedicated a lot of hard work to preparing our inhouse speech corpora  for processing by ASR system development tools. 

I can now build subspace gaussian mixture model based ASR systems with the kaldi toolkit. 
The recipes I have developed to prepare and process the African accented speech can be applied to our other holdings.

Cooperation:

Serve as a bridge between the multilingual computing branch and the academic
community
at UMD. 

Communication:

Publish a journal paper as first author on research on simultaneous
translation.


Customer Relations:

Respond to requests from team customers for advice and technical support on
issues concerning machine translation and machine learning.

Technology Transition:

Transition simultaneous translation code developed for research into branch
projects.


Diversity:
Support ARL's diversity initiatives by participating in locally sponsored
training, outreach and/or special emphasis programs to increase personal
awareness and understanding of the various cultures that exist among
laboratory employees.


SHARP:
Support leadership's efforts to address and prevent sexual harassment and
sexual assault and ensure a respectful work environment for all.
Demonstrate support for the SHARP program by actively participating in
required training and other educational programs.  Intervene and
appropriately respond to any instances of sexual harassment or sexual
assault; encourage others to do the same.


** Top 6 accomplishments
Name: John Morgan

Office:

Team Leader (No):
Grade: 
Series:
# of refereed papers published (Form1) 0
Single or first author0
Co-author0
# of non-refereed papers, reports, published (Form 1)0
Single or first author0
Co-author0
# Presentations  at conferences or significant briefings0
# Field Tests0
Did you complete your IDP ?  No
Recognition (awards, letters of appreciation, etc.)

 List significant projects you are working on & your specific responsibilities:

Project: Adaptation of Automatic Speech Recognition models from the well-resourced world language French to accents in regions of Africa. 
Responsabilities:
Preparation of data including acoustic recordings of speech  and text transcriptions.
Conducting experiments to determine best performing models.
Investigation of adaptation of mathematical models to be adapted including Hidden Markov Models, Subspace Gaussian Mixture Models and Deep Neural Networks.

Project: Application of Deep Learning methods to Machine Translation for language pairs of interest to the US Army.
Responsabilities: 
Bitext data preparation.
Setting up of computing environment with GPU 
Compare DL methods with existing Statistical MT benchmarks.
Conduct experiments to find methods that work best with low resourced language pairs like Dari and Pashto.



6 Most significant Actions / Impacts
1. 
Action: 1. 
Impact: 
2. 
Action: 1. 
Impact: 
3. 
Action: 1. 
Impact: 
4. 
Action: 1. 
Impact: 
5. 
Action: 1. 
Impact: 
6. 
Action: 1. 
Impact:  
Other Comments:
FOR OFFICIAL USE ONLY

-I'm running the alignment script that uses the first sgmms to align the data.
I'm also running the script to make the decoding graph for the first sgmms.
I decoded the test set with the sgmm5 models.
WER: 23.97

- SRI Gabon:
I plan to do the same for the sri_gabon data set that I did for the Answers.
At this point, if for no other reason than to got the steps of processing the data.   
I just realized that I have not been decoding the sri_gabon data as I've been taking the semi supervised stepsp.
I fired up a scrip to decode the sri_gabon data with monophones and triphones.
I don't really have to do this yet.
I only need to do it when I know which model set performed best.

As I'm getting ready to leave, the script that makes the denominator lattices is still running. I need them to do discriminative training. 
** Goals for Friday:
- TODO Write scripts to run semi supervised training with sri_gabon data in addition to the answers data.
- TODO Write accomplishments
- TODO Write top 6 accomplishments document.
- TODO Write more on tr.
- TODO Run scripts to train sgmms discriminatively with mmi.

* Goals for Wednesday set Tuesday:
- TODO Write more on accomplishments.
- TODO Run steps to build models from the Answers semi supervision .

The fst build for tri1 semi supervised had finished. I fired up decoding.
Tri1 semi supervised WER: 43.18
The previous fully supervised WER was 45.33. So we're still doing better.
I've moved on to training tri2 semi supervised models, which are the same as tri1 only that they use the tri1 alignments.

- TODO Investigate question: Is the new LM being used?
- TODO If unigram fst making continues, consider building a different kind
of dnn (nnet2, nnet3, chain models)

It stopped, but I don't think it finished successfully. Maybe it ran out of memory?

- Answers:
I am using the transcriptions from the best sgmm model system output as labels for training. I could also use them to get a better estimate of the WER after decoding.
I guess I don't want to do this because I don't want to overwrite the files I have there now that are working.
I've spent the day running the scripts to build the models 
. As I'm getting ready to leave the script that trains the first sgmm model is running.

This script takes a while.
I'll wait until tomorrow to run the script for the next step.
I've been running the scripts that make the decoding fst for the different models.

Here are the results I got today for the semi supervised yaounde + gp system:
model & WER
mono & 59.39
tri 1 & 43.18
tri2 & 39.05
tri3 & 43.78
tri4 & 41.49
tri5 & 32.62
tri5 ci & 49.67


The tri5 score of 32.62 is the best score I've gotten so far for 
triphone models without using sgmm or dnn techniques, so I'm still hopeful that I'll beat previous bests as I move to sgmms tomorrow.
however, the tri5 ci score of 49.67 is worrisome. It is quite a bit worse than its score of 44.02 for a previous run.
Why did this score come out so bad?
I'm also a little disappointed with the score I get for the answers  when I use the automatic transcripts as the reference.
tri3 & 88.64

I can't remember at which model I started using the automatic transcripts as refernces, but I'm pretty sure I did it for tri3.
I would have guessed this WER would be lower. I'm kind of testing on the training data. In fact, this is really disappointing.

- Dari
I got excited about working with Hazrat on automatically transcribing the data from the Afghan Military Academy. However, this data was already transcribed by transtac, so I'm standing down on that project.

** Goals for Thursday:
- TODO Write more accomplishments.
- TODO Run scripts to build semi supervised sgmm models.
* Goals for Tuesday set Monday: <2016-09-22 Thu>
- TODO Write accomplishments
- TODO Try to continue dnn training where we left off.

I started the mkgraph script yesterday evening. It is still running this morning. Top says it's running at 99% of a cpu.
Is something wrong?

- TODO Start over with Steve's new lm.
- TODO Write scripts to use the answers as semi supervision.

- Semi-supervision
I created a new directory data/train_semi_supervision
I concatenate and sort the 3 files spk2utt, utt2spk and wav.scp under the data/train directory with the same files under data/answers and write them to data/train_supervision.
I concatenate and sort the data/train/text file and the decoder output from the sgmm model run on the answers data and write this file to data/train_semi_supervision/text.

Then I run the plp pitch extractor script on the train_semi_suprevision directory.
I trained monophones using the data from the data/train_semi_supervision directory.
I'm going to try to decode the test set with this new mono_semi_supervision model set.
I'm going to use the same lm. 
I'll start using Steve's new lm soon.
I'm making the decoding graph (fst).
If the monophones don't do well, I don't think I'll continue with the supervision idea.
I decoded the test set with the new semi-supervised monophones.
WER: 59.39 versus 62.10 previously with only full supervision.
I think this means it's worth continuing down this road.
Aligning with semi-supervised monophones.
Naming the directories so that I don't hoze previous runs is getting harder.
I trained and formated the new lm.
Unfortunately, I overwrote the old lm and its fst.
I tried to avoid this, but I failed.
This is going to screw up my results if I don't go back to the old lm.
I'm not sure what to do at this point.
If I get better results, I might just go forward and redo all the steps with the new lm.
If the results are words, then I've got a problem.
I'll have to go back.

I've spent a couple of hours this afternoon writing the scripts to do the semi supervised build from monophones to sgmms.
This was mostly copying the supervised scripts and adding some words to directory names.
The mkgraph scripts take forever.

top reports numbers like 0.461t for virt and res and 82% for mem for the process that is making the fst graph for the dnn.
I am finished writing scripts to build the semi supervised sgmms.
I've written scripts to run decoding at each step.

I'd like to start the next step, but the scripts to run the graph making are still chugging away.

** Goals for Wednesday:
- TODO Write more on accomplishments.
- TODO Run steps to build models from the Answers semi supervision .
- TODO Investigate question: Is the new LM being used?
- TODO If unigram fst making continues, consider building a different kind of dnn (nnet2, nnet3, chain models).




* Monday, September 19, 2016 5:35 PM
To:	Larocca, Stephen A CIV USARMY RDECOM ARL (US); Vanni, Michelle T CIV 
USARMY RDECOM ARL (US); Hernandez, Luis CIV USARMY RDECOM ARL (US)
Subject:	Daily activities Report for Monday September 16 2016
Signed By:	john.j.morgan50.civ@mail.mil



** Goals for Monday set Friday:
- TODO Attempt to resume dnn training.
- TODO If that fails rebuild models?
- TODO Alternatively, transcribe answers with current sgmm models 
- TODO Do semisupervised training with answers (maybe sri_gabon too)
transcriptions.
- TODO Write accomplishments


When I left Friday evening, I tried to fire up monophone training after
rerunning the plp pitch extraction script. The monophone training script
gave me an error about feature dimension. The solution ended up being that I
had to delete the directories having to do with the training data. In the
monophone training I script I stole from the babel recipe, they split the
training data into sub corpora to do different kinds of training.
I think those sub corpora were linked to specific numbers associated with
the plp extraction. When I reran the plp extraction, those numbers got
changed. Right now, this is only a guess. But deleting all the directories
associated with plp and pitch extraction and training data subsetting seems
to have fixed the problem.

- Attempt to restart the dnn training.
The next step requires a unigram language model.
There are problems building this model and converting it into an fst.
The gp corpus still needs conditioning in order to get this to work.
I'm finding some problems with the gp corpus conditioning that might make me
want to restart the training from the beginning.
The s' was not separated from the rest of the word.
I thought this had already been done for the gp transcripts that come with
the corpus.
Apparently not.
This is bad. The tokenization is not consistent in the original corpus. Some
of the l' have been separated from the rest of the word. Bad Bad Bad.
I found 69 cases.
Do I have to do this by hand?
I wrote a script to reattach the apostrophes.
Problem coeur does not appear in the lexicon with the oe ligature spelling. 
Somehow the oe ligature spelling got into my transcripts.
Maybe when I converted from dos to unix and then to utf8?
I'm just going to stick with the oe spelling.
- Lexicon problems:
Since the d' are now separated, i'm finding words that start with d' that do
not have pronunciations in the lexicon.
I'm copying the d' entries to entries with the d' deleted and the initial dd
in the pronunciation removed.
Steve. I think I need your help here.
Now I'm getting problems with words that are lower cased.
I might be doing something wrong.

I'm finding problems in the lexicon.
The words pronounced au need to be fixed.
etat is in my transcriptions, but not in the lexicon.
I checked this.
One of the prompts is:
Jamais par le bras d'autrui, Grands Etat n'ont e"te" conquis.
Etat does not appear in the lexicon.
Is this a  mis spelling?
habituation was missing a space.

- roll back, revert, reset what ever it's called. I messed up the dictionary
and I wanted to get the version of the dictionary before I started changing
it.
I ran:
git checkout "REVISION_HASH~1" local/src/lexicon.txt
I wasn't sure this worked, so I got the old dictionary from the gp
directory.
I was still getting errors.
They were the errors that could be fixed with dos2unix.
So I'm back to a previous dictionary.

- I spent most of the day working on getting pronunciations for words in our
corpora.
Finally I got the fst to compile.
I hope I did not do anything stupid.
The script I'm running computes the cost of something ...
Then it creates FSTs.
It puts these under data/lang
Sorts the arcs.
Checks if it's stochastic.
Determinizes the FSTs.
.


I'm trying to restart the dnn training where I had left off.
I'm making a decoding graph.
I think this is the step that required the unigram fst.

** Goals for Tuesday:
- TODO Write accomplishments
- TODO Try to continue dnn training where we left off.
- TODO Start over with Steve's new lm.
- TODO Write scripts to use the answers as semi supervision.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 16, 2016 6:26 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 15 2016

** Goals for Friday:
- TODO Finish fixing problems with sri and gp data preparation.
- TODO Do top six accomplishments 
- TODO Try to recover dnn nnet training recipe 


I'm writing the scripts to prepare the sri_gabon data.
I have to avoid the conflict with the gp data.

OK it took me most of the day, but I think I finished data preparation  for
the sri_gabon corpus.
I still have to do the lm work and maybe dictionary work? Well ... I guess
I'm not finished yet.
I found more problems with the gp data prep scripts.
I'm training the lm with Steve's new selected corpus.
The data prep never ends.
I put the gp data in directories like gp_001.
Under each one of these directories the gp data has names like
fr001_001.wav.
kaldi does not like this.
It wants something like:
gp_001_FR001_001.wav
in the directory gp_001


I spent all day on data prep for sri_gabon and gp.

I had a problem getting the plp pitch extractioon to run.
It looks like I had to delete a previously created file.
cmvn

** Goals for Monday:
- TODO Attempt to resume dnn training.
- TODO If that fails rebuild models?
- TODO Alternatively, transcribe answers with current sgmm models 
- TODO Do semisupervised training with answers (maybe sri_gabon too)
transcriptions.
- TODO Write accomplishments

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 15, 2016 6:21 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 15 2016

** Goals for Thursday:
- TODO Write script to prepare sri_gabon data.
- DONE Assuming denominator lattice generation is finished, move on to
fdiscriminative training on the laptop so I can do the semisupervised
training with the answers.
- DONE If the dnn nnet pretraining finishes, move to dnn nnet cross entropy
training.
- TODO Incorporate Steve's sri_gabon prompt list into the lm.

The dnn nnet pretraining had finished, so I fired up the next step of cross
entropy training.
The denominator lattice generation for sgmm discriminative training has
finished, so I fired up the discriminative training script that uses mmi on
my laptop.

mmi training finished. I started the graph generation for decoding with the
mmi trained models on my laptop.

- sri_gabon data prep:
Steve gave me 3 files with potential prompts.
I've converted the 3 files to utf8.
I ran dos2unix to get rid of the cr.lf
I then ran iconv -f ISO_8859-1 -t UTF8 INFILE > OUTFILE
to put the file in utf8.


The script that runs the graph maker for the mmi trained sgmm decoding died.
It looks like it runs out of memory on my laptop.
I'm trying it again after rebooting.
It happened again:
The error message is:
std::bad_aloc

The dnn nnet training with cross entropy finished.
Decoding the test set:
WER: 29.22

Problem:
In trying to get the new sri_gabon data incorporated into my recipe I messed
up the old training data.
What should I do now.
I am going to stop the dnn nnet training until I finish preparing the
sri_gabon data.
The problem was with apostrophes.
Somehow, I was not conditioning completely the sri_gabon data.
Apostrophes were not being tokenized correctly.
So I was not able to make the unigram graph that is for some reason required
to do the next step in dnn nnet training.
When I was working on the sri_gabon scripts, I found some conflicts with the
gp corpus.
I made some modifications to the gp data preparation scripts and I did not
test them.
There were problems created.
I need to fix these problems.
To distinguish the gp from the sri_gabon data, I prepended a sri_gabon
prefix to directory names.
I haven't done this everywhere yet.
I really should do the same for the gp data.
Problem:
I am not deleting the data directory, because I was accidentally deleting
all my work.
There are some scripts that append lines to already existing files.
So these files are getting larger and larger each time I run the script.
I need to delete these files that get appended to.

** Goals for Friday:
- TODO Finish fixing problems with sri and gp data preparation.
- TODO Do top six accomplishments 
- TODO Try to recover dnn nnet training recipe 


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, September 14, 2016 6:31 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday September 14 2016


** Goals for Wednesday set Tuesday:
- TODO Figure out how babel does dnn systems differently. How do they do the
semi-supervision?
- TODO Read references.
- TODO Run script to  do semi-supervision with sgmm5 transcripts on the
laptop (let the gpu do the dnn build that I started today)
The dnn recipe run looks like it died. There seems to be no activity
registered since I left yesterday. Top reports that it is indeed running. I
don't understand what is going on. 
I think it could be an issue with the gpu.
The gpu is registering no activity.
Actually, I did find some activity in the log files.
Is it running on the cpu?
It turns out that it was running on the small gpu.
I deleted the process.
Justin reset the gpu.
I restarted the process.
It now reports to be running on the tesla gpu.

- sri_gabon corpus:
I should run our best system on the sri_gabon corpus.

I'm working through the scripts to prepare the sri_gabon data.
There are 5851 read speech files in the sri_gabon corpus.

There's a conflict between the gp and sri_gabon speaker directory names.
They both just number them.
I'll have to copy the files from /mnt/corpora  and rename them I do this for
gp already.
Steve will give me prompts for the sri_gabon data. They are not directly
associated with the file names. 
My plan is to randomly select sentences from Steve's list as transcriptions
for the sri_gabon data.
I really don't have to do this, but it might save time later.
I do want to include Steve's list in my language model.

This is going to take some dedicated work to get right.

The denominator lattice generation is still running on my laptop for the
sgmm discriminative training.
It should finish soo, but I need to leave.

** Goals for Thursday:
- TODO Write script to prepare sri_gabon data.
- TODO Assuming denominator lattice generation is finished, move on to
fdiscriminative training on the laptop so I can do the semisupervised
training with the answers.
- TODO If the dnn nnet pretraining finishes, move to dnn nnet cross entropy
training.
- TODO Incorporate Steve's sri_gabon prompt list into the lm.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, September 13, 2016 4:45 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday September 13 2016

** Goals for Tuesday set Monday:
- DONE Assuming denominator lattice generation finishes successfully,
continue with the mmi discriminative training of the yaounde + gp system.
- TODO Setup the scripts to use the automatic transcriptions of the Answers
data as training data.

- TODO Read reference papers for the babel system.


Yesterday before leaving and after I wrote my report I ran a decoding with
the yaounde + gp tri5 models.
The WER was 34.75.
I thought this was not good news. However, I looked again at the results
from my previous runs and the score for tri3b models was exactly the same
34.75.
The speaker independent version that alsow gets run at the same time gave a
WER of 4402 which is actually better than the previous run that gave 46.80.
I'm still anxious about the results for the sgmm5 models.
The sgmm5 WER came in at 27.72.
It looks like this is slightly worse than my previous results which were
27.47.
I'm not going to worry about such a small difference.
No. I'm actually slightly better at this point.
The previous WER was 28.38.
I'm still waiting for the results for decoding with fmllr.

I started working on a script to use the automatic answers transcriptions as
supervision for training with the answers data.

Discriminative training finished on the gpu machine.
When I run the mkgraph script it doesn't seem to work.

Maybe it was already done?
Yes. I think I already ran the script that makes the FSTs.

I need to write a script to decode the final sgmm5 models that were trained
discriminatively.
If I get good results, I'll be ready to move on.
The fmllr results are in, WER was 27.41, slightly better than before which
was 27.47.

OK, the decoding is done.
The results are slightly better.
The best wer is 25.85 which is down from the previous best of 25.98.

Time to move on.

I have to wait for the answers to get transcribed.

Change of plans:
I'm going to run the dnn recipe that I ran for the gp and yaounde builds.
I think it's the nnet recipe. I'm not sure if it's karel's recipe or Dan's
recipe. My best guess is that it's Karel's.
I'm running this because I had already run it for the other 2 builds and it
looks like I got improvements for those systems.
However, the babel recipes do things differently.

I'm going to run different recipes on the laptop and gpu machine.
On my laptop I'm going to run the semi-supervised training with the
transcripts produced by the sgmm5 system.
I won't have the sgmm5 complete on my laptop until some time tomorrow or
maybe even later.
Right now I'm generating denominator lattices on the laptop for the sgmm
discriminative training.


** Goals for Wednesday:
- TODO Figure out how babel does dnn systems differently. How do they do the
semi-supervision?
- TODO Read references.
- TODO Run script to  do semi-supervision with sgmm5 transcripts on the
laptop (let the gpu do the dnn build that I started today)


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, September 12, 2016 4:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday September 12 2016


** Goals for Monday set Friday:
- TODO Investigate how babel Cantonese builds deep neural network models
including bottle neck filters. How can I apply these methods to our yaounde
and other corpora?

I've decided to follow the babel naming conventions for my yaounde+gp build.
In babel (and other places) dnn recipes build off the tri5 models.
Babel calls it tri5, the previous recipie I was following (I think it was
rm) called it tri3b.
tri5 means there are 5 steps of triphone enhancements.. I could list them
...
tri1 builds off the monophone alignments.
tri2 adds delta features.
tri3 builds off the alignments from tri2.
tri4 uses lda and mllt
tri5 uses speaker adapted training with mllr.
In babel they don't even worrry about decoding test sets at all these lower
levels.
I wanted to read the paper on mllt, but I could not access it.
Apparently, mllt has to do with the covariance matrix. It's a way to use a
full covariance matrix instead of just a diagonal one.

After tri5 alignments are obtained recipes split off into sgmm training and
dnn hmm hybrid training.
 
- TODO Wrap up hmm builds for all 3 data set configurations.
I'm rerunning the yaounde+gp build through sgmm discriminative training
which is the only build yet to be completed.
I'm going to skip fmmi for now.

- TODO Ask Justin to put the babel Cantonese corpus under /mnt/corpora
I'm going to wait until the sgmm training I'm running now on the Cantonese
corpus to finish, otherwise I'm ready to move this corpus to /mnt/corpra.

- TODO Try using output transcripts for Answers as labels for training with
Answers.

- DONE symlink utils and steps directories.

The focus today is on finishing the pre dnn models for the yaounde+gp. This
means getting the sgmm models. Then training them discriminatively. I have
the basic sgmm models and the discriminative training is happening right
now. This is very slow, so it might not finish today.
I want to decode with the sgmm basic models.
When I look back at my previous recipe, I see that I used transforms from
the tri3b=tri5 models when decoding with the sgmm models.
I guess I have to do this again.
Yes, and to get those transforms I have to also run the mkgraph script for
the tri5 models.

I've been trying to  read the references in the  babel  summary, but the
journals make it way to hard to access their papers.

As I'm getting ready to leave I have a couple of processes running that I
should leave alone and come back tomorrow to check on.
I've got 1 process running on my laptop and several on the gpu machine.
On my laptop I'm running the basic training of the sgmm models.
On the gpu machine I have the script that runs the recipe for the Cantonese
babel recipe.
This is the first of many scripts.
It builds the tri5 and sgmm5 models, including the discriminatively trained
sgmm models.
Right now the script is making the denominator lattices for the
discriminative training.
I'm also running 2 processes for the yaounde + gp system.
I'm decoding with the sgmm5 models (not the discriminatively trained ones
yet).
Actually, I got mixed up. The denominator lattices are being generated for
the yaounde + gp system. The Cantonese system is doing the discriminative
training.

** Goals for Tuesday:
- TODO Assuming denominator lattice generation finishes successfully,
continue with the mmi discriminative training of the yaounde + gp system.
- TODO Setup the scripts to use the automatic transcriptions of the Answers
data as training data.
- TODO Read reference papers for the babel system.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 09, 2016 5:09 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 9 2016


** Goals for Friday set Thursday:
- DONE Copy the data from the Babel Cantonese dvd to the gpu machine ( I
can't believe how long this takes.)
- TODO Setup and run the Babel Cantonese recipe.
- TODO Finish the pre dnn part of the recipes for the 3 builds yaounde gp
and yaounde+gp.
The yaounde recipe run is still running. It is just getting to the fmmi
parts.

The gp recipe run had crashed this morning on the denominator lattices
creation for the sgmm2x_4a models.
There were some old graphs/FSTs hanging around. I deleted them and that
still did not fix the problem. There was an extra option setting the number
of jobs to split into. I removed this option and it seems to be running now.

The yaounde + gp build is still chugging away.


- TODO Read reference papers for kaldi and the techniques used in their
recipes.
- TODO Check on runs of yaounde dnn and yaounde+gp build from scratch.

- TODO Clarify the difference between karel's dnn recipe and the dnn hybrid
recipe that I've run on the gp and am running on the yaounde corpus build.



- Setting up babel Cantonese.
Modify the conf/lang -> lang.conf file for cantonese, this is going to
require several passes.
Lesson: The steps and utils directories are symlinked to the wsj versions of
these directories.
If babel can do this, I should do this too.
I  thought it was too good to be true when I was setting up my recipes.
From now on I'm going to symlink the steps and utils directories to our
installation of kaldi.
Specifically:
/home/tools/kaldi/egs/wsj/s5/{steps|utils}
This is going to save me a lot of space and effort copying scripts from thos
directories.
All the new scripts go under the local directory
- I was missing configuration files.
I went to the clisp cluster.
I found the place where the bable development was happening.
All the configuration files seem to be there and the paths coincide with the
ones in the configuration file.
I copied the config files to the gpu machine.
The first run script is running after some tweeks. 
I had to touch a dummy glm file.
The recipe seems to be using plp features with pitch instead of MFCCs.

Monophone training is happening now.

- yaounde:
The run is not finished yet as I'm getting ready to leave. However, it has
already done the decoding with sgmm 4a mmi b02 models. These were the best
models for the yaounde + gp data set build. They are not the best for the
yaounde data set build. They come in at 37.84. The sgmm with fmllr comes in
at 36.42, which is the best so far for yaounde.
The script still has to run the fmmi modeling. I don't think this will beat
the best, but who knows?

- gp:
The script I'm running for the gp data set configuration is doing lattice
generation. It will use these lattices to do discriminative training of sgmm
models.
Those are the sgmm mmi 4a b0 2 models that gave the best results on the
yaounde + gp data set configuration.
I'm actually running a dnn script on this data set configuration too.
I don't understand why I'm not seeing any activity on the gpu. I was seeing
a lot yesterday.

- yaounde + gp:
The script running on this data set configuration is at the stage where it
has built the tri3b models and generated lattices and discriminatively
trained with mmi and is now decoding.
After that it will move on to sgmm modeling.
So this won't finish until some time this week-end.


** Goals for Monday:
- TODO Investigate how babel Cantonese builds deep neural network models
including bottle neck filters. How can I apply these methods to our yaounde
and other corpora?
- TODO Wrap up hmm builds for all 3 data set configurations.
- TODO Ask Justin to put the babel Cantonese corpus under /mnt/corpora
- TODO Try using output transcripts for Answers as labels for training with
Answers.
- TODO symlink utils and steps directories.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 08, 2016 6:15 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday September 8 2016

** Goals for Thursday set Wednesday:
- DONE Check if the yaounde+gp run crashed.
It had crashed before I left. It was a chain models recipe. I think the
chain models recipe need a lot of work before they're ready for prime time. 
yBefore I left yesterday I started the gp build with Karel's dnn hybrid
recipe.
This recipe finished without errors
This morning I've started decoding the test set with the dnn hybrid models. 
The decoding of the test finished very quickly. 
wer 41.47. 
This  is the best so far for the gp builds.



- TODO Incorporate dnn recipes into my recipe.



I tried to incorporate Karel's dnn hmm hybrid models into my yaounde + gp
recipe. 
for some reason it started the whole recipe from the beginning. 
I'm not sure why this happened.
I'm starting from scratch with the yaounde + gp system.
There was a confusion. I'm running 2 different dnn builds. One is karel's
and the other is a dnn hybrid model.

I'm setting up the dnn recipe for the yaounde build.

I got the Cantonese babel corpus from Steve.
I'm putting it on the gpu machine.
Justin can put it on /mnt/corpora on Monday.
My plan is to run through the Cantonese babel recipe for dnn, bottle neck
features and if possible chain models.
My hope is that by observing a working example, I'll be able to replicate it
on my recipe for accented french.
It's taking for ever to retrieve the data from the Cantonese dvd.
OK, the copying finished. The second dvd did not take as long as the first.

I spent most of today getting the yaounde and gp builds to the point where I
can fill out the results table.
There are still sections I'd like to fill out.

** Goals for Friday:
- DONE Copy the data from the Babel Cantonese dvd to the gpu machine ( I
can't believe how long this takes.)
- TODO Setup and run the Babel Cantonese recipe.
- TODO Finish the pre dnn part of the recipes for the 3 builds yaounde gp
and yaounde+gp.
- TODO Read reference papers for kaldi and the techniques used in their
recipes.
- TODO Check on runs of yaounde dnn and yaounde+gp build from scratch.
- TODO Clarify the difference between karel's dnn recipe and the dnn hybrid
recipe that I've run on the gp and am running on the yaounde corpus build.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, September 07, 2016 5:31 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday September 7 2016



** Goals for Wednesday set Tuesday:
- DONE Check on processes on gpu.
They were all in different states of chrash.
I did get one good result from the yaounde + gp build. We broke the 26
percent WER barrier. We're at  25.98 with the sgmm2x_4a_mmi_b0.2 iteration
2.

- DONE  Get Justin to reboot gpu machine.
- DONE Run chain model recipes.
I started the chain model training run on the yaounde build. It is behaving
strangely. It seemed to be using the gpu at the beginning. Now it does not
seem to be using the gpu.
The script claims it fails then it continues. Something is not right.

- TODO Investigate bottle neck feature training.
- TODO Consider how to train on automatically transcribed Answers.
- TODO Figure out why the GP build is crashing on the Answers. This should
not be happening. It does not happen for the other 2 builds.

Justin is recompiling a new fresh version of kaldi for me. So I'm doing
clean up of directories and git repos.

The recompile is done.

I started the runs for the yaound+gp and gp builds.

I am going to stop running more than one build at a time.
I'm going to concentrate on the yaounde+gp for a while.
I killed the gp build.

The chain model yaounde+gp  build crashed

. There's a problem. Maybe it has to do with the parameters.
There are a lot of parameters and who knows what they should be set to.
The mini batch size for example.
I'm seeing problems with inverting matrices on the gpu. It  falls back to
processing on the cpu. This is strange.
This makes me think that there are problems with my parameter settings.

I started the dnn hybrid script. I think this is Karels recipe.

** Goals for tomorrow:
- TODO Check if the yaounde+gp run crashed.
- TODO Incorporate dnn recipes into my recipe.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, September 06, 2016 5:03 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday September 6 2016

** Goals for Tuesday set Friday:
- DONE What happened to the 3 runs?
I came in over the week-end to make tweeks and restart crashed runs.
I reordered the sgmm building before the nnet chain model building.
The fMMI recipe is not done yet.
- DONE Did tri3b decoding on Answers succeed for the 3 runs?
Yes. It looks like I can get transcripts for answers where ever I get
transcripts for test.
- TODO Did the runs reach the chain model step?
Only the yaounde build is getting there this morning.
- TODO Write on tr.
- TODO Get bottle neck model recipe from babel.
This will take some more work.


** Goal for today
- TODO Make the recipe scripts consistent for the 3 data sets.

The yaounde build is ahead of the other 2 builds because it is much smaller.
I've added the FMMI dubm parts to the yaounde recipe. I need to add these to
the 2 other scripts.

The chain part of the yaounde build is now at the point where it depends on
having the gpu available.

Something is wrong with the gp build. It keeps crashing on answers.


I've been looking into the automatic lexicon expansion described in babel.
The babel data came with syllable boundary marks. 
I found references to syllable taggers. One called EasyAlign uses praat and
htk.

I cannot log on to the gpu machine.
I guess it's bogged down with processes.

** Goals for Wednesday:
- TODO Check on processes on gpu.
- TODO Get Justin to reboot gpu machine.
- TODO Run chain model recipes.
- TODO Investigate bottle neck feature training.
- TODO Consider how to train on automatically transcribed Answers.
- TODO Figure out why the GP build is crashing on the Answers. This should
not be happening. It does not happen for the other 2 builds.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, September 02, 2016 4:22 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday September 2 2016

** Goalls for Friday set Thursday:
- TODO Check on the 3 recipe runs that I started this afternoon.
Specifically, did they crash on the answers? 

It looks like the answers are not crashing the run.sh recipe script now. The
WER is infinity.

Are there dummy reference transcriptions that are missing. I would be
surprised if there were none.

I don't know how to check this yet.

This was only working for the gp build this morning.

- TODO Did tri3b fmllr decoding work for the Answers? (important)

Bad news. The gpu machine is really bogged down with all the processes I'm
running. There were 96 processes running when I checked this morning. This
makes everythink slow.

The yaounde+gp is training the tri3b models this morning.

- TODO Mandatory Training
- TODO Write more on TR.
- TODO Finish the pass on the yaounde recipe run.sh script.
- TODO Read papers on kaldi.


I'm making a pass on the run.sh recipe script for the yaounde build.
The chain model  training command line has 28 options, including many
options for the jesus layer.
There are a lot of decodings happening in the sgmm model builds in the
recipe. 
I'm trying to do both the test and answers decoding for each kind of
decoding.
I'm not sure this is worth all the work.
I'd rather spend time on bottle neck feature (filter?) models.

- Good news:
it looks like the problem with decoding the Answers with tri3b models is
solved.
I think I had to go through the process of doing exactly what I had done for
the dev and test sets.
That was for the yaounde build.
The yaounde + gp is not ready yet.
Actually, it was the gp build that had the answers and questions correct.
I now updated the yaounde and yaounde+gp builds to include the answers
fixes.
I had made a modification to the script that puts the prompts in one file. I
assumed the numbers in the file names were separated by a dash. The names
for the answers files are separated by underscores.
That is fixed now.
This makes me feel  a lot better about letting these scripts run over the
week-end.
Weird: I guess it wasn't the gp build that had the answers fixed. Anyway, I
updated it too. Maybe I only had it running on my laptop?
As I'm getting ready to leave for the long week-end, I have all 3  builds
running from a fresh start.
I rewrote the order of the commands in the gp script. I do the sgmm before
the chain models. This makes more sense.
I'll rewrite the other scripts next week to put them in this order.
Unfortunately, they'll crash when they get to the chain models before they
get to the sgmm models, but I don't have the time today to fix this.



The output from the decoding should be labeled with the fold. Right now the
gp build on the tri3b mmi step labels the output of the test fold only as
decode. I really want this to be labeled decode_test. The answers fold
labeled as decode_answers and the dev fold labeled as decode_dev.
This is important because the output  later gets used for mllr transforms or
something like that.
I'll have to have all 3 scripts consistent on this point. 
116 processes are running on the gpu machine.

** Goals for Tuesday:
- TODO What happened to the 3 runs?
- TODO Did tri3b decoding on Answers succeed for the 3 runs?
- TODO Did the runs reach the chain model step?
- TODO Write on tr.
- TODO Get bottle neck model recipe from babel.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, September 01, 2016 4:20 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday September 1 2016

** Goals for tomorrow Thursday set Wednesday:
- TODO Figure out why tri3b decoding is not working for answers but is
working for tri3b dev and test.
- TODO  Check on chain models for yaounde+gp.


I checked the chain model build for the yaounde + gp data set.It died at the
first iteration of neural network training. The gpu was not detected.
This will be a show stopper until the machine gets rebooted.
In the meantime, I'll have to work on the other 2 scripts to get them to
this point. I'll also work on the issue of transcribing the Answers data
with the tri3b models.

I was surprised to find out that the chain models are dnn hmm hybrid models.

I'm going back to the data prep step to get the answers aligned with the
test and dev sets.
Instead of using transcriptions for each file as in the case for dev and
test, I'm going to use the questions that was asked to get the answer that
was spoken.

Decoding without mmlr works for answers.

I'm starting from the beginning with the gp data set build.
I'm hoping that having the (dummy) transcripts for the Answers will enable
fmllr decoding (tri3b) for the Answers.


I've done another pass on the gp and younde+gp run.sh recipe scripts.
I've started on the yaounde recipe script.
I've incorporated the questions to the answers in the scripts. The decode
scripts do not crash now when I decode the answers. The WER results are not
valid since the reference transcriptions wers are only dummy sentences (the
questions not the answers).

** Goalls for Friday:
- TODO Check on the 3 recipe runs that I started this afternoon.
Specifically, did they crash on the answers? Are there dummy reference
transcriptions that are missing. I would be surprised if there were none.
- TODO Did tri3b fmllr decoding work for the Answers? (important)
- TODO Mandatory Training
- TODO Write more on TR.
- TODO Finish the pass on the yaounde recipe run.sh script.
- TODO Read papers on kaldi.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 31, 2016 5:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 31 2016

** Goals for Wednesday set Tuesday:
- TODO Check how far my recipe scripts reached. As I'm preparing to leave
the gp and yaound+gp data set builds are doing the cleanup run. The Yaounde
data set build is training tri2b models.

I've spent the morning concentrating on the yaounde+gp run. I got some
really good news this morning. We broke the 30 WER barrier. The bad news is
that I haven't been able to apply these models to the Answers data. The test
results are worthless if I cannot apply the models to the Answers data.


- TODO write more of the tr.

The answers are not being transcribed with the tri3b models.
The test set gets transcribed. It seems to be using fmmlr transforms.
Somehow I have to get mllr transforms for the answers data.
What is the cleanup?
I guess it is a way to get feedback on the quality of the recordings?

I'm pushing forward on the yaounde + gp data set build.
I'm leaving the issue of decoding the answers for later. It'll take a lot of
concentration.

I'm working on the chain part of the recipe now.

python scripts are getting used now.
 There is a lot of information on chain models at the url below:

http://kaldi-asr.org/doc/chain.html



** Goals for tomorrow Thursday:
- TODO Figure out why tri3b decoding is not working for answers but is
working for tri3b dev and test.
- TODO  Check on chain models for yaounde+gp.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 30, 2016 5:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 30 2016



** Goals for Tuesday set Monday:
- TODO Check on status of run.sh recipe scripts ( they will probably have
failed).

I checked the yaounde + gp run. One thing I noticed is that the ansers
decoding fails, I knew this, but it has become an issue because it makes the
whole run.sh script fail when I have a || exit 1 phrase at the end of the
command.
I noticed that it fails because it does not find the data/answers/text file.
I need to incorporate a dummy file here so that the command does not fail.
My work around now is to remove the || exit 1 phrase.

Actually, now that I look at this closer, I think this is an important 
 file to have. The questions that were asked to the speakers. For each
answer, we should have the question.
I have a list of the scenarios and questions, but I do not have a list of
the question that was asked for each specific answer.

I had not included the options parsing script in the yaounde run.sh script.
This made the build start from the beginning each time I ran it even if I
specified the --stage  option. The --stage option does not get parsed unless
the options parsing script is invoked in the run.sh script.

- TODO Incorporate sgmm training into recipes.
- Todo Write more for tr
- TODO Do another mandatory training

- Questions to Answers:
I wrote a list of 300 questions for the Answers. I'm not sure they align
with the answers.

I'm incorporating the chains recipe into my recipe for the gp data set. I'll
do this for the other 2 recipes later.
There are references to jesus in this recipe.
I have no idea what this is about.

I'm incorporating a script to find bad utterances into my run.sh recipe
scripts.

Here's what I've written so far for the tr:
Bootstrapping A Question Answering Speech
Recognizer With Read Speech
John Morgan
August 30, 2016
Abstract
1 Introduction
Speech to speech (S2S) devices convert speech input by a speaker in one
language
into speech in a different language. The automatic speech recognition (ASR)
system is a key component of a speech to speech device. ASR systemss for S2S
devices are ideally trained on speech that is similar to the task for which
the
device will be used. S2S devices are intended to be used to enable dialogues
between speakers of different languages. Collecting this ideal kind of
dialogue
data is expensive. In order for the data to be used as training data for an
ASR
system it must be transcribed at the word-level. This transcription task is
a
major part of the reason why the data collection is expensive. A way to cut
back
on this cost is to obtain an automatically generated rough draft of the
dialogue
type of speech collected. If the data being collected comes from a language
that
lacks a corpus of speech data or if the data comes from a highly accented
flavor
of a well-resourced language, automatic transcriptions of the data will not
be
possible. One way to solve this problem is to collect a small corpus of
recitations
by each speaker in the data collection. We will refer to this as the read
part of
the corpus. The other part will be refered to as the conversational part. An
ASR system built with the small read corpus will not serve as a training set
for an S2S device. However, it can be used to obtain rough draft
transcriptions
of the conversational speech part. one reason this is possible is because
the
speakers in the read part are the same speakers that are in the
conversational
part. For scientific evaluation, for any ASR task, the speakers in the test
set and
training set are kept disjoint. The taskk becomes much easier when the
speakers
in the training and test sets are the same. The cost of building an ASR
system
with read speech is much lower than building one with conversational speech.
A pronouncing dictionary is the most expensive component of a phone-based
ASR system. For a system built with read speech there is no cost involved
with
transcribing the data. The transcriptions are given by the prompts.
1

2 Methods
Three corpora were used in this project.
The Yaounde corpus: collected in Yaounde ,the capital city of Cameroon. It
consists of two
parts: the read part which consists of recitations fof prompts and the
conversational part which consists of answers to questions.
nch part of the Globalphone corpus:
The Central Accord Corpus: Collected in Gabon from speakers from four
Central African countries. A
small part of the read part of this corpus was used as test data.
All the experiments performed in this project used the kaldi toolkit. Most
of the standard kaldi recipes were used. Three training sets were compared.
Yaounde Consisting of only the prompts from the Yaounde corpus.
GP Consisting of the Globalphone prompts.
Yaounde + GP Consisting of both the Yaounde and Globalphone prompts.
3 Results
model Yaounde GP Yaounde+ GP
monophone 64.03 69.06 62.10
tri1 59.57 56.07 45.33
tri2a 60.83 55.04 44.99
tri2b 64.57 57.26 43.46
tri3b 47.17 34.75
tri3b si 61.74 56.85 46.80
tri 4 48.58
tri4 si 63.31
Table 1: WER scores for models and training sets.
2


** Goals for Tomorrow Wednesday:
- TODO Check how far my recipe scripts reached. As I'm preparing to leave
the gp and yaound+gp data set builds are doing the cleanup run. The Yaounde
data set build is training tri2b models.
- TODO write more of the tr.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 29, 2016 5:27 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 29 2016

There was a problem with the network on Friday August 26 2016.
The network was not available for the entire day.
This meant I could not log on to my enterprise machine (JAWS license?).
The network seems to be working fine today.


** Goals for tomorrow Friday August 26 2016 set Thursday:
- TODO Check if processes have finished.
- TODO Do another sanity check.
- TODO Write more for tr
- TODO Investigate triphone training steps (maybe Dan Povey's paper?)


I looked closer at the kaldi recipes and I decided to take yet another step
back.
Everything is good up through tri1 model training and decoding.
Then there is a branch off to tri2a models.
These models use another step of deltas on the features.
Apparently this is a dead end. 

Model building continues on another branch.
The first step on this branch builds tri2b models.
These models use lda and mllt features.

I had confused myself by ignoring the difference between tri2a and tri2b. I
had renamed tri2a tri2 and tri2b  to tri3. This led to confusion. I thought
one built on the other.

I wrote a run.sh script that includes the tri2a step for the record, but
continues building models on the tri2b branch.
The kaldi recipes put all the steps in a run.sh script.
I was putting the steps in separate files, but now I've moved to writing
everything in the run.sh script.

I wrote run.sh recipe scripts for each of the 3 data sets.
I launched them on the gpu machine.

** Goals for tomorrow:
- TODO Check on status of run.sh recipe scripts ( they will probably have
failed).
- TODO Incorporate sgmm training into recipes.
- Todo Write more for tr
- TODO Do another mandatory training

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 25, 2016 6:00 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 25 2016



** Goals for tomorrow Thursday set Wednesday:
- DONE Check that the yaounde graph making script finished.
- DONE Check that the scripts for the other 2 data sets finished. Some of
the decoding seems to be stuck.

Everything seems to have finished, but I think I've gotten out of sync.
- TODO Do a sanity check on the scripts. They seem to have become
unsynchronized.
- TODO Continue the Constitution training on page 37.
- TODO Start writing the TR



model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri3 & 63.85 & 47.17 & 45.15


- tri3 results: 
Why did the gp models do so much better. Am I really comparing the same kind
of models?

- Sanity checking:
It looks like I'm taking diffeerent steps for the different models.
1. Monophones look consistent for the 3 training data sets.
2. Tri1 ditto
3. tri2: this may be where the steps go along different paths.
Yes.
Yaounde: 
input alignments: tri1_ali
output directory: tri2a
step script: steps/train_deltas.sh

gp:
input alignments: tri1_ali
output directory: tri2
step script: steps/train_lda_mllt.sh

yaounde + gp:
input alignments: tri1_ali
output directory: tri2a
step script: steps/train_deltas.sh


I think the extra step I'm taking for the yaounde and yaounde+gp training
data sets is a waste of time.
I'll go back and skip it and follow the steps I'm taking for the gp training
data set.
The yaounde+gp training step takes noticeably longer than the yaounde
training set script.



- tri2:
I'm going to take a step or 2 back.
The tri1 results are great.
The tri2 results get worse.
I'm going to try 1 more time to fix this.
The alignment step after the tri1 models have been trained and used to
decode can take an option to use graphs.
I'm going to try this.



model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri2 use graph alignment & 64.57 & 57.26 & 43.93
tri3 & 63.85 & 47.17 & 45.15
tri3 using graph alignment & 64.91 & 56.85 & 44.65

When I added the --use-graph option for alignment using tri1 models
, the decoding using tri2 models had mixed results. It was worse for
yaounde, the same for gp, and slightly better for yaounde+gp.

I'm not sure what is going on with the tri3 models.



- Mandatory Training:
I finished reading all 137 pages of the 508 version of the Constitution Day
training.

** Goals for tomorrow Friday August 26 2016
- TODO Check if processes have finished.
- TODO Do another sanity check.
- TODO Write more for tr
- TODO Investigate triphone training steps (maybe Dan Povey's paper?)

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 24, 2016 5:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 24 2016

** Goals for Tomorrow Wednesday set yesterday:
- DONE Check if the fst graph maker scripts finished successfully.
The 3 of them finished successfully.
- TODO Decode test and answers with resulting FSTs.
- TODO Get Steve  to eyeball the yaounde ansers output.
- TODO Run through speech to text tutorial for PySpeech.

- Triphone decoding:
I found a paper on the internet by Dan Povey that says that in kaldi they do
not use hand written questions for the decision tree clustering of
triphones. They use a data driven approach.

model & yaounde & gp & yaounde + gp
mono & 65.42 & 69.06 & 62.47
tri1 & 60.26 & 56.07 & 44.74
tri2 & 61.38 & 57.26 & 45.24
tri3 & 63.85 & & 

The tri2 models produce worse results. 
 I think they're only there to get alignments, but I'm not really sure.
The tri3 models don't look good either.
I think these triphone models are a waste of time.


These results continue to look more  reasonable however.
More data is better and adding relevant data is better.


-PySpeech tutorial:
I run the train_sid.sh script to get speaker id models as suggested in the
decode.sh script.
I think pyspeech assumes audio input is telephony quality, i.e. 8bit8k.
It says this in the tutorial.
It supports a couple of file formats, but all are telephony quality.
Anyway ... I gues we can always downsample.
Unfortunately, I don't think PySpeech is what we're looking for if we want
to do demos.
I hope I'm wrong.

- Constitution Day mandatory training:
There are 134 pages to read in hthis training.
This will take me several days to complete.
I'm at page 37 of 134.

** Goals for tomorrow Thursday:
- TODO Check that the yaounde graph making script finished.
- TODO Check that the scripts for the other 2 data sets finished. Some of
the decoding seems to be stuck.
- TODO Do a sanity check on the scripts. They seem to have become
unsynchronized.
- TODO Continue the Constitution training on page 37.
- TODO Start writing the TR

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 23, 2016 5:12 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 23 2016

** Goals for Tuesday: 
- DONE Check if lm finished training
The file with the training data ended up with  13.3 million segments and 143
million tokens.
The lm is only 149 mb. This seems small.
- TODO run monophone systems with new lm.
- TODO Investigate tri2 models and why they do not improve the WER scores.


- Monophone builds:
Yaounde: The steps that involve the new lm take much longer.
This is the drawback to using a large lm. 
GP and Yaounde + GP: I'm skipping a step that creates a lexicon for the
dictionary. I'm not sure this step is needed.




model & yaounde & gp & yaounde + gp
mono with small lm including CA test prompts & 39.23 & 43.96 & 38.59
mono with large lm no CA test prompts & 65.42 & 69.06 & 62.47



These numbers look really bad, but the transcriptions of the yaounde answers
obtained with these models are looking pretty good.

- PySpeech:
Justin set me up with an account on a sentos machine where he has installed
PySpeech.
I was able to run the tutorial script for extracting ivectors.

- Triphone builds:
I ran alignment and training today for the 3 training data sets.
As I'm getting ready to leave, I now have the triphone graph making script
running for the 3 data sets.

** Goals for Tomorrow Wednesday:
- TODO Check if the fst graph maker scripts finished successfully.
- TODO Decode test and answers with resulting FSTs.
- TODO Get Steve  to eyeball the yaounde ansers output.
- TODO Run through speech to text tutorial for PySpeech.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 22, 2016 5:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 22 2016

** Goals for Monday set on Friday:
- TODO Compare the wer for triphones
- TODO Build a better lm.
- TODO Test the new lm with mono system.

The processes I had started on Friday terminated and they (as far as I can
tell so far) terminated with no errors. So I started the next round of graph
making, alignment and training scripts for triphones.


model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59
tri1 & 44.03 & 38.84 & 28.04

This is a bit strange.
We moved from monophone to triphone models. There are 3 training sets:
1. Yaounde: WER went from 39.23 to 44.03
2. Global phone (possibly used Yaounde as dev set): Wer went from 43.96 to
38.84
3. Globalphone and Yaounde: Wer went from 38.59 way down to 28.04.

Why does the WER go up for The Yaounde-only trained models?
Otherwise these results are looking a lot more reasonable.
When we add relevant data, the WER goes down (and pretty substantially).

I did not take my own advice to be patiaent and I continued to the next set
of triphones:

model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59
tri1 & 44.03 & 38.84 & 28.04
tri2 & 47.22 & 41.80 & 29.97

The tri2 models perform worse for the 3 training sets.

I'm testing a guess that the problem is in the alignment step.
The tri1 models are used to align the data before training the tri2 models.
The script points to the directory data/lang_nosp_test_threegram.
I'm trying to point to the directory data/lang_nosp instead.
The alignment script finished without errors.
The training script for the tri2 models also points to the
data/lang_nosp_test_threegram.
The make graph script also points to the data/lang_nosp_test_threegram, but
this is probably correct. We need to make a graph for the test data.
I got the exact same WER score 47.22 when I swapped the
data/lang_nosp_test_threegram and data/lang_nosp directories.



- lm
I'm spending some time on the subs corpus for the lm.
I retrieved the files from the dvd Steve gave me.
Looking at the French English corpus.
I'm not  sure what the .idx file is.
There are 2 text files. I suppose these are the 2 sides of the parallel
corpus.
I'm repeating the steps Steve took.
Lowercase, Normalize, tokenize, and restrict to sentences with between 6 and
25 tokens.


- Mandatory Training:
I completed the AMC records management basic training.
** Goals for tomorrow:
- TODO Check if lm finished training
- TODO run monophone systems with new lm.
- TODO Investigate tri2 models and why they do not improve the WER scores.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 19, 2016 6:36 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday August 19 2016

** Goals from Yesterday:
- DONE Check if processes have finished  for building the gp yaounde ca
models .
This is running the sgmm2 training which takes a long time, but it's worth
it.
step 26.
At the end of the day I've abandoned this for now. I'll get to it later.
- DONE Check if the yaounde only model processes have finished.
steps 16 and 17.
I was decoding with yaounde only triphones. The scores get a lot worse.
Something is wrong.
Ditto, I've abandoned this for now and I'll get to it later.

- DONE lm 
check if the lm processing steps have finished.
This might be creating a training file that is too big.
Currently I'm running it on the gigafren corpus.

There was a bug in the script. I'm restarting this. I'll probably have to
come back later to use better data.
- DONE Answers:
Extract the text output from the log files to see how well the monophones
performed.
I wrote a script that does this.


- Planning:
The long term goal is to get transcriptions for the Yaounde Answers (YA).
Outline:
1. Produce an ASR system with our existing holdings: gp, yaounde read
(centrral accord?)
I guess we'll use ca for testing for now.
2. Decode the Answers with the system in 1.
3. Evaluate this method. Can this work for Central Accord?

As part of 1. Figure out the best system for decoding the Answers.
Concentrate first on monophones. Be patient.
Step 1: Compare the performance of 3 systems:
1. Yaounde  Read (YR)
2. GlobalPhone (Gp)
3. Yaounde Read + GlobalPhone (YRGP)


Note that there are questions about what devset data to use.
I think I'll ignore this question for  now while building the 3 mono
systems.


What about the LM?


- Monophones:
1. Train on YR test on CA. and decode YA
2. Train on GP test on CA and decode YA.
3. Train on YRGP test on CA and decode YA.

model & yaounde YA & gp YA & gp YR & yaounde + gp YA
mono & 39.23 & 43.96 & 30.60& 38.59

In the first row yaound YA means train on Yaounde and test on YA.
So the column labled gp YR  means trained on gp and tested on yaounde read.
Again we see that gp (30.60) is better than Yaounde (39.23). But this is
because the prompts for the test set (YR) are in the lm.
Forget it, I think the gp yr column is confusing. I have it there because
later on I'm going to use the YR as dev data.


model & yaounde & gp & yaounde + gp
mono & 39.23 & 43.96 & 38.59

So the Yaounde + GP system is the best.
I think this is better than what I was getting before. These results seem
more reasonable.
When we add the data from the 2 corpora, we get slightly better results.

I put the results of decoding the answers with the mono system in a tsv file
under:
/home/data/scratch/answers_decode_output_y_gp_y+gp.tsv


** Goals for Monday:
- TODO Compare the wer for triphones
- TODO Build a better lm.
- TODO Test the new lm with mono system.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 18, 2016 5:09 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 18 2016

- Alignment:
The script finished yesterday evening despite the power outage.

- Decoding:
Decoding of the test set finished.
WER: 72.53 exactly the same as before. This is good news. We are getting
consistent results.
It looks like decoding of the dev set did not survive.
I wonder if the power outage shutdown the gpu machine?
It's up and running now.
- ansers
I wwrote a script that makes 3 files for the ansers data:
wav.scp
spk2utt
utt2spk
For mfcc extraction, I think only the wav.scp is required.
I run the mfcc extraction script having it point to the ddirectory that
contains the above 3 files.
This extraction script succeeds.
So now I have the mfccs for the ansers data, can I decode it?
I'm trying to decode with the monophone system.
Hey! it looks like it is decoding.
I see text coming out of the log files.
So ...
This really should be done with the yaounde data alone.
This is going to be my major project for the next couple of weeks.
Plan?
What models should I use?
What LM should I use?
Are the decoding results, i.e. the text output, available in an easier
format than the log files?
This is putting our money where our mouth is.
Can we build a system that can decode data we have that is not transcribed.
If this works, this could be a method for data collection.
Collect read speech (maybe around 10 hours).
Collect free range answers to questions or scenarios.
Build a system with the read speech.
Use that system to transcribe the answers.
How well does this work?


- Yaounde only models
I'm going through the steps for building the yaounde only models.
I should probably work on the lm first.

- UN corpus
Should I use the UN corpus of French?
I am writing scripts to process this corpus to be used as data for an lm.

- Mono decoding of the Answers:
This finished.
Of course the scoring failed since there are no reference transcriptions.
I have not found a file containing a clean version of the output text.
There are log files that contain the output text.
I might have to use these files.


- Tomorrow:
Check if processes have finished  for building the gp yaounde ca models .
This is running the sgmm2 training which takes a long time, but it's worth
it.
step 26.
Check if the yaounde only model processes have finished.
steps 16 and 17.

- lm 
check if the lm processing steps have finished.
This might be creating a training file that is too big.
Currently I'm running it on the gigafren corpus.
- Answers:
Extract the text output from the log files to see how well the monophones
performed.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 17, 2016 4:47 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 17 2016

- Decoding with the 5e ensemble script:
The script is still running.
It is at pass 143.
I think it wil continue until pass 240, so it'll take another day.

- pySpeech.
Justin tried to install the rpm packages. 
We need to request the ubuntu or debian packages instead.

- Stupidity
I killed the 5e ensemble training process that had been running for 2 days.

I did this because I was rewriting the data setup script.
I was rewriting the data setup script because I had time on my hands waiting
for the 5e ensemble training script to finish :(

Anyway ... I'm happy with the data setup rewriting.
I separated the scripts by corpus.
This is the style followed by the kaldi scripts.
The scripts are under the local directory.
The main data setup script calls  data setup scripts for each corpus:
central accord, yaounde, and gp.

- Decoding Mono:
started the monophone decoding.

- Answers
I made a little progress on getting the yaounde answers data into a format
for processing.

- Tomorrow:
Continue rebuilding the ASR models.
Did decoding finish correctly?
What about alignment?
Continue processing the yaounde answers data.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 16, 2016 6:04 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 16 2016

- Results from the decoding run on the dev set for the nnet2 5a p-norm
script:




Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93
nnet2 p-norm & 50.55 & 59.65
net2 5c & 51.33 & 57.23


Notice that the sgmm2 scores are much better than the tri3 scores
The speaker adaptation kicks in at some point, but I think it's at the next
step.

- Training with the 5c script:
This training finished.

- Training with the 5e (ensemble) :
This script is still running.

- Decoding the 5c system:
See results above

The results are mixed. 


- Diving into nnet3
There's a script called run_ivector_common.sh.
The first thing it does is to perturb the data.
The mfcc data are perturbed.
Then it tries to do alignment.
Alignment is failing again, just as it did before.
I finally found the log file that shows where a problem is happening.
There are errors in the make_mfcc log files.
It says the wav files are much smaller than what is indicated by the header.
The number of bytes that are actually read looks correct.
The number in the header looks wrong.

- Tomorrow:
Check if the training run for the ensemble system finished.

Fix the problem with the mfcc feature extraction. this should fix the
alignment problem too.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 15, 2016 4:50 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 15 2016

- Monday:
Did the run_5b_gpu.sh script finish successfully?
No.
Alignment is still failing.

What now?
I think the problem might be that there is something specific to the corpus
that the script was intended for.


-Chain
This is the latest recipe being developed.
- tdnn
time delayed neural network

I'm trying a script from another recipe, the librispeech recipe.
It's named run_5a_clean_100.sh
It seems to be working.
At least it's doing neural network training.
It uses a script called:
steps/nnet2/train_pnorm_fast.sh
I think it must be using the alignments from the tri3 run.
The gpu is registering 99% usage.
Looking at some files under the exp directory:
lda_dim
360
ivector_dim
0
Does this mean I'm not using ivectors?
feat_dim
40

It looks like this nnet2 script only requires the tri3 alignment step which
I made in step 24.

Training finished.
- Decoding:



Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93
nnet2 p-norm & 50.55 &


I started training with a script called 5c.
I guess it uses the tanh activation functions.

I'm also looking at 5e, ensemble training.

I'm running the training scripts for both the ensemble and 5c methods
together.
I'm surprised this works. They don't make each other crash.

I just remembered that I also have the decoding script running on the dev
set data.

- Tomorrow:
Check how the 2 scripts finished.
They are  numbered 56 and 58.
Try to finish up with nnet2 and move on to nnet3.
Don't forget about decoding the yaounde answers data.
Also check the results of the decoding of the dev set. It is running the
script numbered 55.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 12, 2016 4:13 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Friday August 12 2016

- Yesterday I thought I had another step to go for the nnet build. I was
wrong. The only thing missing was the results of the decoding on the dev
set:


Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 69.93


The best score remains an sgmm with mmi.

- nnet2
nnet was written by Karel Vesely.
nnet2 was written by Dan Povey.

Dan first does something with the input features.
Several copies are made and they are perturbed.
VTLN warping and time warping.
vtln is vocal tract length normalization.
He says this is a way to artificially expand the amount of data.
There  is one command to perturb fbank features and one to perturb mfcc
features.
- Align
Dan's script refers to tri4b, I don't have that , so I'm using tri3.
tri4b seems specific to wsj.
Alignment is failing.
Why?
The training graph compiling is finishing successfully.

As I'm getting ready to leave, I am still stuck on the alignment step.
I'm going back and trying to run a script directly (more directly) from the
kaldi wsj recipe.
The wsj script refers to a specific fold of the corpus.
I replace that reference with the training fold in our corpus.
The script I am running now is:
local/nnet2/run_5b_gpu.sh
I think the 5 refers to the fact that 5 copies of the data are perturbed.
This script performs the feature extraction, laignment, training and
decoding.
I was separating out each of these steps into their own scripts.
Now I'm trying to run them all from one script.

- Monday:
Did the run_5b_gpu.sh script finish successfully?
If not, figure out what the problem is?
Otherwise continue with the other scripts in the local/nnet2 directory.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 11, 2016 5:43 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 11 2016

- Deep Belief Network pretraining
The script finished and apparently it finished successfully.
There was a problem getting this script to run.
The tail command probably had nothing to do with the problem.
( tail --pid=$$ -F $LOG_FILE_NAME 2> /dev/null)
This creates and opens the file in $LOG_FILE_NAME.
On the next line we had a command: like
$cuda_cmd $LOG_FILE_NAME SCRIPT_FILE_NAME arguments
The 2 variables in front of the name of the script file to be run are
variables that get assigned. 
The problem is that I had commented out the line where the cuda_cmd variable
gets assigned in my cmd.sh file.
The lesson to learn is that when you run a bash command, you can assign
environment variables on the commandline just before the name of the script
file.
 - Cross Entropy training:
Training neural network

The training stopped after 13 iterations.

- Decoding with nnet models:




Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33


The dbn pretrained dnn models are a little disappointing.
I did not do the make fst graph step.
Is it done somewhere in the scripts?
is it not required?
The next step is to align.
Then the denominator latttices are made.
This involves FSTs.

- smbr training
What is smbr.
I think it's sequential minimum bayes risk.
Train using minimum phone error (mpe)
Run 6 passes of this training.
Then make priors.
Is this smbr?
- Decode :
Decoding seems to be faster with these models.

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18
dbn pretrained dnn & 60.35 & 72.33
dbn pretrained dnn smbr & 58.76 & 

As I'm preparing to leave I am still waiting for the decoding of the dev
data to finish.
I also started the alignment using the current smbr models and I started the
denominator lattice generator. 
The denominator lattice generation indeed creates a decoding graph.

- Tomorrow:
Check if the decoding, denominator lattices generation and alignment have
finished successfully.
If so, continue with training and then decoding.
this would be the last step for the nnet2 system build.
Start the nnet3 build.




- An overview of nnet in kaldi is at:

http://kaldi-asr.org/doc/dnn.html

It looks like I want to go to nnet3.
Timit is not the recipe for nnet3.

- What is the output of the neural network?
This is an interesting question to keep in mind.
The neural network performs classification.
I think the input is a speech frame, i.e. an mfcc vector (maybe?)
It looks like the input vectors are enhanced with I vectors (maybe?).
Anyway ...
The output  of the neural network  is an assignment to a class.
What are the classes in the output?
There is a class corresponding to each context dependent state in the ASR
system.
The system is a network of states?



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 10, 2016 5:44 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 10 2016

- The script that makes denominator lattices for mmi training of sgm models
finished successfully. It looks like I just had to use fewer subjobs. I set
the number of subjobs to 1. Does this have something to do with
hyperthreading?
- Fired up the script that does mmi training of the sgmm2 models.
- Decoding 
There are 4 iterations of decoding in the timit recipe.
I guess lattices are rescored after each decoding.

iteration & test & dev & WER
1 & 50.05 & 58.57
2 & 50.70 & 58.45
3 & 50.55 & 58.51
4 & 50.89 & 59.18

There was a  "job failed" error in the dev decoding.
WER does not necessarily get better after rescoring.
I think these methods are geared towards very large systems.


Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06
sgm2 mmi 1 & 50.05 & 58.57
sgmm2 mmi 2 & 50.70 & 58.45
sgmm2 mmi 3 & 50.55 & 58.51
sgmm2 mmi 4 & 50.89 & 59.18



There's one more method in the timit recipe that combines the  dnn and sgmm
models. 
The scripts for this method depend on specific characteristics of the timit
corpus.
I'm skipping it for now.

- Karel's nnet build:
My first attempt failed.
I'm splitting his script into steps.
I'm stuck on the dbm pretrain step.
dbn stands for deep belief network.
A dbn is a stack of restricted boltzmann machines.

I'm talking to Justin about  getting this script to run.

Hi Justin,
The script I'm struggling with is at:
/home/tools/kaldi/egs/timit/s5/local/nnet/run_dnn.sh
The part I'm having trouble understanding and the place where it is failing
is around line 53.
The command on lines 56 and 57 apparently has 2 arguments in front of the
filename for the script that gets run.
The script steps/nnet/pretrain_dbm.sh gets invoked.

Anyway, I'm struggling with this script under:
/home/john/yaounde/kaldi-trunk/egs/gp_train_yaounde_dev_central_accord_test/
44pretrain_dbm.sh

When I invoke the command separately (not as a script) on the commandline it
seems to be working.
Actually, it runs in a script as long as I don't preceed the command with
the tail line and the 2 variable assignment arguments .

The dbn pretrain script command is still running and using the gpu as I'm
getting ready to leave.
	
I've written the scripts for the next couple of steps that follow.

- Tomorrow:
Check if the dbn pretrain script finished successfully.
If so, continue with step 45 to train nnet with cross entropy optimization.


- Longer term goal:
Use one of these systems (the best) to get a rough transcription of the
yaounde answers.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 09, 2016 5:40 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 9 2016

The mkgraph command finished successfully.
- Started decoding with sgmm2
- Started aligning with sgmm2



Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60

- mmi training
What is mmi?
mmi is maximum mutual information.
Apparently it is an alternative to maximum likelihood 
Discriminative models?
Minimize error instead of maximize probability.
I was having trouble with  the make_denlats_sgmm2.sh script.
I set the number of sub jobs to 1 and it seems to be working now.
the script makes denominator lattices for mmi training with sgmm2 models.
minimum phone error is an alternative to mmi.




- DNN hybrid system
I'm trying to figure out the recipes for building nnet, nnet2 and nnet3
systems.
I'm not even sure about the terminology to use here.
I think nnet, nnet2 and nnet3 are types of models in kaldi, analogous to
mono, tri1 and sgmm2.
The recipes refer to wsj, timit, voxforge etc, roughly to the corpora.
So, I'm trying to make a recipe for our Central Africa corpus using nnet
models.
I'm trying to copy the nnet model build from the timit recipe.
The script credits Karel Vesely.
A comment says that you can use his script local/run_dnn.sh
I should do this at some point, but right now I'm running a different code
chunk from the timit recipe.
There's a section called dnn hybrid.
 I ran a script that does neural network training.
Now I'm running another script that does decoding.
It looks like the script I'm running now builds  nnet2 models.
When I've finished this script, I want to go back to karels script, which
seems to build the nnet models.

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56
sgmm2 & 50.20 & 58.60
dnn huybrid (nnet2) & 51.86 & 56.06

It does not look like the dnn hybrid beat the sgmm2 for the test set, but it
is best on the dev set.


As I'm getting ready to leave I'm still waiting for the denominator lattices
script to finish.

- Tomorrow:
Check if the make_denlats_sgm2.sh script finished successfully.
If so, move on to  my step 32 in the train_mmi_sgm2.sh script.
Read papers on  ASR, mmi, dnn hybrid model etc 


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 08, 2016 5:22 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Monday August 8 2016

- Decoding dev set with tri2 models succeeded:
Wer: 70.12

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12

I'm not sure what to think of these results.

- Next Step: Align
Align what?
The timit recipe has a comment that says:
Align tri2 system with train data
Yes, we want to align the training data, I get that.
Align the tri2 system?
Yes, that makes sense too.
There is a slight difference.
We use data/lang_nosp instead of data/lang_nosp_test_threegram
these directories contain the FSTs for the dictionary, phone lists, etc.
We use the more general FSTs instead of the test versions.
Somehow this is going to enable speaker adaptation.



- SAT training:
Next we do speaker adaptive training.
The script says that it estimates fmllr transforms.
I guess those utterance to speaker files and speaker to utterance files get
used here.
I don't think I was doing speaker adaptation before, although the only
difference seems to be the data/lang_nosp directory.

- Decoding with tri3 SAT after the alignment

Model & test & dev
mono & 72.53 & 72.33
tri1 & 65.38 & 71.85
tri2 & 65.81 & 70.12
tri3 & 62.57 & 70.56

- Remember to return to this step to do dnn.

- Train ubm and sgm2

- Run Karel's dnn training:
There's a script that computes the duration of all the wav files in the gp
corpus.
The mean duration is 9.2.
The min is 0.5 and the max is 23.


- I'm waiting for the sgmm2 training to finish
- Attempted some mandatory training: hopeless
I'll have to get help.

- The sgm2 training finished, it took a while.
Now the mkgraph command is taking forever.

- Tomorrow:
Check if the mkgraph script finished successfully.
If so, decode using the sgm2 models in steps 28 and 29 and run alignment
with the sgmm2 models. in step 30
I wish I could have started these scripts today.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, August 05, 2016 3:53 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily activities Report for Friday August 5 2016

Decoding with the monophones did not successfully complete.
Actually, I'm not sure if the decoding succeeded, but the computation of the
WER failed.
The error message indicates that a hypothesis file is not found.
The hypothesis file is cameroon_m_001_001.
When I look at the log file for the decoding, I see a text string was
produced.
So I think decoding succeeded.
The problem is with the scoring.
OK, I found another missing script in a best_path log file.
It was a script that maps the integers to symbols.
I got the script and fired up the decoder again.

- Decoding succeeded for monophones trained only on gp and tested on CA:
WER: 72.53
- Note that the yaounde data was held out for dev data.
The lm was the same as before, namely: subs sample, yaounde scenarios and
questions, yaounde prompts, and gp training prompts.

Model & WER
mono & 72.53
tri1 & 65.38

I trained the tri2 models. In this case the tri2 models are trained with lda
and mllt.
I'm now decoding the dev (yaounde) data with the tri2 models.
My understanding is that this produces graphs (I think they're actually
lattices).
These graphs are then used in the next step to align the data again in
preparation for the tri3 models.
So the graphs depend on the dev (yaounde) data and hence the tri3 models are
speaker dependent?
The problem right now is that the dev data takes a long time to decode.
So I probably won't get to the next step until Monday.

Model & WER
mono & 72.53
tri1 & 65.38
tri2 & 65.81



I don't understand why the WER goes up.
What is the difference between tri1 and tri2?
Tri2 models use lda and mllt. It can cluster triphones with a decision tree.
I don't really have a set of questions right now, so I don't think much is
going on with tri2.

- Monday:
Check the results of decoding the tri2 models on the dev set.
If decoding succeeded, move on to step 19 to align the data with the tri2
models and the graphs produced by the decoding in step 18.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, August 04, 2016 6:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Thursday August 4 2016

The monophone training I started yesterday evening finished successfully.
- Continue building the gp yaounde ca system:
I moved the steps around a bit:
00 setup data prep
01 prepare dictionary
02  extract mfcc features
03  compute cmvn stats
04 train monophones
05 prepare and train lm
06 make fst decoding graph for monophone system
07 decode dev set with monophones
08 decode test set with monophones
09  start building the tri1 system by aligning the data with the monophone
models
10 Train the triphone models
11  make the fst graph for the triphone tri1 system 
12 Decode the dev set with the  tri1 models
13 Decode the test set with the tri1 models.

Up to this point I mostly copied the scripts from the previous gp system
build.
I found something strange in the next step.
14 Start building the tri2 system by aligning the data with the tri1 models.
In the gp system I had the script pointing to the test dictionary.
At least this is what I understand right now.
The timit recipe has the script point to the general dictionary.
15 Train the tri2 models 

- Problems:
The gp transcripts only need to be lowercased. I was normalizing and
tokenizing them.
I found the problem.
I was using a script from the  yaounde build to remove punctuation.
I should have been using the one from the yaounde+gp build.

The monophone models were failing to decode the test set.
I don't know why yet.
The error mesage says that the hypothesis for  a given file was not found.

I'll start over.

as I'm leaving today, I'm back at the monophone decoding step and the first
step in building the triphone models by aligning the data with the monophone
models.

- Tomorrow:
Did decoding with the monophones succeed?
If not, focus on this problem.
Otherwise continue building triphone models.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, August 03, 2016 5:40 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Wednesday August 3 2016

Result for Model trained only on Yaounde Read prompts and lm trained using
subs:
model & gp & gp + yaounde & yaounde
mono & 71.54 & 72.80 & 74.62
tri1 & 60.38 & 61.08 & 68.47
tri2a & 60.62 & 62.11 & 67.74
tri2b & 61.98 & 62.39 & 67.53
tri3b si & 60.77 & 62.33 & 68.04
tri 3b & 51.03 & 52.87 & 55.16
sgmm2 & 47.19 & 49.14 & 44.90


- The best performing system was the yaounde trained sgmm2
That number is a surprise. It is out of place in the table.

- nnet
I want to incorporate nnet into our recipe.
I'll start by incorporating it into the gp there than in the yaounde and
yaounde + gp systems.system since I have the steps labeled better 
Up till now I was following the recipe for iban.
To incorporate nnet, I have to look at other recipes.
The recipe for timit looks promissing.
Where do I insert the steps to build a nnet system?
From the timit recipe it looks like this is done after the alignment step
for the sgmm step but before the train ubm step is taken.
In my gp system step 21 uses the tri2b models for alignment.
Step 22 trains the tri3b model with sat.
Timit gets confusing here.
Are they doing nnet or nnet2?
I'll follow the timit recipe in any case for now.
I'm trying to get this to run by using the minimum number of scripts.
so I copy the command from the timit recipe.
I run the command.
I get a "missing script" error.
I copy the missing script into directory and run the command again.
the appropriate directory in my gp 
I've got the neural network training running.
It looks like the gpu is getting used.
I'm afraid this is going to take a long time to train ...
The nnet stuff seems to come later in the timit recipe.

The nnet code in the timit recipe refers to the dev data.
It is time to go back and restore a dev set.
I had ignored the dev set in trying to get a kaldi system up and running.

- Train on GP, Develop on Yaounde, and test on Central Accord.
Writing all the scripts to use gp as training data, yaounde as dev and ca as
test.
Done with step 0 the data prep setup 
done with step 1 dictionary prep
- step 2: lm data prep
I'm using the same script as before.
data:
subs sample, yaounde read prompts, yaounde scenarios and questions, gp
training prompts.
arpa 3gram
- Step 3: Extract mfcc features from dev, test and train data.
- step 4: compute cmvn stats.
- Step 5: train monophones.
I'm stuck here.
I've run into the same problem before.
It has to do with a program called feat-to-dim.
Some utils scripts are missing in my new directory.
They don't get traced when I use nohup.
I'm running the steps/train_mono.sh script without options.
It first initializes the monophone system.
Then it compiles the training graphs.
Then it aligns the data equally.
This is all working when I run the script without options.
The problem must be with one of the options.
The number of jobs is an option. I have it set to 12.
The default is 4.
So this is definitely one problem.
The number of jobs is limited.
I'm rerunning with 10 jobs. It seems to be working.

I'll stop here for today.
I'm going to delay asking Justin to reboot until I get to the tnnet
programs.

I feel like I made good progress today.
I got the nnet programs up and running. I thought that would be harder.
I'm going to incorporate the dev set into my recipes.


- Tomorrow:
Resume at step 5: monophone training.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, August 02, 2016 6:04 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily activities Report for Tuesday August 2 2016

- Building gp system baselines with sample subs lm:


model & gp & gp + yaounde
mono & 71.54 & 72.80
tri1 & 60.38 & 61.08
tri2a & 60.62 & 62.11
tri2b & 61.98 & 62.39
tri3b si & 60.77 & 62.33
tri 3b & 51.03 & 52.87
sgmm2 & 47.19 & 49.14

- Note that these results are very strange.
When we remove data that is similar to the test data the WER goes down.


- What about yaounde and the subs lm?
model & gp & gp + yaounde & yaounde
mono & 71.54 & 72.80 & 74.62
tri1 & 60.38 & 61.08 & 68.47
tri2a & 60.62 & 62.11 & 67.74
tri2b & 61.98 & 62.39 & 67.53
tri3b si & 60.77 & 62.33 & 68.04
tri 3b & 51.03 & 52.87 & 55.16
sgmm2 & 47.19 & 49.14 & 

Again today I'm leaving with one more result left to filll in the table.
The results in the table above are pretty consistent.
The system trained only on gp does best. The system trained on both gp and
yaounde does slightly worse. The system trained only on Yaounde is the
worst.

- Tomorrow:
Finish this part of the project  by getting the sgmm2 results.
Then move on to nnet.
This might take a while to get setup.
Try not to get distracted. It's important to get the nnet, nnet2 and nnet3
models into our recipe.
The sgmm2 training is currently running.
Pick up tomorrow with the next step that makes the fst graph and then do
decoding.
The lm remains an issue, but I'm going to stick with the lm I have now until
we get training data we're happy with.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, August 01, 2016 5:39 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily Activities Report for Friday July 29 2016

- Finish current experiment that uses an LM trained on the small sample subs
corpus an no test prompts.
I had started tri2a training.
Resume at step 15 by making fst graph for decoding with tri2a.

model & WER
mono & 72.80
tri1 & 61.08
tri2a & 62.11
tri2b & 62.39
tri3b si & 62.33
tri 3b & 52.87
sgmm2 & 49.14

- Run same baselines for gp with same (subs) lm

model & WER
mono & 71.54

This does not look right.
We added data, that is similar to the test data  and the WER went up from
71.54 to 72.80?

- Tomorrow:
- TODO Resume gp baselines at step 10 training tri1 models.
- Longer term
- TODO incorporate nnet into recipe


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 29, 2016 4:33 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Friday July 29 2016

This morning I fixed the data normalization problems I was having yesterday.
For each corpus I run the moses tokenizer tools:
lowercase
normalize
tokenize
deescape special characters

The deescaper has to be run because the tokenizer replaces special
characters with their xml tags. For example the apostrophe gets replaced
with &apos;. This is  a problem since I want to delete punctuation like the
semicolon.
I am running an experiment by including 4 sources of text in the lm.
1. The training prompts,
2. The scenarios and questions from the Yaounde data collection,
3. The words from the CA test prompts,
4. The small sample of the subs corpus.

- Decode using Monophones and CA word list:
WER: 72.52

- Decode same as above without CA word list:
WER: 72.80

- Decode with CA word list and no subs :
WER: 82.17

- Decode with Subs and CA test prompts and CA word list:
WER: 41.65

- decode with subs and ca test prompts:
WER: 42.06

- Decode with CA test prompts:
WER: 33.63

This is interesting.
The list of words contributes very little.
Including the test prompts in the lm (oracle) makes a huge difference.
The sample subs lm got us about 1/5 of the way to the oracle.

- Decode CA test set with tri1a models and subs lm no cheating:
WER: 61.08

** Goals for Monday morning:
- TODO Finish current experiment  with subs lm step 14 tri2a
- TODO Write script to prepare better lm
- TODO investigate recipes that use nnet

--- 
Thursday July 28 2016



Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 12.40

- Decoded sgmm2 models trained on gp+younde tested on ca includeing ca in
lm:
WER: 12.40

** First Goal for Today:
- TODO Repeat model building without including ca in lm training.
It's not just a matter of training the lm without ca and testing all the
models.
Why?
Because the models are built incrementally.
The tri1 models use the alignments from the mono models.
But ...
The alignments don't use the lm.
So ... I was wrong, it is in fact just a matter of training the lm without
ca and testing all the models.
The lm gets used only for decoding.

I trained an lm without the ca prompts -- call this the no cheating lm.
Now I'm decoding the ca test set with each model using the no cheating lm.
This is really easy to do in kaldi.
No ... I forgot a step.
The fst graph has to be built before decoding.
Before building the fst graph an fst has to be built for the lm.
The fst for the lm has to be run only once and each model can use it.
The fst graph that incorporates all the components has to be run before each
decoding run.
I forgot to run the lm fst building step so I was getting the same results
as before and I restarted from scratch.
In the future if I want to experiment with different LMs, I only need to run
the lm data prep, lm fst once, then the make fst graph for each model.
I'm actually trying to do this in the gp directory.
In order not to hoze the preexisting  work, I need to go back to the step
where the FSTs for the dictionary and lexicon are made.
This step also only needs to be done once.


- First result:
Monophone models trained on GP+Younde and tested on CA.
Lm without CA test prompts : 83.28
Recall that when tested with an lm that included the CA prompts the WER was:
32.50

-  Results for GP:
Without CA prompts in LM: 41.31
With CA prompts 33.46

So something doesn't seem right.
I did not start the GP build from scratch, so maybe there's contamination.


I did indeed make a mistake.
Instead of getting the data for the lm and excluding the CA prompts, I
concatenated the new data to the old data.
I reran the same setup an this time excluding the CA prompts and got the
results:
Monophone models trained on GP and tested  on CA: 
excluding the CA prompts from the LM: WER: 85.15
including the CA prompts: WER: 33.46
So, we have a problem. 

_ Data Prep
Basic text normalization needs attention.
I got bogged down all afternoon with this.
But it has to be done before moving on.
Problem:
Apostrophes.
There are several characters that are used for the apostrophe.
We want only one!
There are moses scripts that normalize and tokenize text.
I think they do a pretty good job.
Problem:
They convert the apostrophe into an xml escape character: &apos;
This is a problem since I want to remove semicolons (probably ampersands
too).
Also I don't think s&apos; is in our dictionary.
Solution:
Moses also has a script to deescape the xml tags for special characters.
This script maps &apos; to '.
That's where I'm at as I'm getting ready to leave.
I'm surprised I was getting good results despite this apostrophe problem.
I was removing all punctuation.
So s'il was mapped to s il
Now s'ill gets mapped to s' il .
I believe this is the correct way to work.

** Goal for Tomorrow:
- TODO Finish data conditioning
- TODO experiment with medium size LM (possibly from subs)
- TODO Focus on getting a good monophone  model set  with training and test
data disjoint.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 28, 2016 6:08 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 28 2016


Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 12.40

- Decoded sgmm2 models trained on gp+younde tested on ca includeing ca in
lm:
WER: 12.40

** First Goal for Today:
- TODO Repeat model building without including ca in lm training.
It's not just a matter of training the lm without ca and testing all the
models.
Why?
Because the models are built incrementally.
The tri1 models use the alignments from the mono models.
But ...
The alignments don't use the lm.
So ... I was wrong, it is in fact just a matter of training the lm without
ca and testing all the models.
The lm gets used only for decoding.

I trained an lm without the ca prompts -- call this the no cheating lm.
Now I'm decoding the ca test set with each model using the no cheating lm.
This is really easy to do in kaldi.
No ... I forgot a step.
The fst graph has to be built before decoding.
Before building the fst graph an fst has to be built for the lm.
The fst for the lm has to be run only once and each model can use it.
The fst graph that incorporates all the components has to be run before each
decoding run.
I forgot to run the lm fst building step so I was getting the same results
as before and I restarted from scratch.
In the future if I want to experiment with different LMs, I only need to run
the lm data prep, lm fst once, then the make fst graph for each model.
I'm actually trying to do this in the gp directory.
In order not to hoze the preexisting  work, I need to go back to the step
where the FSTs for the dictionary and lexicon are made.
This step also only needs to be done once.


- First result:
Monophone models trained on GP+Younde and tested on CA.
Lm without CA test prompts : 83.28
Recall that when tested with an lm that included the CA prompts the WER was:
32.50

-  Results for GP:
Without CA prompts in LM: 41.31
With CA prompts 33.46

So something doesn't seem right.
I did not start the GP build from scratch, so maybe there's contamination.


I did indeed make a mistake.
Instead of getting the data for the lm and excluding the CA prompts, I
concatenated the new data to the old data.
I reran the same setup an this time excluding the CA prompts and got the
results:
Monophone models trained on GP and tested  on CA: 
excluding the CA prompts from the LM: WER: 85.15
including the CA prompts: WER: 33.46
So, we have a problem. 

_ Data Prep
Basic text normalization needs attention.
I got bogged down all afternoon with this.
But it has to be done before moving on.
Problem:
Apostrophes.
There are several characters that are used for the apostrophe.
We want only one!
There are moses scripts that normalize and tokenize text.
I think they do a pretty good job.
Problem:
They convert the apostrophe into an xml escape character: &apos;
This is a problem since I want to remove semicolons (probably ampersands
too).
Also I don't think s&apos; is in our dictionary.
Solution:
Moses also has a script to deescape the xml tags for special characters.
This script maps &apos; to '.
That's where I'm at as I'm getting ready to leave.
I'm surprised I was getting good results despite this apostrophe problem.
I was removing all punctuation.
So s'il was mapped to s il
Now s'ill gets mapped to s' il .
I believe this is the correct way to work.

** Goal for Tomorrow:
- TODO Finish data conditioning
- TODO experiment with medium size LM (possibly from subs)
- TODO Focus on getting a good monophone  model set  with training and test
data disjoint.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 27, 2016 5:54 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Wednesday July 27 2016

- Results for GP training on Yaounde corpus as test:
WER: 87.04
The Yaounde prompts were not included in the lm. Apparently this makes a big
difference.

** Goals for Today:
- TODO Finish Gp system testing on CA
- TODO Finish GP+Yaounde system testing on CA


- Decode GP tri1 on CA:
WER: 29.54

Model & WER
Mono & 33.46
Tri1 & 29.54

- Problem at steps to build tri2a models.
It looks like  I skipped a number in the steps.
Step 14 uses the tri1 models to align.
Step 15 should train the tri2a models.
Step 16 should make the fst graph that includes the trained models from step
15.
So I need to swap my scripts for steps 15 and 16. I had them in the wrong
order.

- Decoded using gp tri2a models on CA data:
WER 29.18

Model & WER
Mono & 33.46
Tri1 & 29.54
Tri2a & 29.18

- decode gp tri2b models on CA:
WER: 31.71

- Decode gp tri3b models on CA:
WER: 26.10

Model & WER
Mono & 33.46
Tri1 & 29.54
Tri2a & 29.18
Tri2b & 31.71
Tri3b & 26.10
Tri3b si & 34.82
Sgmm2 & 21.67


- Build gp+yaounde system
-Decode  with monophones  on CA
WER: 32.50

The corresponding WER for the GP alone system was 33.46

- decode with gp+yaounde tri1 on CA
WER: 22.45

Model & Gp & gp + yaounde 
Mono & 33.46 & 32.50 
Tri1 & 29.54 & 22.45
Tri2a & 29.18 & 22.84
Tri2b & 31.71 & 21.67
Tri3b & 26.10& 16.72
Tri3b si & 34.82 & 24.47
Sgmm2 & 21.67 & 

- Suspicious:
The score for gp+yaounde tri3b and gp sgmm2 are the same: 21.67

I won't have the final sgmm2 score for gp+yaounde until tomorrow,but the
gp+yaounde results look like what we would expect.

- I'm finished with testing the mono, tri1, tri2a, tri2b, tri3b, tri3b si,
and sgmm2 models on the central accord test set after training on global
phone and including the central accord prompts in the lm.
I feel pretty good about the results posted above.

There's only 1 result left to compute for the same models as above but
trained on gp and younde.
- Yaounde Answers:
I spent a lot of time this afternoon trying to rename the files.
It looks like I finally succeeded.
I still have one step to go in renaming the Answers.
For the read data I made the speaker numbers unique.
Right now the speakers from machine ctell 2 start over at 1.


Michelle got me the questions for the Yaounde corpus.
- Plan:
- Incorporate these questions into the lm and decode the Yaounde answers.
Hopefully, this will help transcribing the data.

** Tomorrow:
- TODO Continue gp+yaounde at step 28
- TODO gp and gp+younde without ca prompts in lm
- TODO Finish renaming yaounde Ansers
- TODO Incorporate Scotia's transcripts into lm

** Longer term goals:
- TODO Decode Yaounde Answers (with or without younde read in training?)
- TODO Incorporate nnet, nnet2, and nnet3 into recipes
- TODO Build appropriate lm



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 26, 2016 6:07 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Tuesday July 26 2016

- Directory renaiming
The structure only has 2 levels now.
COUNTRY_GENDER_SPEAKERNUMBER/COUNTRY_GENDER_SPEAKER_NUMBER_UTTERENCENUMBER.w
av
For example:
Cameroon_m_001/Cameroon_m_001_001.wav
This might cause trouble down the road, since I think the files need to be
ordered by speaker number.

There's a problem with the script that writes the test utt2spk file.
Well ... there was a problem, it seems to be working now.
The utt2spk file has 515 lines, one line per utterance.
Each line has 2 fields: the utterance and the speaker.
-  utt2wav script
This points from each utterance to its wav file.
- Finished preparation of transcriptions
- LM prep
Make an LM with the training prompts?
Should we include the test prompts?
I'm including them for now.
- lang prep
- mfcc extraction
It looks like mfcc extraction succeeded for the new test data
- Ditto for cmvn stats
- mono training and lm2fst
- make fst graph
- decode mono:
WER: 41.61

This is a system trained on 85 speakers from the Yaounde corpus and tested
on 12 speakers from the Gabon corpus.
The lm included the Gabon test prompts.

- align with mono
- make fst graph for tri1
- decode with tri1
WER: 46.02
Up from 41.61 to 46.02
Why?
- align with tri1
- train tri2a
- make tri2a fst graph
- decode with tri2a
WER: 46.98
Even worse? Up from 46.02 to 46.98

- align with tri2a
- train tri2b (lda + mllt)
- make fst tri2b graph
- decode with tri2b
WER: 50.18
What! 

- Align with tri2b
- train tri3b (sat)
- make fst tri3b graph
- decode with tri3b
WER: 31.99
Finally some improvement!

- decode  tri3b si
WER: 48.04
- align with tri3b fmllr
- train ubm (Gaussian mixture models)
- train sgmm2 (subspace gmm)
This seems to take longer.
There's a lot going on in this  sgmm training script.
Realignment seems to be taking a long time.
Tre clustering, model initialization, Gaussian selection, graph compiling,
alignment conversion, 5 training passes, data realignment ..., more training
passes, more realignment, , more training passes ... , more realignment,
alignment model lbuilding, 
- make sgmm2 fst graph
- Decode with sgmm2
WER: 23.84

Summary of WER scores:
Model & WER
Mono & 41.61
Tri1 & 46.02
Tri2a & 46.98
Tri2b & 50.18
Tri3b & 31.99
Tri3b si & 48.04
Sgmm2 & 23.84

- Moving on to GP
I need to incorporate the central accord data as test data instead of the gp
folds.
I'll  use all the gp data for training.
The renaming script I had written is not working.
I found the problem.
The gp names end up out of order.
I have a script that renames them.
I have to point to the place where the new files with good names get stored.
I don't think this is taken care of in the kaldi recipe for gp.

- interesting lm problem
By mistake I made a training set that was the concatenation of the same data
3 times.
This training set makes srilm fail to produce a 3gram lm.
Srilm succeeds when I use only 1 copy of the data.
- gp data prep
- Decode with gp training and ca test, lm includes ca prompts:
WER: 33.46

- decode gp traing on yaounde corpus  as test data:
This will take a while since there is a lot of yaounde data.
I'll get the results tomorrow.

Tomorrow:
- Continue building gp system
I'm running step 11 which trains the tri1 models.
I'll also get the results for running the gp models on the yaounde corpus.
- Continue building the gp+yaounde system
I'm running step 5 that trains the monophones.






- Similarly fixed spk2utt script.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, July 25, 2016 6:12 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

** Goals for today:
- TODO Finish the last step for the GP system.
- TODO Incorporate the Gabon test set into the gp system

** Longer term goals
- TODO Incorporate kaldi's nnet, nnet2, and nnet3 programs into our recipe.

My laptop disk has no more space on it.
I removed some repeated corpora that I had under /home/data.

I have no permissions on /home/data/scratch/CA_test_set

- Ran step 29:the script to make the decoding graph 
- running decoding with sgmm2_5b2
- Decoding GP with sgmm2_5b2 finished:
WER: 24.32

WER Summary of GP system:

Models & WER
Mono & 41.80
Tri1 & 29.08
Tri2a & 29.01
Tri2b & 27.97
Tri3b.si & 29.66
Tri3b & 26.38
Sgmm2_5b2 & 24.32

- processing the central Africa data
- renamed directories.
The pattern is:
COUNTRY/GENDER/SPEAKERNUMBER/xxx.wav

For example:
Cameroon/m/001/001.wav

- Write data setup scripts to handle central Africa test set

- Got bogged down on file names.
There was a file without a leading 0 in the utterance number.
Cameroon/m/001/24.wav


- Maybe a missing wav file for a .txt file?

I renamed gabon/m/004/0{38-50}.wav
and gabon/m/007/u30.wav


- This scheme is not working.
I can only have 2 levels in the hierarchy.
Renaming directories again.
I'm not sure what the best way to do this is.
Some scripts depend on a correct or specific kind of ordering.
I'm not sure I  have the right ordering now.

Tomorrow:
Resume data prep work .
Yaounde directory
00data_setup.sh script
Local/get_utt2spk_test.sh


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 22, 2016 6:42 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

- I'm back !
Training tri2b finished successfully.
Step 20 that makes the graph ran without any logging output. This seems a
little suspicious, but the next step, decoding with tri2b seems to run ok.
For some reason this decoding step had the number of jobs set 2 only 2
instead of 10 as in previous steps.
I think this is a mistake, it's taking longer.
- decoding with tri2b (lda mllt)
WER: 27.97
Improvement from 29.01 to 27.97
- aligning  with tri2b and graphs.
Super fast
- train sat tri3b
- make graph with tri3b
This writes lattices first to a directory called decode_test.si
The scoring using these lattices yields:
WER: 29.66
I wonder if the SI stands for speaker independent?
Now it has gone on to write lattices to the standard decode_test directory
It looks like transforms are also written to this directory.
WER: 26.38
Improvement from 27.97 to 26.38
- aligning using tri3b
- training ubm
- training sgmm2

I'm not going to finish  the last decoding step today.
- Monday: resume with steps 29 make graph 30 decoding.

- Change of plans for next week:
Start over using Gabon as test set.

-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 22, 2016 3:00 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: DailyActivities Report for Friday July 222016

- Summary of Yaounde WERs from yesterday:
Model & WER
Mono & 27.71
Tri1 & 24.74
Tri2a & 24.92
Tri2b (lda mllt) & 24.32
Tri3b (sat) & 24.37
Sgmm2_5b2 & 1412

-  GP system today
- Ran data prep, mfc extraction, lm and dict prep.
- The script that creates the fst graph takes a long time. 
Ok, I know what is wron.
I have it pointing to the big lm I created yesterday.
I'm going to back off this and use the small lm.
- I started over since I had some old directories hanging around that  might
have gotten used.
I've been writing separate scripts to run each step and naming the file with
an initial step number so it is easy to build the system by running each
step in sequence.
The kaldi style is to put all the steps in a file called run.sh.
So I'm appending each of my scripts to a file called run.sh.
- Decoded using monophones:
GP:
WER: 41.80
Yaound: 
WER: 88.39

The Yaounde prompts are not in the GP lm, that why this last WER is so bad.

- Aligned
- trained triphones with delta+delta-delta features
- Made fst graph
- Decoded with triphones
WER: 29.08
Big drop here from 41 to 29
- Aligned
- train tri2a
Runs 34 passes
No. I made a mistake here. I ran the training before making the graph. I
don't understand how the training ran without having made the graph
previously. I guess training does not require the graph?
- Now run make graph
- run decoding  with tri2a system
WER: 29.01
Slight improvement from 29.08 to 29.01
- aligning with tri2a
- training tri2b

I'm leaving now to fix my bike with Phil David.
If we finish early, I'll return. Otherwise, next week pick up on step 20 to
make the tri2b graph.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 21, 2016 5:35 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 21 2016

- Working on yaounde system with small lm
- Decoded tri1
WER: 24.74
- Align with tri1 system

There's an incremental building process.
Train models, 
use models to align the data
train models with realignments

So we trained tri1 models, then we used them to get alignments, next we
train tri2a models.

More specifically:
Run steps/align_si.sh 
Run steps/train_deltas.sh 
Run  utils/mkgraph.sh
Run steps/decode.sh

The last mkgraph step ran very quickly. Too quickly?


Stephen Tratz suggested I ask Justin to turn off hyperthreading to speed up
processing. Maybe Justin did this already?

Decoding is taking its time ...

Mkgraph is not multithreaded
Decoding uses multiple jobs
I set the number of jobs to 10.
Too many jobs?

Decoding finished.
WER: 24.92
The WER went up slightly from 24.74 to 24.92.


Now we align again this time using the tri2a models.
Alignment is super fast.

Next we run a new kind of training:
Steps/train_lda_mllt.sh
This will be system tri2b

What is lda_mllt
It is a method for transforming the mfcc features.
Lda refers to linear discriminant analysis not latent dirichlet allocation.
Lda is a method for dimension reduction.
Splicing is performed over several frames.
What is spliced?
The splicing then requires dimension reduction?
Mllt a.k.a. ctc is a diagonalizing transform

Reading: 
http://kald-asr.org/doc/transform.html

lda is typically used for non-speaker specific transforms

lda_mllt training finished
run mkgraph for tri2b system
Again this went really quickly.
Decode using tri2b system

WER: 24.32
It's not clear to me what is being adapted to what here.


- Align again using tri2b system.
Here --use-graphs option is given to the align_si.sh script.
What does this do?
It uses the graphs to do the alignment.
- train using train_sat.sh
Sat stands for speaker adapted training.
It looks like the train_sat.sh is a script for doing mllr adaptation.
The mllr can be done on a system whose features have already been
transformed with lda and mllt.

- Make the graph for a new system tri3b.
What about tri3a?

- decode
This uses steps/decode_fmllr.sh

Lattice rescoring?
It looks like this script decodes once, then rescores the lattices and
decodes again.

WER: 19.22


- alignment
Uses steps/align_fmllr.sh
- train using steps/train_ubm.sh
This trains a mixture of gaussians.
I guess a ubm stands for a mixture of gaussians.
The system is called ubm5b2
I'm guessing this means 5 mixtures and 2 ... ?
Gaussian selection?

- train using  subspace Gaussian mixture model sgmm
This is taking a while ...

We need to work on a legitimate language model.
Right now  we don't really have one.

- Problems
The scripts I'm using from the iban recipe get  confusing at this point.
I don't see decoding for the ubm system.
Apparently, the ubm system is trained for the sgmm system.
?Anyway, I'll have to figure this out later.
- Decoding using the sgmm system:
WER: 14.12



I'm done with this project.
Summary:
I followed the iban recipe where I used the yaounde corpus instead of the
iban corpus.

- lm:
The voxforge recipe uses an lm trained on the transcripts of the training
data.
I did not do this for our yaounde corpus.
This is tricky for our yaounde corpus.
We have to choose our test set so that it does not overlap at the sentence
level with the training set.
I'm not sure this is possible, since the informants on one machine read a
list of prompts that were also read by informatnts on another machine.
Now that we have the Gabon test set, we could use the yaounde corpus for
training, and test on the Gabon data.
We would use the prompts from the yaounde corpus to train the lm.
We could add some other text to the lm training set as long as we do not add
the Gabon prompts.

- G2P:
I had a discussion with Hazrat about the g2p lexicon.
There seem to be 3 approaches:
1. Jalalabad
2. Kanduhar
Uses spelling from 1, but pronunciation from kanduhar.
3. Expat Kanduhari
Uses both spelling and pronunciation from Kanduhar.

We need to figure out what # means in sampa and how it differs from the .
sign.



- Plan for tomorrow:
Run through the same steps for the GP corpus and the younde+gp system.

I'm a little disappointed with the iban recipe. I thought it included more
methods.
It does not go into neural network methods or ivector methods.
I'll have to get these methods from another recipe.
The babel recipe would probably be the best.





-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 20, 2016 3:15 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: RE: Daily Activities Report for Tuesday July 19 2016

The 3 and 4 ngram models are there this morning.
No srilm errors, slthough there are Unicode non-character warnings for the
lower casing of the gigaword corpus.


- Try to use these ngram models in the gp and yaounde systems.

It looks like monophone training does not require an lm.
The lm is used for decoding.

- lm2fst
This took a while
- make graph 
This is taking even longer.
This is the step that expands all the component FSTs into 1 big fst.

- running mk graph on all 3 systems at once: yaound, gp, and yaound+gp
 
Maybe I should stick to the small lm  for a first pass at getting all the
steps to build?

- Ran alignments using yaound+gp monophone system 
This is the first step to building a triphone system.
- Ran alignment diagnostic
This tells you some interesting statistics about each phone.
The proportion of the occurrence of each phone
The average duration in frames of each phone.
A final statistic says  that the corpus has a total of 31 hours of frames 24
of which are non-silence.

I'm giving up on the huge LM for now.
I'm going back and using the prompts to traing the lm, at least for the
yaounde system.

- ran alignment on yaounde data
Diagnostic claims a total of 7.3 hours and 4.2 non-silence.

- running train deltas
I'm not sure what this does.
It uses the questions to build the decision tree to cluster the triphones.
I don't think we have a set of questions yet.
I have a stand in file for the questions.
This adds delta and delta delta features
- Moving along with triphone training on yaounde
Ran the mk_graph script that expands the fst networks with all fst
components.

I'll pick up tomorrow with decoding of the tri1 system.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 19, 2016 6:23 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Tuesday July 19 2016

- lower case the prompts in the gp corpus
This would be easy were it not for the fact that the file names contain
upper case letters.
I have  a script that writes the file name followed by the prompt.
I cannot just lower case each line, since the file name needs to remain in
upper case.
So I first  store the file name and the prompt separately, down case the
prompts, then I paste the resulting files into 1 file.

I put in the time to write the filenames in the kaldi style.

- build the gp system

Why am I having to go through all this trouble to build gp?
There's a recipe for it on kaldi.
It assumes a language model.
But other than that it should work.

- Decoded gp on gp 
WER: 41.80
- Decoded gp on yaounde:
WER: 88.39

- decoded yaounde on gp
WER: 92.86

I think there is a reason these are so bad.
The gp system uses the gp prompts to build the lm and the yaounde system
uses the yaounde prompts.
There is a mismatch between the gp prompts and the yaounde prompts.


- lm
Time to get a fixed lm to work with.
I'm going to try to make an lm from the following corpora:
1. yaounde prompts
2. gp prompts
3. gigafren

I'm making a separate directory to do the lm work.

- training lm

Hopefully when I come in tomorrow the script will have produced ngram
language models.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Monday, July 18, 2016 5:30 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Monday July 18 2016

I left off last week on the yaounde + gp system.
I had setup the train and test folds in a way that I ended up testing on the
training data.
I was running the same script that generates a random train/test split on
the concatenation of the yaounde + gp data, but I was testing on the
previous yaounde and gp random split.

I now setup my scripts to use the split  done previously for yaounde and gp
separately.
I ran through the data setup script.
On the way I'm trying to make the file naming conventions follow the kaldi
conventions.
TThis mostly means using underscores instead of dashes.

I run into a problem when I run the lm prepare script.
I'm using the prompts from yaounde and gp to make the lm training corpus.
There are around 15k sentences.
I get a kn discount error.
I think this means there is not enough data to train 3 grams.
This is a good time to get the yaounde prompts that Steve worked on.
In working with Steve's prompts, I found a problem with the previous prompt
list.
I must have inserted a newline after an "i.e.".
This inserted at least one extra sentence line.
This could really make a difference if we have a lot of recordings of
sentences beyond 1343.
I don't think we do.

I've spent most of the day going back to the basic yaounde system.
I realized that I was not  conditioning the prompts before using them for
training.
Well ... I was, but I should have been doing it earlier in the process.
I separated data preparation into 3 steps:
1. setup
2. lm prep
3. dictionary prep

- extracted mfccs
- convert lm to fst
- train monophones
I get warnings.
This could possibly mean there are problems with transcripts.
- make graph
- decode test data

So I spent the whole day redoing the yaounde system
It looks like the gp data does not have the same problem.
It looks like it was ready to process out of the box.

- Decoding  yaound on yaound finished.
WER: 26.16
So obviously something was wrong before.
I think the labels were way off.
I verified that the sets of speakers in the train and test sets were
disjoint.

- Back to yaounde + gp
I need to set this to use the new yaounde prompts.
It's suspicious that the gp does not do as well as the Yaounde.
Sure enough ... I did not downcase the gp data.
Punctuation was stripped, but no lowercasing.

Tomorrow:
- Return to the basic gp system
Start by lower casing the gp prompts.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Friday, July 15, 2016 6:10 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Friday July 15 2016

I'm going to skip gender for now on this gp+yaounde monophone system.
- sorting problems
The filenames in the GP corpus have upper case letters.
When the environment variable LC_ALL is set to C upper case letters are
ordered before  lowercase letters.
In a script I sort the filenames.
I was not setting the LC_ALL variable.
The kaldi recipes have scripts that check for these kinds of problems.
This was a problem when combining the filenames from yaounde and gp.
Having trouble with monophone training.
Could it be a sorting problem again.
I sort the feats.scp file.
Maybe I need to set LC_ALL=C here too?
Yup, it looks like that was the problem.

- Trained monophones 
- Decoded yaounde + gp  on yaounde + gp test set
WER: 55.21%
This is a monophone system trained on both Yaounde and GP and tested on 7
speakers from GP and 3 speakers from Yaounde.

- Decoded yaounde + gp system on yaounde test set:
WER: 63%
This is the  same yaounde + gp system tested on the same 10 yaounde speakers
as the yaounde system that got 88% WER.

- Decoded Yaounde + GP system on GP test set:
WER: 25.75
Not sure why this is so good compared to gp on gp 44%.
I'm getting these results from a file called best_wer.
Apparently tests are run using different parameter settings.
For example, I see 3 settings for word penalty.
OK ... I'm being stupid ...
I'm testing on training data.
When I build the Yaounde + GP system I need to use the train/test fold split
from the separate Yaounde and GP systems.
So, I need to  rebuild the Yaounde + GP system.
I'll stop here for this week.
Next week I need to  resume by getting the appropriate training/test sets
for a Yaounde + GP system.
I'll have to fix the scripts that get the lists of training and test data.

Summary table:

Train & test & WER %
Y & Y & 87
GP & GP & 44 
Gp & y & 92.54
Y+GP & Y+GP & 55.21
Y+GP & Y & 63
Y+GP & GP & 25.75

The 55.21 is ok for y+gp on y+gp, but the 63 and 25 for y+gp on y and y+gp
on gp were obtained on contaminated data.


Y stands for Yaounde


The motivation of the problem we're working on is that a system trained on
(even a large) corpus of European French performs poorly on French as spoken
in Africa.  Also the corpora we have for sub populations inAfrica are small
compared to corpora for European French, so systems trained only on those
sub populations still perform poorly. So the fact that we get 87 wer for
Yaounde on Yaounde supports our project. Although I expect that the 87 will
drop with more sophisticated models.

I need the results of gp on Yaounde to fill in the motivation. I think I got
a 92, but I did not save the results
 
GP on gp 44
Y on y 87
Gp on y 92

These numbers support our story. GP on GP is good (44), while gp on Y (92)
and Y on Y (87) are horrible.
 
I'm running the gp on y again now.
Yes.gp on y gives 92.54, really bad.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Thursday, July 14, 2016 5:37 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Thursday July 14 2016

Mono training finished successfully yesterday.
- mkgraph ran successfully ( I think)
This is an important step in the system building process.
It creates a "fully expanded decoding graph"
It basically creates the "model", including the lm, pronunciation
dictionary, context dependency and hmm structure.
The output is a finite state transducer (fst).
I think each component is expanded to an fst then they are all combined into
1 fst.

- Ran decoding
- Ran scoring
I had trouble with scoring.
The problem was a missing script:
Utils/int2sym.pl
The fact that this script was missing was buried in a log file for the
scoring script run.
I reran scoring:
Wer: 44%
The test data here are from the GP corpus.
I guess what we want now is to test The GP system on Yaounde data.

** TODO Test GP on Yaounde

I have to downsample the Yaounde data from 22050 to 16k.
Done.
I asked  Justin to put this data on the corpora disk.

- Ran decoding on Yaounde test data:
WER: 92.54
I consider this preliminary.
There are a lot of files hanging around and I'm not sure the correct files
are being used in decoding and scoring.
I'm not sure how fst decoding works.
There's an fst built with the training data.
This is the "model" the decoder uses.
The decoder "decodes" test data.
The test data is stored in a directory.
When I ran the GP decoder on the GP test data the decoder pointed at the GP
test directory .
When I ran the GP decoder on the yaounde test data I pointed the GP decoder
to the yaounde test directory.
I'm worried that the yaounde decoder and the gp decoder get the same score
on the yaounde data 92%.
Actually, with Steve's dictionary the Yaounde system got 87.
- Reran Yaound build with downsampled data:
WER: 88.08
The test set is different on every build since it is chosen randomly.
This might explain the difference between 87% for 22050 and 88% for 16000.


- Moving on to a system built on GP concatenated with Yaounde.
- working through the data setup script.
This mostly involves making the script I used for gp to run on the union of
gp and yaounde.
To get one script to run, I made a symbolic link from the corpora disk to
the working directory on the gpu machine.
I'm not sure this will work down range.
I'm leaving today at the point where the speaker genders are extracted from
info files.
I'm not sure if I'm going to do gender at this point.
I might leave it for a later refinement.
Next steps include lexicon preparation and lm prep.




-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Wednesday, July 13, 2016 5:19 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for July 13 2016

Writing the perl command line in a separate perl script seems to have solved
the problem I was having yesterday with the script that prepares the lm
training data.
Currently I'm only using the prompts to train the lm.
** TODO Decide on what to use for lm training data

- Working on extracting MFCCs.
GP data sampled at 16k
Yaounde data sampled at 22050

I think we need to downsample the Yaounde data to 16k.
I did this in the past, I unintentionally skipped downsampling this time.

I realized that gp file names are not in numerical order.
I renamed wav files and prompts.

The training script was failing.
I finally figured out that the number of jobs must be the same for the mfcc
extraction and the training.
I set the number of jobs to 10 for both cases.
More than 10 fails since there are only 10 speakers in the test set.

- running a script  called format_lms.sh 
I think this script converts the LM into an fst.

- run make graph script

- run decoder

- run scoring

On scoring I found a problem with my transcripts.
The script that writes the prompts to each speaker directory did not write
the speaker directory to the paths.
I'm running each script again  to see if the problem gets fixed in scoring.
I'm surprised this did not cause a problem somewhere else.
I'm leaving having started the run of the mono train script.
I'm not going to wait for it to finish before leaving.
This script basically aligns mfcc vectors with monophones and computes
parameters for the gaussians.
If it succeeds, I need to next run the script that creates the graph.


-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* Tuesday, July 12, 2016 4:57 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for July 12 2016

I got the scoring script to run. Indeed the problem was with the 3
arguments.
Score.sh takes 3 arguments:
1. A data directory that contains  a bunch of files and directories
associated with the run of the decoder on the test data.
2. A language directory containing file and directories associated with the
fst that is input to the decoder. There are a lot of files referring to
phones and words  here.
3. A directory where the results and trace from the scoring are placed.

The WER was 92%, so only 8% of the words were correctly recognized.
Hoepfully, this will be the lowest score we get :)


** TODO Rebuild previous system with Steve's dictionary

EU is not a phone in Steve's dictionary.
Eu is a rounded phone as in deux
Ee (schwa) needs to be added to phone list.

The uy phone appears in the dictionary, but not in the phone list.
Add uy to phone list.
Add gn to phone list.

I rebuilt the system with the new dictionary.
WER: 87% down from 92%
Installed 2 diagnostic scripts.
LM:  3gram only trained on 1339 prompts.

Moved on to building similar ASR system with Kaldi for the GlobalPhone
corpus.
Wrote data setup scripts :
This mostly gets lists of files, associates wav files with prompts,
utterences with speakers, speakers with utterences, etc .
Wrote dict setup script:
This mostly deals with preparing the phone lists to build FSTs and what to
do with silence.

I'm leaving in the middle of writing the  script to  prepare the lm
There's 1 script to get the lexicon.
This takes an argument that does not seem to be there yet.
I have a perl command line written inside a bash script.
I'm going to write the perl script in a separate file.



-----Original Message-----
From: Morgan, John J CIV USARMY RDECOM ARL (US) 
* >Monday, July 11, 2016 3:42 PM
To: Larocca, Stephen A CIV USARMY RDECOM ARL (US)
<stephen.a.larocca.civ@mail.mil>; Vanni, Michelle T CIV USARMY RDECOM ARL
(US) <michelle.t.vanni.civ@mail.mil>; Hernandez, Luis CIV USARMY RDECOM ARL
(US) <luis.hernandez2.civ@mail.mil>
Subject: Daily Activities Report for Monday July 11 2016

I'm going to write a short Daily Journal, mostly so the next day I can
remember where I left off the previous day.
If this annoys you let me know.
John
I'm writing a kaldi recipe to build a monophone ASR system trained and
tested on the Yaounde corpus of French.
I have written the data preparation scripts, the scripts that run the mfcc
extractor the script that runs the training, and the script that decodes
test data.
I'm leaving today in the middle of setting up the script that runs the score
script.
I'm expecting this to give me WER scores for the test data.
It looks like the score script is expecting a file containing words.
I haven't generated this file yet.
I also have to look closely at the 3 arguments to the score script.
