\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[acronym,toc,xindy]{glossaries}
\makeglossaries
\tolerance=1000
\author{John Morgan \and Michelle Vanni \and Stephen LaRocca }
\date{\today}
\title{A Minimal Example of Multi-Task Learning Using The Librispeech 960 and Tunisian MSA Corpora}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.3.2 (Org mode 8.2.10)}}
%ABBREVIATIONS
\newacronym{ASR}{asr}{Automatic Speech Recognition}
\newacronym{DL}{dl}{Deep Learning}
\newacronym{ML}{ml}{Machine Learning}
\newacronym{MT}{mt}{Machine Translation}
\newacronym{MTL}{mtl}{Multi-Task Learning}
\newacronym{NN}{nn}{Neural Network}
\newacronym{BL}{bl}{Background Language}
\newacronym{TL}{tl}{Target Language}
\newacronym{MSA}{msa}{Modern Standard Arabic}
\newacronym{LR}{lr}{Low Resource}
\newacronym{SAT}{sat}{Speaker Adaptive Training}
\newacronym{GMM}{gmm}{Gaussian Mixture Model}
\newacronym{HMM}{hmm}{Hidden Markov Model}
\newacronym{OOV}{oov}{Out Of Vocabulary}
\newacronym{SRILM}{srilm}{Stanfor Research Institute Language Model}
\newacronym{WER}{wer}{Word Error Rate}
\newacronym{MFCC}{mfcc}{MMel Frequency Cepstral Coefficients}
\newacronym{PLP}{plp}{Perceptual Linear Prediction}
\newacronym{DNN}{dnn}{Deep Neural Network}
\newacronym{AM}{am}{Acoustic Model}
\newacronym{QCRI}{qcri}{Qatar Computing Research Institute }

\begin{document}

\maketitle
\tableofcontents


\section{ABSTRACT}
\label{sec-1}
Multi-Task Learning was applied to a Large corpus of English and a small corpus of Modern Standard Arabic  read speech. 
An improvement in Word Error Rate over the best Single-Task Learning method was observed. 
\section{INTRODUCTION}
\label{sec-2}
We are using \gls{NN}s as a framework for acoustic modeling in \gls{ASR}. 
large \gls{ASR} systems are trained on tens of thousands of hours of speech data \cite{Heigold13multilingualacoustic}. 
\gls{NN} models perform well when these amounts of data are available. 
We assume we are under conditions of severe training data sparcity in the \gls{TL}.
We also assume that we have access to a large corpus of speech in another language. 
We will refer to this other language as the \gls{BL}.
In these situations where training data is scarce for the \gls{TL}, more sophisticated methods need to be employed in order to see benefits from \gls{NN} models. 
\gls{MTL}\cite{Caruana93multitasklearning:} is a framework that enables the advantages of \gls{DL} to be applied in the situation where we have access to large amounts of data in the \gls{TL} and scarce   resources in the \gls{TL}.  

In \gls{MTL} We build an \gls{NN} that has two kinds of layers.
\begin{enumerate}
\item Shared Layers.
\item Language Specific Layers.
\end{enumerate}

The shared layers are trained on all the training data from all the languages.
We assume that the shared layers extract the important features that are common across languages. 

The language specific layers are trained only on data from the \gls{TL}.

For this note we present an experiment to test the \gls{MTL} method on two specific publically available corpora: the large Librispeech English corpus and a small corpus of Tunisian Accented \gls{MSA}. 

We formulate our research question as follows.

Can \gls{NN} acoustic models trained with \gls{MTL} on data  from  two different languages improve performance of an  \gls{ASR} system for a \gls{LR} \gls{TL}?

\section{DATA}
\label{sec-3}
\subsection{Librispeech}
\label{sec-3-1}
LibriSpeech is a corpus of read speech, based on LibriVox's public domain audio books. 
We used the cleaned training fold of 960 hours of speech. 

\subsection{Tunisian \gls{MSA}:}
\label{sec-3-2}
This is a corpus of ten hours of \gls{MSA} as spoken by 120 male and female Tunisians in 2003. 
The informants provided recitations and answers to questions. 


The \gls{NN} for Each language has 8 layers.

\begin{enumerate}
\item One input layer,
\item 6 hidden layers,
\item One Bottleneck layer,
\item One affine layer, and
\item One soft max layer.
\end{enumerate}

The dimension of the hidden layers is 1024.
The dimension of the Bottleneck layer is 512.

The soft max layer outputs a probability density function over the clustered triphones.

Context:
16 frames to the left, 12 frames to the right.

\section{EXPERIMENT}
\label{sec-4}
We used the kaldi toolkit\cite{Povey11thekaldi} to build our ASR systems. 
We derived our setup from the kaldi babel$_{\text{multilang}}$ recipe. 
We tried to implement our setup to contain only methods that are required to run the \gls{MTL} method. 
Thus, We did not use i-vectors which are standard in many kaldi recipes. 
We did include however a bottleneck layer. 

We built baseline \gls{SAT} \gls{GMM} \gls{HMM} \gls{AM}s for both the Librispeech and Tunisian \gls{MSA} corpora.
For the Tunisian \gls{MSA} baseline system we derived our pronouncing dictionary from the 2 million entries from the \gls{QCRI} vowelized dictionary\cite{aaliArabicKaldi} available at:
\url{http://alt.qcri.org/resources/speech/dictionary/ar-ar_lexicon_2014-03-17.txt.bz2}
We added the \gls{OOV} words from the Tunisian \gls{MSA} training set and the test set. 
We trained our $3$-gram language model with the \gls{SRILM} toolkit on the transcripts from the training and test data. 
Our best \gls{WER} results for Tunisian \gls{MSA} were obtained with online chain models.

We followed the kaldi standard recipe for the librispeech cleaned 960 hours of speech task with one exception. 
We extracted and trained with \gls{PLP}$_{\text{pitch}}$ features instead of \gls{MFCC} features. 
We followed this path since we derived our scripts from the kaldi babel$_{\text{multilang}}$ recipe. 

Alignments generated by the  \gls{SAT} \gls{GMM} \gls{HMM}s were used to train a 7-layer bilingual raw deep \gls{NN}  on the combined set of training examples from the English Librispeech and Tunisian \gls{MSA}  corpora.

%Note that instead of considering this as a bilingual model it can be viewed as a Tunisian \gls{MSA} model whose parameters are shared with a English model. 

The data from the Tunisian \gls{MSA} corpus was used to readjust the parameters in the last two layers of the bilingual \gls{DNN} model to produce a new monolingual Tunisian \gls{MSA}  acoustic model. 

Similarly, a new monolingual English model was produced.
These two models share the parameters in their first five layers, only their final 2 layers are different.

The monolingual Tunisian \gls{MSA} \gls{AM} was used to decode a test set of speech from four speakers, 3 Libyan males and one Tunisian female.

\section{RESULTS}
\label{sec-5}

Tunisian \gls{MSA} Baseline: \gls{WER} 11.03 %[ 726 / 6584, 61 ins, 237 del, 428 sub ] exp/chain/tdnn1a$_{\text{sp}}$$_{\text{online}}$/decode$_{\text{test}}$/wer$_{\text{13}}$$_{\text{0}}$.0

WER after MTL: \gls{WER} 7.12 %[ 469 / 6584, 100 ins, 124 del, 245 sub ] exp/multi/Tunisian$_{\text{MSA}}$/decode$_{\text{test}}$/wer$_{\text{13}}$$_{\text{0}}$.0


\bibliographystyle{plain}
\bibliography{jjm}
\printglossaries

% Emacs 25.3.2 (Org mode 8.2.10)
\end{document}