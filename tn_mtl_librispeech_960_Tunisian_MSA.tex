\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[acronym,toc,xindy]{glossaries}
\makeglossaries
\tolerance=1000
\author{John Morgan \and Michelle Vanni \and Stephen LaRocca }
\date{\today}
\title{A Minimal Example of Multi-Task Learning Using The Librispeech  and Tunisian MSA Corpora}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.3.2 (Org mode 8.2.10)}}
%ABBREVIATIONS
\newacronym{ASR}{asr}{Automatic Speech Recognition}
\newacronym{DL}{dl}{Deep Learning}
\newacronym{ML}{ml}{Machine Learning}
\newacronym{MT}{mt}{Machine Translation}
\newacronym{MTL}{mtl}{Multi-Task Learning}
\newacronym{NN}{nn}{Neural Network}
\newacronym{BL}{bl}{Background Language}
\newacronym{TL}{tl}{Target Language}
\newacronym{MSA}{msa}{Modern Standard Arabic}
\newacronym{LR}{lr}{Low Resource}
\newacronym{SAT}{sat}{Speaker Adaptive Training}
\newacronym{GMM}{gmm}{Gaussian Mixture Model}
\newacronym{HMM}{hmm}{Hidden Markov Model}
\newacronym{OOV}{oov}{Out Of Vocabulary}
\newacronym{SRILM}{srilm}{Stanfor Research Institute Language Model}
\newacronym{WER}{wer}{Word Error Rate}
\newacronym{MFCC}{mfcc}{MMel Frequency Cepstral Coefficients}
\newacronym{PLP}{plp}{Perceptual Linear Prediction}
\newacronym{DNN}{dnn}{Deep Neural Network}
\newacronym{AM}{am}{Acoustic Model}
\newacronym{QCRI}{qcri}{Qatar Computing Research Institute }
\newacronym{FST}{fst}{Finite State Transducer}

\begin{document}

\maketitle
%\tableofcontents


\section{ABSTRACT}
\label{sec-1}
Multi-Task Learning was applied to a Large corpus of English and a small corpus of Modern Standard Arabic  read speech. 
An improvement in Word Error Rate over the best Single-Task Learning method was observed. 
\section{INTRODUCTION}
\label{sec-2}
For the experiments described here, we used the \gls{NN} as a framework for acoustic modeling in \gls{ASR}. 
Recently, large \gls{ASR} systems have been trained on tens of thousands of hours of speech data \cite{Heigold13multilingualacoustic}. 
\gls{NN} models have performed well when these amounts of data are available. 
We assumed we are under conditions of severe training data sparcity in the \gls{TL}.
We also assumed that we have access to a large corpus of speech in another language. 
We refer to this other language as the \gls{BL}.
%In these situations where training data is scarce for the \gls{TL}, more sophisticated methods need to be employed in order to see benefits from \gls{NN} models. 
\gls{MTL}\cite{Caruana93multitasklearning:} is a framework that enables the advantages of \gls{DL} to be applied in the situation where we have access to large amounts of data in the \gls{BL} and scarce   resources in the \gls{TL}.  

an \gls{NN} was built with two kinds of layers.
\begin{enumerate}
\item Shared Layers.
\item Language Specific Layers.
\end{enumerate}

The shared layers were trained on all the training data from all the languages.
%They extracted the important features that were common across languages. 

The language specific layers were trained only on data from the \gls{TL}.

Our research question was formulated as follows.

Can \gls{NN} acoustic models trained with \gls{MTL} on data  from  two different languages improve performance of an  \gls{ASR} system for a \gls{LR} \gls{TL}?

\section{DATA}
\label{sec-3}

We ran an experiment to test the \gls{MTL} method on two specific publically available corpora: the large Librispeech English corpus and a small corpus of Tunisian Accented \gls{MSA}. 



\subsection{Librispeech}
\label{sec-3-1}
LibriSpeech is a corpus of read speech, based on LibriVox's public domain audio books.
The corpus is available at:

\url{http://www.openslr.org/resources/12}.

We used the cleaned training fold of 960 hours of speech. 

\subsection{Tunisian \gls{MSA}:}
\label{sec-3-2}
This is a corpus of ten hours of \gls{MSA} as spoken by 120 male and female Tunisians in 2003. 
The informants provided recitations and answers to questions. 
It can be downloaded at:

\url{http://www.openslr.org/resources/46}.

\section{EXPERIMENT}
\label{sec-4}
We used the kaldi toolkit\cite{Povey11thekaldi} to build our ASR systems. 
We derived our setup from the kaldi babel multilang recipe. 
We tried to implement our setup to contain only methods that are required to run the \gls{MTL} method. 
Thus, We did not use i-vectors which are standard in many kaldi recipes. 
We did include however a bottleneck layer. 

We built baseline \gls{SAT} \gls{GMM} \gls{HMM} \gls{AM}s for both the Librispeech and Tunisian \gls{MSA} corpora.
For the Tunisian \gls{MSA} baseline system we derived our pronouncing dictionary from the 2 million entries from the \gls{QCRI} vowelized dictionary\cite{aaliArabicKaldi} available at:
\url{http://alt.qcri.org/resources/speech/dictionary/ar-ar_lexicon_2014-03-17.txt.bz2}
We added the \gls{OOV} words from the Tunisian \gls{MSA} training set and the test set. 
We trained our $3$-gram language model with the \gls{SRILM} toolkit\cite{Stolcke02srilm-} on the transcripts from the training and test data. 
Our best \gls{WER} results for Tunisian \gls{MSA} were obtained with online chain models.

We followed the kaldi standard recipe for the librispeech cleaned 960 hours of speech task with one exception. 
We extracted and trained with \gls{PLP}$_{\text{pitch}}$ features instead of \gls{MFCC} features. 
We followed this path since we derived our scripts from the kaldi babel multilang recipe. 

\subsection{Neural Network Configuration}
\label{sec:nnconfig}


The \gls{NN} for both languages had 8 layers.

\begin{enumerate}
\item One input layer,
\item 6 hidden layers,
\item One Bottleneck layer,
\item One affine layer, and
\item One soft max layer.
\end{enumerate}

The dimension of the hidden layers was 1024.
The dimension of the Bottleneck layer was 512.

The soft max layer outputs a probability density function over the clustered triphones.

The frame Context was set to:
16 frames to the left, 12 frames to the right.

\subsection{Training}
\label{sec:train}


A bilingual raw deep \gls{NN}  was trained on the combined set of training examples from the English Librispeech and Tunisian \gls{MSA}  corpora.
The data from the Tunisian \gls{MSA} corpus was used to readjust the parameters in the last two layers of the bilingual \gls{DNN} model to produce a new monolingual Tunisian \gls{MSA}  acoustic model. 
Similarly, a new monolingual English model was produced.
These two models shared the parameters in their first six layers, only their final 2 layers were different.

\subsection{Decoding}
\label{sec:decode}


The monolingual system with the Tunisian \gls{MSA} \gls{AM} was used to decode a test set of speech from four speakers, 3 Libyan males and one Tunisian female.
The same \gls{FST} decoding graph that was built for the Tunisian \gls{MSA} \gls{SAT} \gls{GMM} \gls{HMM} system was used for decoding with the \gls{MTL} \gls{AM} set.

  \section{RESULTS}
\label{sec-5}

TThe Single-Task unisian \gls{MSA} Baseline system with chain model trained \gls{AM}s yielded a \gls{WER} of 11.03. %[ 726 / 6584, 61 ins, 237 del, 428 sub ] exp/chain/tdnn1a$_{\text{sp}}$$_{\text{online}}$/decode$_{\text{test}}$/wer$_{\text{13}}$$_{\text{0}}$.0

After MTL, the Tunisian \gls{MSA} system gave a \gls{WER} of 7.12. %[ 469 / 6584, 100 ins, 124 del, 245 sub ] exp/multi/Tunisian$_{\text{MSA}}$/decode$_{\text{test}}$/wer$_{\text{13}}$$_{\text{0}}$.0


\bibliographystyle{plain}
\bibliography{jjm}
\printglossaries

% Emacs 25.3.2 (Org mode 8.2.10)
\end{document}