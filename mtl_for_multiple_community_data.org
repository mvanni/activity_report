#+TITLE: Multi-Community  Multi-Task Learning 

AI, Machine Learning, Deep Learning and Neural Networks  have surged in the past decade as a paradigm for modeling the processes of Automatic Speech Recognition (ASR), Machine Translation  and Text to Speech. 
Reference ASR systems are now trained on tens of thousands of hours of speech data. 
Neural Network models perform well  when these amounts of data are available. 
However, in situations where training data is scarce more sophisticated methods need to be employed in order to see benefits from neural network models. 
Multi-task Learning (MTL) is a framework that enables the advantages of Deep Learning to be applied in situations of data scarcity. 

The US Army frequently finds itself in the situation where it needs help communicating with people who speak a low resource language. 
It often can benefit from speech to speech applications  that work well for populations of speakers who speak with very special phonological inventories in conversations  about very special topics. 
In this paper we will investigate a way to leverage the benefits of the latest advances in Neural Networks despite a severe paucity of training data in the target linguistic environment . 

In many cases the Army is interested in a low resource language for wich some speech resources are available. 
A small data collection project may have been conducted with a small number of members from the speaking population who are easily accessible. 
Recordings from a radio broadcast also maybe available. 
A corpus of recordings of telephone conversations  may also exist. 
Taken separately, these resources usually are not adequate for building an ASR system that can be used in the target speech to speech application. 
In this paper we will consider using the MTL method to incorporate resources from more than one speech technology community in order to build ASR acoustic models that are adequate for the speech to speech task. 

We set as a goal to produce components of an ASR   system  for African Accented French. 
These components include acoustic models, a lexicon and a language model.
They will be designed to be embedded in   one side of a speech to speech device. 
The language model  will be  trained   on conversations about military issues. 

Our neural network acoustic models will be trained with MTL on speech data from varying numbers of corpora in varying numbers of languages including varying amounts of data from African Accented French. 

We will  experiment with incorporating speech from other sources, such as broadcast news; other speaking styles , such as prompted and spontaneous speech; and other recording conditions, such as noisy  telephone conversations. 



We will address the following research question that we believe has not been answered in the literature.
Can military relevant ASR tasks in very low resource conditions benefit from the incorporation of speech data from diverse speech technology communities  into the MTL training method? 
For example, can a corpus of broadcast news data be used to improve the ASR component of an African Accented French /English speech to speech device that was previously trained only on read and prompted speech? 

So far, we have demonstrated that we can build systems with the MTL method by producing a minimal example that uses only two languages. 
We built baseline SAT GMM HMM acoustic models for Japanese and Mandarin Chinese with data from the GlobalPhone speech corpus. 
We implemented MTL as follows:

-  Alignments generated by the Japanese and Mandarin  SAT GMM HMMs were used to train a 7-layer bilingual neural network acoustic model on the combined set of training examples from the Japanese and Mandarin corpora. 
Note that instead of considering this as a bilingual model it can be viewed as a Mandarin model whose parameters are shared with a Japanese model.  

- The data from the Mandarin corpus was used to readjust the parameters in the last two layers of the bilingual neural network model to produce a new monolingual Mandarin acoustic model  . 
Similarly, a new monolingual Japanese model was produced. 
These two models share the parameters in their first five layers, only their final 2 layers are different.

- The monolingual Mandarin acoustic model was used to decode a test set. 
Preliminary WER scores showed a slight improvement over the WER scores for the Mandarin SAT GMM HMMs. 
Although this is good news, we do not necessarily expect the new monolingual Mandarin acoustic models to yield better WER scores than models trained with the state-of-the-art chain  model objective. 
We do expect the new MTL trained models to be more robust. 

Our minimal example did not include a prefinal layer of bottleneck (BN) features and it was not trained using i-vectors. 
BN features and i-vector training have been shown to lower WER scores. 
We are expecting our MTL trained models to yield lower WER scores once they use i-vectors and BN features. 

For our experiments we plan on applying MTL to up to 30 languages and up to 300 hours of speech.
The GlobalPhone corpus has data for 17 languages. 
We also have 9 government-owned speech corpora that we plan on incorporating into the MTL training project. 
Most of the data in these corpora were collected in recitative mode through close mounted microphones. 
For broadcast news data we plan on using the GALE Arabic and Mandarin Chinese corpora. 
For noisy telephone conversations we will use data from the Babel project.
