#+TITLE: Multi-Community Multi-Task Learning for ASRJohn J. Morgan, Michelle Vanni, Stephen A. La Rocca
* ABSTRACT
* 1 INTRODUCTION
AI, Machine Learning, Deep Learning and Neural Networks have surged in the past decade as a paradigm for modeling the processes of Automatic Speech Recognition (ASR), Machine Translation (MT) and Text to Speech (TTS). 
Reference ASR systems are now trained on tens of thousands of hours of speech data. 
Neural Network models perform well when these amounts of data are available. 
However, in situations where training data is scarce more sophisticated methods need to be employed in order to see benefits from neural network models. 
Multi-task Learning (MTL) is a framework that enables the advantages of Deep Learning to be applied in situations of data scarcity.


* 2 BACKGROUND
The term Low Resource Language (LRL) is used to refer to those human languages for which, from the perspective of computational processing, there exist very few digital resources and little to no automated technology [Cite? Cieri et al. 2016 LREC?; Beyerlein et al., ASRU1999]. 
The resources, which have included lexicons, parsers and morphological analyzers, among others, now most often consist almost entirely of training data, such that, references to LRLs in this paper equate to languages for which little to no training data is available.
NICHE APPLICATIONS AND THIS PARTICULAR ARMY APPLICATION: ASR has niche applications in medical and business domains [Cite?] Military requirements often …   specifically ... the US Army frequently finds itself in the situation where it needs help communicating with people who speak a low resource language. It often can benefit from speech to speech applications that work well for populations of speakers who speak with very special phonological inventories in conversations about very special topics. 
TECHNIQUE FOR SOLVING ARMY PROBLEM:  In this paper we will investigate a way to leverage the benefits of the latest advances in Neural Networks despite a severe paucity of training data in the target linguistic environment.
DATA AS RESOURCE FOR SOLVING ARMY PBM: Although, by definition, data volumes are sparse for LRLs, in many cases, the Army is interested in a LRL for which a modicum of speech resources does exist. This data may have been produced as a result of a small data collection project, conducted to capture the acoustic evidence of the speech of a small number of easily accessible members of a subpopulation. Or, recordings from a radio broadcast may have been made for language learning purposes.  Corpora of telephone conversations, recorded to support research program collection efforts, may also exist. Taken separately, these resources usually are not adequate for building an ASR system that can be used in the target speech to speech (S2S) application. In this paper, however, we incorporate resources from these diverse speech technology communities in a MTL approach to the building of ASR acoustic models to meet the needs of the Army’s niche S2S task for LRLs.
3 DATAOur experiments 
4 EXPERIMENTS 4.1 4.2 4.3
We set as a goal to produce components of an ASR system for African Accented French. These components include acoustic models, a lexicon and a language model. They will be designed to be embedded in one side of a speech to speech device. The language model will be trained on conversations about military issues.
Our neural network acoustic models will be trained with MTL on speech data from varying numbers of corpora in varying numbers of languages including varying amounts of data from African Accented French.
We will experiment with incorporating speech from other sources, such as broadcast news; other speaking styles, such as prompted and spontaneous speech; and other recording conditions, such as noisy telephone conversations.
We will address the following research question that we believe has not been answered in the literature. Can military relevant ASR tasks in very low resource conditions benefit from the incorporation of speech data from diverse speech technology communities into the MTL training method? Forexample, can a corpus of broadcast news data be used to improve the ASR component of an African Accented French /English speech to speech device that was previously trained only on read and prompted speech?
So far, we have demonstrated that we can build systems with the MTL method by producing a minimal example that uses only two languages. We built baseline SAT GMM HMM acoustic models for Japanese and Mandarin Chinese with data from the GlobalPhone speech corpus. We implemented MTL as follows:.Alignments generated by the Japanese and Mandarin SAT GMM HMMs were used to train a 7-layer bilingual neural network acoustic model on the combined set of training examples from the Japanese and Mandarin corpora.
Note that instead of considering this as a bilingual model it can be viewed as a Mandarin model whose parameters are shared with a Japanese model. 
.The data from the Mandarin corpus was used to readjust the parameters in the last two layers of the bilingual neural network model to produce a new monolingual Mandarin acoustic model.  
Similarly, a new monolingual Japanese model was produced. These two models share the parameters in their first five layers, only their final 2 layers are different.
.The monolingual Mandarin acoustic model was used to decode a test set.
Preliminary WER scores showed a slight improvement over the WER scores for the Mandarin SAT GMM HMMs. Although this is good news, we do not necessarily expect the new monolingual Mandarin acoustic models to yield better WER scores than models trained with the state-of-the-art chain model objective. We do expect the new MTL trained models to be more robust.
Our minimal example did not include a prefinal layer of bottleneck (BN) features and it was not trained using i-vectors. BN features and i-vector training have been shown to lower WER scores. We are expecting our MTL trained models to yield lower WER scores once they use i-vectors and BN features.
For our experiments we plan on applying MTL to up to 30 languages and up to 300 hours of speech. The GlobalPhone corpus has data for 17 languages. We also have 9 government-owned speech corpora that we plan on incorporating into the MTL training project. Most of the data in these corpora were collected in recitative mode through close mounted microphones. For broadcast news data we plan on using the GALE Arabic and Mandarin Chinese corpora. For noisy telephone conversations we will use data from the Babel project.
* 5 RESULTS
* 6 RELATED WORK
* 7 CONCLUSIONS AND FUTURE WORK
* 8 REFERENCES
Beyerlein, et al., 1999, ASRU
Cieri, et al., 2016, LRECABBREVIATIONSASR Automatic Speech RecognitionMT Machine TranslationMTL Multi-Task LearningS2S Speech-to-SpeechTTS Text-to-Speech
